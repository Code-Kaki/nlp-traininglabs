{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dad747",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Seq2Seq (Encoder-Decoder) Model Architecture** has become ubiquitous due to the advancement of **Transformer** Architecture in recent years. Large corporations started to train huge networks and published them to the research community. Recently Open API has licensed their most advanced pre-trained Transformer model **GPT-3** to Microsoft. Even though the practical implementation of RNN has become almost non-existent, anyone starting to learn the most advanced algorithms still need to understand how to implement a Seq2Seq Model just using RNN and its variants (LSTM, GRU). In this notebook, we are going to implement **Machine Translation using Recurrent Neural Network and PyTorch** from scratch.\n",
    "\n",
    "![RNN](../../../images/RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec173b9",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before you start this notebook, you should know:\n",
    "1. The basic of pytorch and implementation of neural network using pytorch\n",
    "\n",
    "\n",
    "2. Understand the concept of recurrent neural network (RNN)\n",
    "\n",
    "\n",
    "3. Have basic understanding on seq-to-seq (encoder-decoder) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b73d4",
   "metadata": {},
   "source": [
    "# What will we accomplish?\n",
    "\n",
    "Steps to implement machine translation using Recurrent Neural Network with Pytorch:\n",
    "\n",
    "> Step 1: Multi30k dataset preparation \n",
    "\n",
    "> Step 2: Encoder-decoder Model Architecture\n",
    "\n",
    "> Step 3: Model Training and Evaluation\n",
    "\n",
    "> Step 4: Inference and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a8fc0",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [Import Libraries](#Import-Libraries)\n",
    "\n",
    "\n",
    "* [Dataset Preparation](#Dataset-Preparation)\n",
    "\n",
    "\n",
    "* [Encoder-Decoder Model Architecture](#Encoder-Decoder-Model-Architecture)\n",
    "\n",
    "    * [Encoder Model Using Pytorch](#Encoder-Model-Using-Pytorch)\n",
    "        * [__init__()](#__init__())\n",
    "        * [forward()](#forward())\n",
    "    \n",
    "    * [Decoder Model using PyTorch](#Decoder-Model-using-PyTorch)\n",
    "        * [One Time Step of Decoder](#One-Time-Step-of-Decoder)\n",
    "        * [Decoder Model](#Decoder-Model)\n",
    "        * [Teaching Force](#Teaching-Force)\n",
    "\n",
    "\n",
    "* [Combine Encoder and Decoder](#Combine-Encoder-and-Decoder)\n",
    "\n",
    "\n",
    "* [Model Initialization](#Model-Initialization)\n",
    "\n",
    "\n",
    "* [Training Loop](#Training-Loop)\n",
    "\n",
    "\n",
    "* [Inference](#Inference)\n",
    "    \n",
    "    * [predict()](#predict())\n",
    "    \n",
    "\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b79b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1110f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4489ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509785a5",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "We will be using Multi30k dataset with Spacy tokenizer.\n",
    "\n",
    "In case you are interested to learn more about Spacy, please visit the following link: https://www.presentslide.in/2019/07/implementing-spacy-advanced-natural-language-processing.html\n",
    "\n",
    "The `get_datasets()` function is where we prepare the dataset. We will reverse the German tokens as it enforces the initial LSTM layers in the Decoder to get more influenced by the initial part of source German tokens, which if you think about it makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d3badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(batch_size=128):\n",
    "    # Download the language files\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # define the tokenizer\n",
    "    def tokenize_de(text):\n",
    "        return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "    # Create the pytext's Field\n",
    "    source = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "    target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "    # Splits the data in Train, Test and Validation data\n",
    "    train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(source, target), root='../../../resources/.data', train='train', validation='val', test='test2016')\n",
    "\n",
    "    # Build the vocabulary for both the language\n",
    "    source.build_vocab(train_data, min_freq=3)\n",
    "    target.build_vocab(train_data, min_freq=3)\n",
    "\n",
    "    # Create the Iterator using builtin Bucketing\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          sort_within_batch=True,\n",
    "                                                                          sort_key=lambda x: len(x.src),\n",
    "                                                                          device=device)\n",
    "    \n",
    "    return train_iterator, valid_iterator, test_iterator, source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d32e4",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model Architecture\n",
    "\n",
    "Below is the diagram of basic **Encoder-Decoder Model Architecture**. We need to feed the input text to the Encoder and output text to the decoder. The encoder will pass some data, named as Context Vectors to the decoder so that the decoder can do its job.\n",
    "\n",
    "![Model Architecture](../../../images/model_architecture.jpg)\n",
    "\n",
    "This is a very simplified version of the architecture. As we build each part, we will focus more on specifics. \n",
    "\n",
    "Encoder-Decoder Model can be used in different fields of Artificial Intelligence such as **Machine Translation**, **Named Entity Recognition**, **Summarization**, **Chat-Bot**, **Question-Answering** and many more.\n",
    "\n",
    "Here we will be translating from **German to English**. For the datasource we will use the one provided by PyTorch as it takes much lesser computation power to train using this dataset. You can use Google CoLab to train your model if you donâ€™t have access to a GPU.\n",
    "\n",
    "We will start with a simple Encoder-Decoder architecture, then get into more complex version gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd96d46",
   "metadata": {},
   "source": [
    "## Encoder Model Using Pytorch\n",
    "\n",
    "We will defer the simple data processing steps until the model is ready. However just understand that, the input data will be a sequence of strings in array which will start with `<sos>` and end with `<eos>`. Take a look at a simple version of encoder architecture.\n",
    "\n",
    "![Encoder](../../../images/encoder.png)\n",
    "\n",
    "As you already know that Neural Network can only **understand number**, we need to first **convert each word to unique token of integer number**, then use **One-Hot Encoding to represent each word** (which is depicted as `one-hot` in the diagram above). This will be taken care as part of the preprocessing.\n",
    "\n",
    "We need to use PyTorch to be able to create the **embedding** and **RNN** layer. We will create the sub-class of the `torch.nn.Module` class and define the `__init__()` and `forward()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb1ea7",
   "metadata": {},
   "source": [
    "### `__init__()`\n",
    "\n",
    "The Embedding layer will take the **input data** and output the **embedding vector**, hence the dimension of those needs to be defined as `input_dim` and `embedding_dim`.\n",
    "\n",
    "The `vocab_len` is nothing but the **number of unique words** present in our vocabulary. After pre-processing the data, we can count the number of unique words in our vocabulary and use that count here.\n",
    "\n",
    "The `embedding_dim` is the output/**final dimension** of the **embedding vector** we need. A good practice is to use **256-512** for sample demo app like we are building here.\n",
    "\n",
    "Next we will define our LSTM Layer, which takes the `embedding_dim` as the input data and create total 3 outputs â€“ `hidden`, `cell` and `output`. Here we need to define the **number of neurons we need in LSTM**, which is defined using the `hidden dimension`. Again, this is just a number and we will set this as **1024**.\n",
    "\n",
    "LSTM can be stacked, hence we will pass the `n_layers` as a parameter, however for our initial implementation we will just use **1 layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ead798",
   "metadata": {},
   "source": [
    "### `forward()`\n",
    "\n",
    "The forward function is very straight forward. Notice we are using a **dropout layer** after the **embedding layer**, this is absolutely optional.\n",
    "\n",
    "The encoder is the most simple among rest of the code. Notice we are completely ignorant on the **batch size** and the **time dimension (sentence length)** as both will be taken care dynamically by PyTorch.\n",
    "\n",
    "The Embedding layer uses the `vocab_len` for converting the `input_batch` to one-hot representation internally.\n",
    "\n",
    "Another important point to notice here is, we can feed an entire batch at once to the encoder model. A batch will have the dimension of `[time_dimension, batch_size]`. In PyTorch if donâ€™t pass the `hidden` and `cell` to the RNN module, it will initialize one for us and process the entire batch at once.\n",
    "\n",
    "So the output (`outputs`, `hidden`, `cell`) of the LSTM module is the final output after processing for all the time dimensions for all the sentences in the batch. We do not need the outputs vector from the LSTM, as we need to pass just the **context vector** to the **decoder block**, which consists of the `hidden` and `cell` vector only. Hence letâ€™s return them from the function here.\n",
    "\n",
    "Note: Since we are using LSTM we have the additional **cell state**, however if we are using GRU, we will have only the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a2e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, embedding_dim, hidden_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_prob)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        outputs, (hidden, cell) = self.rnn(embed)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37a795",
   "metadata": {},
   "source": [
    "## Decoder Model using PyTorch\n",
    "\n",
    "Implementation of Decoder needs to be done in **two steps**. Letâ€™s understand more from the diagram below.\n",
    "\n",
    "![Decoder](../../../images/decoder.png)\n",
    "\n",
    "The decoderâ€™s input in a time step **t**, is **dependent** on the output of the previous time step `tâˆ’1`. When `t=0` it will take the **output of the Encoder** as the input for its `initial hidden`, `cell state`.\n",
    "\n",
    "We will first create a Decoder Model just for **one time step of the decoder** and later add a wrapper for the entire time sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa25206",
   "metadata": {},
   "source": [
    "### One Time Step of Decoder\n",
    "\n",
    "The one time step of the decoder looks like the following diagram. Here all we want to implement is one **Embedding Layer**, **LSTM** and **Linear Layer**.\n",
    "\n",
    "![One Time Step Decoder](../../../images/one_step_decoder.png)\n",
    "\n",
    "**Note**: Some of the implementation uses a **LogSoftMax** layer (e.g official PyTorch documentation) after the **Linear layer**. Since we do not need a probability distribution here and can work with the most probable value, we are omitting the use of **LogSoftMax** can will just use the output of the **Linear layer**. The LogSoftMax might be useful in other use cases such as **Beam Search**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ded93",
   "metadata": {},
   "source": [
    "The code for `OneStepDecoder` is very simple to implement. There are however few important points to notice.\n",
    "\n",
    "Since the **output of the Linear layer** will be the **input to the Embedding layer** of the **next time step**, the output dimension should be same as the decoderâ€™s input dimension and target sentences vocabulary size. Here we are naming it as `input_output_dim`.\n",
    "\n",
    "Secondly, the target_token is just one dimensional as we are just passing the previous most probable generated index of the word for all the batches. However as discussed previously, the Embedding layer expects input as `[time_dimension, batch_size]`. Hence call the unsqueeze(0) function just to add an **additional time dimension** as 1.\n",
    "\n",
    "We will take the output of the LSTM and remove this time_dimension before passing it to the **Linear layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDecoder(nn.Module):\n",
    "    def __init__(self, input_output_dim, embedding_dim, hidden_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        # self.input_output_dim will be used later\n",
    "        self.input_output_dim = input_output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, input_output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, target_token, hidden, cell):\n",
    "        target_token = target_token.unsqueeze(0)\n",
    "        embedding_layer = self.dropout(self.embedding(target_token))\n",
    "        output, (hidden, cell) = self.rnn(embedding_layer, (hidden, cell))\n",
    "\n",
    "        linear = self.fc(output.squeeze(0))\n",
    "\n",
    "        return linear, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac6ea0",
   "metadata": {},
   "source": [
    "### Decoder Model\n",
    "\n",
    "Now we are ready to build the **full Decoder model**. First, pass the instance of `OneStepDecoder` in the constructor.\n",
    "\n",
    "The main objective is to call the OneStepDecoder as many times we have the **time dimension in our batch**.\n",
    "\n",
    "So far we have ignored the **Time and Batch dimension** as PyTorch was taking care of that automatically, however now we need get them (`target_len`, `batch_size`) from the `target`.\n",
    "\n",
    "We need to store the `output` of each **Decoder Time Step for each batch**, we created a tensor named `predictions` using PyTorch.\n",
    "\n",
    "Next, take the very first input from the target data (which will be `<sos>`) and pass it along with the **`hidden`** and **`cell`** from the encoder. The `input`, `hidden` and `cell` variable will be overwritten in the consecutive time step.\n",
    "\n",
    "Finally loop through the time step ( remember that each batch may have a **variable number of time sequence and batch size** ) and call the `one_step_decoder`. Store the predicted output to the predictions vector and get the most probable word token by call the `argmax()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba523d",
   "metadata": {},
   "source": [
    "### Teaching Force\n",
    "\n",
    "Now, we will just add one more concept called **Teacher Forcing** in the **Decoder** mode.\n",
    "\n",
    "When one of `OneStepDecoder` **predicts the wrong word**, the next consecutive OneStepDecoder does not learn as it receives the wrong input and the trend continues for the remaining of the tokens in the sequence. This leads to **very slow convergence** of model.\n",
    "\n",
    "One way of addressing this problem is to **randomly provide the correct input to the `OneStepDecoder`**, irrespective of the output from the previous time step. This way we are enforcing the current OneStepDecoder to **learn from correct data**. This leads to **faster convergence**. The process is called as **Teacher Forcing**, since we are intermittently helping the decoder to learn from correct target sequence.\n",
    "\n",
    "Below is the updated version of the previous diagram in order to get an intuition about the idea. As you see all we are doing is randomly choosing between the **previous stepâ€™s output** vs the **actual target**.\n",
    "\n",
    "![Teaching Force](../../../images/teaching_force.png)\n",
    "\n",
    "The code is straightforward, first we want to control how much of teacher forcing to use, hence pass that as a parameter as during inference we wont be using it at all.\n",
    "\n",
    "The following code is part of our Decoder loop for enabling Teacher Forcing. We can pass the `teacher_forcing_ratio` to 0 in order to disable it during inference time.\n",
    "\n",
    "    do_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    \n",
    "    input = target[t] if do_teacher_forcing else input\n",
    "    \n",
    "Notice the `teacher_forcing_ratio` is being passed as an **argument** to the `forward()` method and not to the constructor, so that the value can be changed during the life cycle of the training. We can have more teacher forcing in the beginning of the training, however as training progresses we can reduce the value so that the network can learn by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ea96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, one_step_decoder, device):\n",
    "        super().__init__()\n",
    "        self.one_step_decoder = one_step_decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, target, hidden, cell, teacher_forcing_ratio=0.5):\n",
    "        target_len, batch_size = target.shape[0], target.shape[1]\n",
    "        target_vocab_size = self.one_step_decoder.input_output_dim\n",
    "        # Store the predictions in an array for loss calculations\n",
    "        predictions = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "        # Take the very first word token, which will be sos\n",
    "        input = target[0, :]\n",
    "\n",
    "        # Loop through all the time steps, starts from 1\n",
    "        for t in range(1, target_len):\n",
    "            predict, hidden, cell = self.one_step_decoder(input, hidden, cell)\n",
    "\n",
    "            predictions[t] = predict\n",
    "            input = predict.argmax(1)\n",
    "\n",
    "            # Teacher forcing\n",
    "            do_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            input = target[t] if do_teacher_forcing else input\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7634d95",
   "metadata": {},
   "source": [
    "## Combine Encoder and Decoder\n",
    "\n",
    "The next step will be to **combine the Encoder and Decoder** models. The below diagram shows the model hierarchy. We already have the Encoder and Decoder model, we need to combine them in a model named `EncoderDecoder`.\n",
    "\n",
    "![Encoder Decoder Model](../../../images/encoder-decoder_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d6e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        hidden, cell = self.encoder(source)\n",
    "        outputs = self.decoder(target, hidden, cell, teacher_forcing_ratio)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7c082",
   "metadata": {},
   "source": [
    "# Model Initialization\n",
    "\n",
    "This part is similar to any other PyTorch program. Initialize the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60ce445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(source, target):\n",
    "    # Define the required dimensions and hyper parameters\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 1024\n",
    "    dropout = 0.5\n",
    "\n",
    "    # Instanciate the models\n",
    "    encoder = Encoder(len(source.vocab), embedding_dim, hidden_dim, n_layers=2, dropout_prob=dropout)\n",
    "    one_step_decoder = OneStepDecoder(len(target.vocab), embedding_dim, hidden_dim, n_layers=2, dropout_prob=dropout)\n",
    "    decoder = Decoder(one_step_decoder, device)\n",
    "\n",
    "    model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Makes sure the CrossEntropyLoss ignores the padding tokens.\n",
    "    TARGET_PAD_IDX = target.vocab.stoi[target.pad_token]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TARGET_PAD_IDX)\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e99d1",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "This code is also very generic, except just one part. We will be discarding the first token from the forward pass and also from the target token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iterator, valid_iterator, source, target, epochs=10):\n",
    "    model, optimizer, criterion = create_model(source, target)\n",
    "\n",
    "    clip = 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pbar = tqdm(total=len(train_iterator), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', unit=' batches', ncols=200)\n",
    "\n",
    "        training_loss = []\n",
    "        # set training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop through the training batch\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            # Get the source and target tokens\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, trg)\n",
    "\n",
    "            # reshape the output\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Discard the first token as this will always be 0\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            # Discard the sos token from target\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                epoch=f\" {epoch}, train loss= {round(sum(training_loss) / len(training_loss), 4)}\", refresh=True)\n",
    "            pbar.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Set the model to eval\n",
    "            model.eval()\n",
    "\n",
    "            validation_loss = []\n",
    "\n",
    "            # Loop through the validation batch\n",
    "            for i, batch in enumerate(valid_iterator):\n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(src, trg, 0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = trg[1:].view(-1)\n",
    "\n",
    "                # Calculate Loss\n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "                validation_loss.append(loss.item())\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            epoch=f\" {epoch}, train loss= {round(sum(training_loss) / len(training_loss), 4)}, val loss= {round(sum(validation_loss) / len(validation_loss), 4)}\",\n",
    "            refresh=False)\n",
    "        pbar.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3332485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator, source, target = get_datasets(batch_size=128)\n",
    "model = train(train_iterator, valid_iterator, source, target, epochs=25)\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'source': source.vocab,\n",
    "    'target': target.vocab\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'model/nmt-model-lstm-25.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e5419",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now we will learn how make predictions using **Encoder Decoder Models**. Inference on Seq2Seq models are not as straight forward as other models, hence lets get to that in detail.\n",
    "\n",
    "The hierarchy of the inference model will be bit different. We donâ€™t need the EncoderDecoder and Decoder Model anymore. Here are the high level steps:\n",
    "\n",
    "1. Load the model and vocabulary from the checkpoint file.\n",
    "\n",
    "\n",
    "2. Load the Test (Unseen) dataset.\n",
    "\n",
    "\n",
    "3. Convert each source token to integer values using the vocabulary\n",
    "\n",
    "\n",
    "4. Take the integer value of `<sos>` from the target vocabulary.\n",
    "\n",
    "\n",
    "5. Run the forward pass of the Encoder.\n",
    "\n",
    "\n",
    "6. Use the hidden and cell vector of the Encoder and in loop run the forward pass of the OneStepDecoder until some specified step (say 50) or when `<eos>` has been generated by the model.\n",
    "\n",
    "\n",
    "7. Record the most probable word inside the loop.\n",
    "\n",
    "\n",
    "8. Find the corresponding word from target vocabulary and print in console.\n",
    "\n",
    "![Inference on Machine Translation Model](../../../images/inference_MT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36f28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model, dataset and vocabulary.\n",
    "def load_models_and_test_data(file_name):\n",
    "    test_data = get_test_datasets()\n",
    "    checkpoint = torch.load(file_name)\n",
    "    source_vocab = checkpoint['source']\n",
    "    target_vocab = checkpoint['target']\n",
    "    model = create_model_for_inference(source_vocab, target_vocab)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    return model, source_vocab, target_vocab, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_datasets():\n",
    "    # Download the language files\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # define the tokenizer\n",
    "    def tokenize_de(text):\n",
    "        return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "    # Create the pytext's Field\n",
    "    source = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "    target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "    # Splits the data in Train, Test and Validation data\n",
    "    _, _, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(source, target), test=\"test2016\")\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a83203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_for_inference(source_vocab, target_vocab):\n",
    "    # Define the required dimensions and hyper parameters\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 1024\n",
    "    dropout = 0.5\n",
    "\n",
    "    # Instanciate the models\n",
    "    encoder = Encoder(len(source_vocab), embedding_dim, hidden_dim, n_layers=2, dropout_prob=dropout)\n",
    "    one_step_decoder = OneStepDecoder(len(target_vocab), embedding_dim, hidden_dim, n_layers=2, dropout_prob=dropout)\n",
    "    decoder = Decoder(one_step_decoder, device)\n",
    "\n",
    "    model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417183a",
   "metadata": {},
   "source": [
    "## `predict()`\n",
    "\n",
    "Get the specific example from test dataset using the id and convert the sentence to number of integers using the sentence tokenizer and source vocabulary.\n",
    "\n",
    "Then create a batch of 1 test data using the `unsqueeze()` function.\n",
    "\n",
    "Set the model to **`eval`** mode for inference and call the forward method of the Encoder by passing the tokenized source sentence at one.\n",
    "\n",
    "Create an array named `outputs` in order to store the generated words. Loop through specific number of times (or until end of sentence has been received). Call the `forward` method of `OneStepDecoder` directly.\n",
    "\n",
    "Then find the most probable predicted output and save the corresponding word from the Target vocabulary.\n",
    "\n",
    "Print the ground truth and predicted sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a47c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id, model, source_vocab, target_vocab, test_data, debug=False):\n",
    "    src = vars(test_data.examples[id])['src']\n",
    "    trg = vars(test_data.examples[id])['trg']\n",
    "\n",
    "    # Convert each source token to integer values using the vocabulary\n",
    "    tokens = ['<sos>'] + [token.lower() for token in src] + ['<eos>']\n",
    "    src_indexes = [source_vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Run the forward pass of the encoder\n",
    "    hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # Take the integer value of <sos> from the target vocabulary.\n",
    "    trg_index = [target_vocab.stoi['<sos>']]\n",
    "    next_token = torch.LongTensor(trg_index).to(device)\n",
    "\n",
    "    outputs = []\n",
    "    trg_indexes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Use the hidden and cell vector of the Encoder and in loop\n",
    "        # run the forward pass of the OneStepDecoder until some specified\n",
    "        # step (say 50) or when <eos> has been generated by the model.\n",
    "        for _ in range(30):\n",
    "            output, hidden, cell = model.decoder.one_step_decoder(next_token, hidden, cell)\n",
    "\n",
    "            # Take the most probable word\n",
    "            next_token = output.argmax(1)\n",
    "\n",
    "            trg_indexes.append(next_token.item())\n",
    "\n",
    "            predicted = target_vocab.itos[output.argmax(1).item()]\n",
    "            if predicted == '<eos>':\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(predicted)\n",
    "    if debug:\n",
    "        print(f'Ground Truth    = {\" \".join(trg)}')\n",
    "        print(f'Predicted Label = {\" \".join(outputs)}')\n",
    "\n",
    "    predicted_words = [target_vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bb957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def cal_bleu_score(dataset, model, source_vocab, target_vocab):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        target = vars(test_data.examples[i])['trg']\n",
    "        predicted_words = predict(i, model, source_vocab, target_vocab, dataset)\n",
    "        predictions.append(predicted_words[1:-1])\n",
    "        targets.append([target])\n",
    "\n",
    "    print(f'BLEU Score: {round(bleu_score(predictions, targets) * 100, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f778c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth    = a boston terrier is running on lush green grass in front of a white fence .\n",
      "Predicted Label = a german athlete runs in the grass in front of a white fence .\n",
      "Ground Truth    = a girl in karate uniform breaking a stick with a front kick .\n",
      "Predicted Label = a girl in a karate leotard is using a saw to a toddler .\n",
      "Ground Truth    = three people sit in a cave .\n",
      "Predicted Label = three people are sitting in a hut .\n",
      "Ground Truth    = people standing outside of a building .\n",
      "Predicted Label = people standing outside of a building .\n",
      "BLEU Score: 19.22\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = 'model/nmt-model-lstm-25.pth'\n",
    "model, source_vocab, target_vocab, test_data = load_models_and_test_data(checkpoint_file)\n",
    "predict(1, model, source_vocab, target_vocab, test_data, debug=True)\n",
    "predict(2, model, source_vocab, target_vocab, test_data, debug=True)\n",
    "predict(14, model, source_vocab, target_vocab, test_data, debug=True)\n",
    "predict(20, model, source_vocab, target_vocab, test_data, debug=True)\n",
    "\n",
    "cal_bleu_score(test_data, model, source_vocab, target_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd17f5d",
   "metadata": {},
   "source": [
    "There are some predictions made by our model, which worked nicely. Remember the model hasnâ€™t seen the test data yet, hence it has generalized well for shorter sentences. However, there are also some not so good prediction. We can clearly see that RNN is **suffering from long sentences**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966492fe",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This tutorial provides the implementation details of Machine Translation using Encoder Decoder model with RNN. There are many advancements to this basic RNN model, however itâ€™s probably wise to just add attention mechanism to this network for performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Machine Translation using Recurrent Neural Network and PyTorch](http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-recurrent-neural-network-pytorch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
