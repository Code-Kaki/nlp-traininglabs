{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c36e3d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is to demonstrate the implementation of text classification on news data using machine learning approach. The classifier are required to classify the text data into their corresponding categories in supervised manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b73d4",
   "metadata": {},
   "source": [
    "# What will we accomplish?\n",
    "\n",
    "Steps to implement text classifier in machine learning:\n",
    "\n",
    "> Step 1: Importing Libraries\n",
    "\n",
    "> Step 2: Loading Datasets & Exploratory Data Analysis\n",
    "\n",
    "> Step 3: Text Pre-processing\n",
    "\n",
    "> Step 4: Feature Extraction (Vectorization)\n",
    "\n",
    "> Step 5: Running ML algorithms\n",
    "\n",
    "> Step 6: Grid Search for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2297e",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [Getting Started with BERT](#Getting-Started-with-BERT)\n",
    "\n",
    "    * [Hugging Face Transformers](#Hugging-Face-Transformers)\n",
    "    \n",
    "    * [Generating BERT Embeddings](#Generating-BERT-Embeddings)\n",
    "        * [Import Libraries](#Import-Libraries)\n",
    "        * [Download Pre-trained BERT Model](#Download-Pre-trained-BERT-Model)\n",
    "        * [Preprocessing the Input](#Preprocessing-the-Input)\n",
    "        * [Getting the Embedding](#Getting-the-Embedding)\n",
    "\n",
    "\n",
    "* [Fine-tuning BERT for Downstream Tasks](#Fine-tuning-BERT-for-Downstream-Tasks)\n",
    "\n",
    "    * [Text Classification](#Text-Classification)\n",
    "        * [Import the Dependencies](#Import-the-Dependencies)\n",
    "        * [Loading the Model and Dataset](#Loading-the-Model-and-Dataset)\n",
    "        * [Train-Test Split](#Train-Test-Split)\n",
    "        * [Download and Load Pre-trained Model](#Download-and-Load-Pre-trained-Model)\n",
    "        * [Preprocess the Dataset](#Preprocess-the-Dataset)\n",
    "        * [Training the Model](#Training-the-Model)    \n",
    "        \n",
    "    * [Q&A with Finetuned BERT](#Q&A-with-Finetuned-BERT)\n",
    "        * [Import Dependencies](#Import-Dependencies)\n",
    "        * [Load Pre-trained Model](#Load-Pre-trained-Model)\n",
    "        * [Preprocessing the Input](#Preprocessing-the-Input)\n",
    "        * [Getting the Answer](#Getting-the-Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9f495",
   "metadata": {},
   "source": [
    "# Getting Started with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89cfce",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers\n",
    "\n",
    "**Hugging Face** is an organization that is on the path of democratizing AI through natural language. Their open source transformers library is very popular among the **Natural Language Processing (NLP)** community. It is very useful and powerful for several NLP and **Natural Language Understanding (NLU)** tasks. It includes **thousands of pre-trained models** in more than 100 languages. One of the many advantages of the transformer's library is that it is compatible with both PyTorch and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3a0d0",
   "metadata": {},
   "source": [
    "## Generating BERT Embeddings\n",
    "\n",
    "In this section, we will learn how to extract embeddings from the pre-trained BERT model. Consider the sentence *I love Paris*. Let's see how to obtain the contextualized word embedding of all the words in the sentence using the pre-trained BERT model with Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c714767",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04af23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16908e69",
   "metadata": {},
   "source": [
    "### Download Pre-trained BERT Model\n",
    "\n",
    "Next, we download the pre-trained BERT model. We can check all the available pre-trained BERT models [here](https://huggingface.co/transformers/pre-trained_models.html). We use the `'bert-base-uncased'` model. As the name suggests, it is the BERT-base model with 12 encoders and it is trained with uncased tokens. Since we are using BERTbase, the representation size will be 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2627765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da7722",
   "metadata": {},
   "source": [
    "Next, we download and load the tokenizer that was used to pre-train the bert-baseuncased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5781284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191990d",
   "metadata": {},
   "source": [
    "### Preprocessing the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9403b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence\n",
    "sentence = \"I love Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6752eb",
   "metadata": {},
   "source": [
    "Tokenize the sentence and obtain the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfea97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f8f07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'paris']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9b042",
   "metadata": {},
   "source": [
    "Add the `[CLS]` token at the beginning and the `[SEP]` token at the end of the `tokens` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aadb70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb79ced",
   "metadata": {},
   "source": [
    "Say we need to keep the length of our `tokens` list to 7; in that case, we add two `[PAD]` tokens at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ed4ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c20b4",
   "metadata": {},
   "source": [
    "Next, we create the attention mask. We set the attention mask value to 1 if the token is not a `[PAD]` token, else we set the attention mask to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6587a3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i != '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeecff56",
   "metadata": {},
   "source": [
    "As we can see, we have attention mask values 0 at positions where have a `[PAD]` token and 1 at other positions.\n",
    "\n",
    "\n",
    "Next, we convert all the tokens to their token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f33343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3000, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531a9ea",
   "metadata": {},
   "source": [
    "From the output, we can observe that each token is mapped to a unique token ID.\n",
    "\n",
    "\n",
    "Now, we convert `token_ids` and `attention_mask` to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20869d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88540093",
   "metadata": {},
   "source": [
    "Next, we feed token_ids and attention_mask to the pre-trained BERT model and get the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70d88e",
   "metadata": {},
   "source": [
    "### Getting the Embedding\n",
    "\n",
    "As shown in the following code, we feed `token_ids` and `attention_mask` to model and get the embeddings. Note that model returns the output as a **tuple** with two values. The first value indicates the hidden state representation, `hidden_rep`, and it consists of the representation of all the tokens obtained from the final encoder (encoder 12), and the second value, `cls_head`, consists of the representation of the `[CLS]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc4f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_rep, cls_head = model.forward(input_ids=token_ids, attention_mask=attention_mask, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dde23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0719,  0.2163,  0.0047,  ..., -0.5865,  0.2262,  0.1981],\n",
      "         [ 0.2236,  0.6536, -0.2294,  ..., -0.3547,  0.5517, -0.2367],\n",
      "         [ 1.0410,  0.7755,  1.0335,  ..., -0.5621,  0.5218, -0.0852],\n",
      "         ...,\n",
      "         [ 0.6156,  0.1036, -0.1875,  ..., -0.3799, -0.7008, -0.3500],\n",
      "         [ 0.0791,  0.4287,  0.4147,  ..., -0.2417,  0.2403,  0.0378],\n",
      "         [-0.0165,  0.2459,  0.4566,  ..., -0.2179,  0.1876,  0.0228]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "\n",
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embedding (representation) of all the tokens in our input\n",
    "print(hidden_rep, hidden_rep.shape, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8b00a",
   "metadata": {},
   "source": [
    "The size [1, 7, 768] indicates [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "We can obtain the representation of each token as follows:\n",
    "* `hidden_rep[0][0]` gives the representation of the first token, which is `[CLS]`.\n",
    "* `hidden_rep[0][1]` gives the representation of the second token, which is I.\n",
    "* `hidden_rep[0][2]` gives the representation of the third token, which is love.\n",
    "\n",
    "Therefore, we can obtain the contextual representation of all the tokens. This is the contextualized word embeddings of all the words in the given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b04828b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.1920e-02,  2.1631e-01,  4.7180e-03, -8.1534e-02, -3.0399e-01,\n",
       "        -2.6997e-01,  3.6993e-01,  4.3028e-01,  1.1932e-02, -2.0674e-01,\n",
       "        -8.9630e-02, -1.3917e-01,  1.7530e-01,  4.8318e-01,  3.0506e-01,\n",
       "        -5.9535e-03, -1.7049e-01,  4.9769e-01,  4.6345e-01, -1.6272e-01,\n",
       "         2.8591e-02, -2.6006e-01, -3.3321e-01, -8.1934e-02, -8.8632e-02,\n",
       "        -3.5845e-01, -1.2788e-01, -7.6149e-02,  3.1540e-01, -1.5370e-02,\n",
       "         2.4448e-01,  7.5998e-02, -6.1328e-02,  1.8551e-01,  2.3354e-01,\n",
       "        -5.2519e-02,  3.3775e-01, -1.0754e-01, -3.2548e-02,  2.1909e-01,\n",
       "         1.7896e-01, -8.9923e-03,  2.1548e-01, -4.8307e-02,  2.7949e-01,\n",
       "        -2.8501e-01, -1.8575e+00, -3.7983e-02, -6.7010e-02, -2.6804e-01,\n",
       "         2.5982e-01, -9.3902e-02,  4.1909e-01,  3.3008e-01,  5.1305e-02,\n",
       "         2.5632e-01, -3.9642e-01,  6.5480e-01,  1.2961e-01,  3.6180e-01,\n",
       "         1.5786e-01,  1.1038e-03, -1.5318e-01,  3.4398e-02, -1.8015e-01,\n",
       "         2.6369e-01,  3.7324e-02,  2.1566e-01, -3.7156e-02,  2.9641e-01,\n",
       "        -1.0159e-01, -1.0149e-01,  2.2780e-01, -3.1291e-02, -2.2098e-01,\n",
       "        -2.1771e-01, -1.7454e-01,  3.4137e-02, -1.0499e-01, -1.5992e-01,\n",
       "         2.2057e-01,  2.2134e-01,  1.1054e-01,  2.8978e-01,  2.3832e-01,\n",
       "         4.9259e-01, -4.1767e-01, -4.4610e-01,  4.1602e-01,  2.0261e-01,\n",
       "        -3.8862e-01, -7.4976e-02, -1.5559e-01,  4.7930e-01,  2.7494e-01,\n",
       "        -1.4979e-01, -2.3028e-01,  1.5289e-01,  1.4343e-02,  3.1676e-01,\n",
       "        -6.9211e-02,  1.2347e-01,  2.4580e-01, -2.5904e-01,  3.8756e-02,\n",
       "        -1.3979e-02, -4.0968e-02, -1.3726e-01,  5.3324e-01, -3.0292e+00,\n",
       "         1.5335e-01,  1.9361e-02, -2.7532e-01, -1.3892e-01, -1.9138e-01,\n",
       "         5.6742e-01,  4.3964e-01, -6.3729e-02, -1.6237e-01,  1.4878e-01,\n",
       "        -2.8738e-01,  4.5703e-01,  3.8015e-02, -8.9919e-02,  1.1129e-01,\n",
       "         2.1732e-01,  2.3429e-01, -2.4718e-01,  8.9462e-02,  2.4086e-01,\n",
       "         3.4622e-02,  4.9634e-01, -2.0499e-01, -3.9347e-01, -1.0094e-01,\n",
       "         9.7377e-03,  3.6877e-01, -7.0591e-02, -3.0286e-02, -1.3559e-01,\n",
       "        -2.1845e-01, -3.3528e-02, -3.3769e+00,  2.2325e-01,  5.6023e-01,\n",
       "         2.2025e-02, -3.4522e-01, -1.1284e-01,  2.0824e-01,  2.2083e-02,\n",
       "         1.5656e-01, -7.7579e-02, -6.5006e-02,  3.1331e-01, -4.4251e-01,\n",
       "        -2.8162e-01, -6.0462e-02, -3.9692e-02,  3.1270e-01,  2.8261e-01,\n",
       "         7.4946e-02, -1.6038e-01,  2.7198e-01, -8.2149e-02, -4.1589e-01,\n",
       "         1.1507e-01,  3.9943e-01,  1.5294e-01,  1.7692e-01, -1.3174e-01,\n",
       "        -2.5276e-01,  2.1237e-02,  1.8457e-01,  2.4213e-01,  3.4318e-01,\n",
       "        -3.8068e-01, -1.9973e-02,  3.1037e-01, -1.4330e-01,  1.0105e-01,\n",
       "        -1.0447e-02,  1.8389e-01,  1.4408e-02,  6.7894e-02,  1.5822e-01,\n",
       "         1.2886e-01,  3.7850e-01, -1.6640e-01, -2.2876e-01,  4.0455e-01,\n",
       "         5.0134e-02,  9.4934e-02,  6.8950e-02,  1.8718e-01,  4.6107e-01,\n",
       "        -1.5952e-01,  7.9853e-02, -5.2728e-01,  3.5441e-01,  3.5842e-01,\n",
       "        -8.1287e-02, -1.8307e-01,  3.6243e-02,  2.3830e-01,  1.7185e-01,\n",
       "         3.6390e+00,  9.0289e-02, -2.1140e-01,  2.2297e-01,  2.7892e-01,\n",
       "        -1.4007e-01,  1.5690e-01, -2.1330e-02, -7.1896e-02, -6.1043e-02,\n",
       "        -2.4204e-01,  2.4583e-01, -3.8148e-02, -1.9057e-01,  1.3859e-01,\n",
       "         2.5276e-01,  2.5328e-01, -9.8204e-02,  1.1605e-02, -8.9290e-02,\n",
       "         4.8514e-02, -6.2467e-03,  1.5648e-01, -2.4296e-01, -9.8586e-01,\n",
       "        -8.1815e-02, -2.0363e-01, -5.4936e-01,  3.6241e-01, -2.8716e-01,\n",
       "         6.7122e-02, -2.5417e-01, -4.8720e-01,  1.2639e-01,  2.9214e-01,\n",
       "         2.6864e-01, -7.5652e-02,  2.6110e-01,  2.3721e-01, -2.0160e-01,\n",
       "         3.8724e-01,  1.1523e-01, -8.2917e-02,  1.1748e-01, -1.3464e-01,\n",
       "         4.2640e-01, -1.9114e-01, -9.4061e-02, -1.8349e-01, -1.0604e-01,\n",
       "         1.3748e-01,  1.3736e-01,  1.4987e-01, -3.8054e-01,  4.7658e-03,\n",
       "        -3.1677e-01,  4.7421e-02,  4.7324e-01,  4.0897e-02, -5.3537e-02,\n",
       "        -1.0757e-01, -1.5435e-01, -4.4836e-01,  1.2407e-01, -3.3533e-02,\n",
       "        -2.5904e-01, -1.0940e-01, -2.6489e-02, -4.5658e+00,  1.3760e-01,\n",
       "         4.4588e-01, -6.4315e-02,  2.7092e-01,  1.2436e-01,  1.4365e-01,\n",
       "         3.8417e-01,  1.1392e-01, -4.4748e-01,  3.0691e-01, -5.1140e-02,\n",
       "        -2.4310e-02,  1.2401e-01, -3.2062e-01,  2.0383e-01,  1.5906e-01,\n",
       "         7.8306e-02, -3.0065e-01, -8.6556e-02,  2.1684e-01,  2.1188e-01,\n",
       "        -3.3717e-01,  1.6344e-01,  3.1737e-02, -2.0693e-01, -1.1014e-01,\n",
       "        -1.7839e-01,  1.6027e-01, -2.5965e-01,  1.3198e-01,  7.6645e-02,\n",
       "        -4.5259e-02,  1.1285e-01, -6.0865e-02, -1.8924e+00,  3.1132e-02,\n",
       "        -1.2720e-01, -1.7087e-01, -1.4933e-02, -2.2994e-01,  3.3450e-01,\n",
       "         1.4140e-02, -1.1474e-01,  2.2014e-02,  1.3599e-01,  1.5196e-01,\n",
       "        -5.6655e-02,  2.2857e-01, -3.0145e-02,  1.4506e-02,  2.1584e-01,\n",
       "         9.4243e-02,  9.9624e-03,  9.7372e-02, -1.8241e-01,  4.5774e-01,\n",
       "         5.9424e-02, -1.0023e-01, -2.4868e-02,  5.1680e-01, -1.8573e-01,\n",
       "        -1.2511e-01, -2.9155e-01,  1.2167e-01, -1.1441e-01,  1.2527e-01,\n",
       "         1.3521e-01, -3.3497e-01, -2.8952e-01, -1.6202e-01,  2.8342e-01,\n",
       "         1.2046e-01,  3.3227e-01,  1.3041e-01, -2.0423e-01,  3.8298e-01,\n",
       "        -2.2314e-02,  3.4789e-01,  3.7139e-01,  7.0134e-02, -2.6471e-01,\n",
       "        -1.0594e-01,  2.0483e-01,  1.5908e-01, -8.6156e-02,  3.4881e-02,\n",
       "         1.1424e+00, -3.2981e-02,  3.3570e-02, -9.8300e-02,  4.3245e-01,\n",
       "        -4.7516e-02,  2.8329e-01,  1.2895e-01,  5.1757e-01,  1.1298e-01,\n",
       "         1.6417e-01,  1.2000e-01,  2.4257e-01, -3.3287e-01,  2.5652e-01,\n",
       "        -5.4119e-01, -2.2151e-01,  3.6409e-01, -1.6067e-02, -1.3111e-02,\n",
       "        -1.7314e-01, -6.3513e-01, -3.1411e-01,  3.1078e-01, -2.0938e-01,\n",
       "         6.6951e-02,  2.5844e-01, -2.3611e-01,  3.8946e-02,  2.9757e-02,\n",
       "        -3.1428e-02,  2.2566e-01, -2.5382e-01, -4.1613e-02, -8.4577e-02,\n",
       "         5.3319e-02, -2.2533e-01,  1.6836e-01, -2.9121e-01,  2.2587e-01,\n",
       "         9.3202e-02,  2.1438e-02, -4.0248e-02, -5.8573e-03,  4.5277e-01,\n",
       "        -9.0513e-01, -5.8709e-02, -2.9747e-01, -2.7868e-01, -2.4352e-01,\n",
       "        -1.9931e-01,  1.3347e-01, -8.9147e-02, -1.5944e-01, -7.8239e-02,\n",
       "         4.2049e-01, -9.3481e-02,  2.9669e-01, -7.3147e-02,  9.2810e-02,\n",
       "         5.0954e-02,  1.0311e-01,  7.2915e-01, -1.6780e-01,  1.3015e-01,\n",
       "         4.8504e-01,  3.2287e-02,  2.8090e-01,  1.0165e-01,  1.4419e-01,\n",
       "         1.8340e-01, -1.1009e-01, -3.9004e-01, -3.1668e-02,  7.3039e-02,\n",
       "        -4.3114e-01, -4.0612e-01, -1.7548e-01, -9.1505e-03, -1.3609e-01,\n",
       "        -2.7555e-01, -6.5322e-01, -3.2615e-01, -2.2488e-01, -3.5946e-01,\n",
       "         1.0733e-01,  3.2005e-01, -4.8575e-03, -1.2712e-01,  1.5202e-03,\n",
       "        -1.9316e-01,  5.0541e-01, -4.7586e-02,  3.5730e-01, -2.3380e-02,\n",
       "        -5.6561e-02, -1.3984e-01,  2.4792e-01, -7.1496e-03, -2.6959e-01,\n",
       "         3.9159e-01, -3.4671e-01,  1.5862e-01, -2.3871e-01,  6.1375e-02,\n",
       "        -1.1259e-01,  2.9662e-01,  1.0288e-01, -5.4371e-02,  8.6823e-02,\n",
       "        -1.1151e+00,  4.7860e-01,  3.5550e-01, -8.6611e-02,  3.4973e-01,\n",
       "        -2.3870e-01, -1.9610e-01,  3.2064e-01, -6.5215e-02,  1.4353e-01,\n",
       "        -8.6778e-03, -1.0079e-01, -2.4458e-01,  1.3018e-01,  5.6466e-02,\n",
       "         2.0525e-01,  2.2732e-01, -1.9637e-01, -8.1522e-02,  1.1559e-01,\n",
       "        -1.1821e-01,  4.7548e-01, -1.4465e-01, -1.5668e-01, -1.8621e-01,\n",
       "        -2.6867e-01,  1.3801e-01,  4.4069e-01, -9.8198e-03,  2.4961e-01,\n",
       "         7.7968e-02, -4.3461e-01, -7.5157e-01, -2.9894e-01, -2.5836e-02,\n",
       "        -2.2586e-01,  3.2253e-01,  6.5692e-02,  2.8023e-01,  3.0000e-01,\n",
       "        -1.8985e-01,  4.7286e-01,  2.6957e-01, -2.1993e-01,  2.4988e-01,\n",
       "         3.7612e-01, -1.5289e-01,  3.5296e-01,  2.3264e-01, -3.9820e-01,\n",
       "        -7.5369e-02,  1.4702e-01,  1.7145e-01, -2.4918e-01,  1.7875e-01,\n",
       "        -1.6057e-01, -1.2924e-02, -6.9197e-03, -4.7353e-02, -1.0608e-01,\n",
       "         1.4446e-01, -3.3367e-01, -2.9582e-01,  4.4504e-01, -4.8908e-01,\n",
       "        -4.0565e-01, -6.3208e-02, -2.1249e-01, -6.9111e-02,  3.6526e-01,\n",
       "         3.1471e-01, -3.6971e-03, -1.6240e-02,  1.1703e-02, -2.7503e-01,\n",
       "        -5.8359e-02,  5.5184e-02,  2.2002e-01,  1.4365e-01, -2.3213e-01,\n",
       "         1.1930e-01, -4.1597e-01, -2.7510e-01,  1.0264e-01, -1.0298e-02,\n",
       "         1.3058e-01, -2.8003e-02, -1.4401e-01, -2.6905e-01, -1.6099e-01,\n",
       "        -1.9162e-01, -4.4446e-01,  5.3204e-01,  1.2685e-01,  1.6826e-02,\n",
       "        -5.6685e-02,  1.9315e-01,  9.1423e-02, -1.3183e-01,  3.6546e-02,\n",
       "        -1.8706e-01,  5.0915e-01,  2.8731e-01,  1.4674e-01,  4.2552e-01,\n",
       "         2.5227e-01,  2.3511e-01,  2.7884e-01, -3.6146e-01, -2.5480e-01,\n",
       "        -1.7897e-01,  2.0431e-01, -1.9949e-01, -1.9732e-01,  2.5785e-01,\n",
       "        -7.4531e-02,  2.8320e-01, -2.0079e-01,  2.4464e+00,  4.5186e-01,\n",
       "         2.5414e-01, -2.2750e-01,  3.3208e-01,  1.2118e-01, -2.8476e-01,\n",
       "        -8.3928e-02, -4.0313e-01,  2.8980e-01,  2.1727e-02,  1.9596e-02,\n",
       "         8.0200e-02,  2.6017e-01,  3.9976e-01,  1.4162e-01, -2.6691e-01,\n",
       "        -2.4128e-01, -4.5518e-01, -3.3106e-02, -4.4964e-01,  4.5196e-01,\n",
       "         1.7941e-01, -1.2177e-01,  1.5660e-01,  2.1685e-03, -1.3234e-01,\n",
       "        -1.3851e-01, -1.2365e-02,  6.5952e-02, -1.1538e-01, -1.4826e-01,\n",
       "         1.3182e-01,  3.6363e-01, -2.1129e-02,  3.0922e-01,  1.3873e-01,\n",
       "        -3.3244e-01, -2.1274e-01,  7.3462e-02, -3.0817e-01, -4.5410e-01,\n",
       "         4.3248e-01,  1.8296e-02,  1.6110e-01,  3.0801e-01, -2.0656e-01,\n",
       "        -1.7136e-02,  3.2579e-01, -5.8657e-02, -1.6500e-01, -2.1732e-01,\n",
       "        -2.8289e-01,  1.1546e-01, -3.2717e-01, -7.1849e-02, -7.1623e-02,\n",
       "        -2.6504e-01, -1.9374e-01,  2.4515e-01,  9.9459e-02,  1.6781e-01,\n",
       "         3.6243e-02, -5.5762e-02,  5.1508e-02, -1.9876e-01, -3.0607e-01,\n",
       "         1.3402e-01,  9.6509e-02, -4.3372e-01, -1.0380e-01,  4.5016e-01,\n",
       "         4.1506e-02, -2.9332e-01,  1.5389e-01, -5.3551e-02,  3.2837e-01,\n",
       "        -1.7775e-01,  8.6972e-02, -3.7028e+00, -2.5702e-01, -2.5950e-01,\n",
       "        -8.6591e-02,  1.6411e-01,  2.7960e-01,  2.1380e-02,  2.3375e-01,\n",
       "        -5.9884e-02, -3.1606e-01,  1.3072e-01,  3.0361e-01,  1.3923e-01,\n",
       "         2.7389e-01,  1.8263e-01,  3.5823e-01,  1.8555e-01, -4.0123e-01,\n",
       "        -3.2070e-01, -4.3406e-02, -1.3346e-01, -8.1946e-02, -3.4063e-02,\n",
       "         4.5781e-02, -3.1390e-01,  1.3019e-01, -1.9540e-02, -2.9165e-01,\n",
       "        -1.6745e-01,  2.3763e-01, -2.7336e-01,  4.5229e-01, -3.2311e-02,\n",
       "         2.1072e-01, -1.0338e-01, -4.2971e-02, -2.2648e-01, -1.9904e-01,\n",
       "         2.4757e-01, -4.2875e-02,  1.1200e-01,  4.2492e-01,  3.0052e-02,\n",
       "         8.8812e-02, -2.1316e-01, -3.0072e-02,  3.1105e-01, -1.5331e-01,\n",
       "         5.5415e-01, -1.4675e-01, -1.6847e-02,  6.0564e-02, -4.0230e-02,\n",
       "        -7.5879e-02,  2.6141e-01,  1.3437e-01,  9.4423e-02,  5.8480e-02,\n",
       "         1.2569e-01, -3.4157e-01,  1.6329e-01,  9.7568e-02,  2.9379e-02,\n",
       "         2.2206e-01,  1.4146e-01, -2.4696e-01, -6.8976e-02, -1.5553e-01,\n",
       "        -7.7449e-02,  1.7090e-01, -2.4685e-01, -6.8907e-02,  2.7183e-01,\n",
       "         8.2355e-02,  7.2382e-02,  1.0585e-01,  3.3916e-01,  4.4341e-01,\n",
       "         1.2001e-01,  1.7591e-01,  6.8762e-03, -5.1443e-02, -1.3779e-01,\n",
       "        -4.6466e-02,  1.6583e-01, -9.2295e+00, -9.1583e-02, -2.8230e-01,\n",
       "        -1.2691e-01,  2.8261e-01, -1.6951e-01,  4.5645e-02, -3.5348e-01,\n",
       "         1.6200e-01,  1.3207e-01,  3.8282e-01, -8.0000e-02, -3.4472e-03,\n",
       "        -5.8645e-01,  2.2624e-01,  1.9810e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_rep[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0ef9a",
   "metadata": {},
   "source": [
    "Now, let's take a look at `cls_head`. It contains the representation of the `[CLS]` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea785f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(cls_head.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f629b",
   "metadata": {},
   "source": [
    "The size [1, 768] indicates [batch_size, hidden_size]\n",
    "\n",
    "We learned that `cls_head` holds the **aggregate representation**, so we can use `cls_head` as the representation of the sentence *I love Paris*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ea187",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for Downstream Tasks\n",
    "\n",
    "So far, we have learned how to use the pre-trained BERT model. Now, let's learn how to fine-tune the pre-trained BERT model for downstream tasks. Note that fine-tuning implies that we are not training BERT from scratch; instead, we are using the pre-trained BERT and updating its weights according to our task.\n",
    "\n",
    "In this section, we will learn how to fine-tune the pre-trained BERT model for the following downstream tasks:\n",
    "* Text classification (Sentiment Analysis)\n",
    "* Question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7c16c",
   "metadata": {
    "id": "C7QsO2hzfowo"
   },
   "source": [
    "## Text Classification \n",
    "\n",
    "Let's learn how to finetune the pre-trained BERT for text classification tasks. Say, we are performing sentiment analysis. In the sentiment analysis, our goal is to classify whether a sentence is positive or negative. Suppose, we have a dataset containing sentences along with their labels. \n",
    "\n",
    "Consider a sentence: 'I love Pairs'. First, we tokenize the sentence, add the [CLS] token at the beginning, and [SEP] token at the end of the sentence. Then, we feed the tokens as an input to the pre-trained BERT and get the embeddings of all the tokens. \n",
    "\n",
    "Next, we ignore the embedding of all other tokens and take only the embedding of [CLS] token which is $R_{[CLS]}$. The embedding of the [CLS] token will hold the aggregate representation of the sentence. We feed $R_{[CLS]}$ to a classifier (feed-forward network with softmax function) and train the classifier to perform sentiment analysis. \n",
    "\n",
    "Wait! How does it differ from what we saw at the beginning of the section. How finetuning the pre-trained BERT differs from using the pre-trained BERT as a feature extractor?\n",
    "\n",
    "In \"Extracting embeddings from pre-trained BERT\" section, we learned that after extracting the embedding $R_{[CLS]}$ of a sentence, we feed the $R_{[CLS]}$ to a classifier and train the classifier to perform classification. Similarly, during finetuning, we feed the embedding of $R_{[CLS]}$ to a classifier and train the classifier to perform classification.\n",
    "\n",
    "The difference is that when we finetune the pre-trained BERT, we can update the weights of the pre-trained BERT along with a classifier. But when we use the pre-trained BERT as a feature extractor, we can update only the weights of a classifier and not the pre-trained BERT. \n",
    "\n",
    "During finetuning, we can adjust the weights of the model in the following two ways:\n",
    "\n",
    "- Update the weights of the pre-trained BERT along with the classification layer \n",
    "- Update only the weights of the classification layer and not the pre-trained BERT. When we do this, it becomes the same as using the pre-trained BERT as a feature extractor\n",
    "\n",
    "The following figure shows how we finetune the pre-trained BERT for the sentiment analysis task:\n",
    "\n",
    "\n",
    "![title](../../../images/text-clf-BERT.jpg)\n",
    "\n",
    "As we can observe from the preceding figure, we feed the tokens to the pre-trained BERT and get the embedding of all the tokens. We take the embedding of [CLS] token and feed it to a feedforward network with a softmax function and perform classification. \n",
    "\n",
    "Let's get a better understanding of how finetuning works by getting hands-on with finetuning the pre-trained BERT for sentiment analysis in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cdc58",
   "metadata": {
    "id": "sQcW-cj9fowz"
   },
   "source": [
    "## Finetuning BERT for Sentiment Analysis \n",
    "Let's explore how to finetune the pre-trained BERT for a sentiment analysis task with the IMDB dataset. The IMDB dataset consists of movie reviews along with the respective sentiment. You should have the dataset in the `dataset` folder.\n",
    "\n",
    "Import the dependencies \n",
    "\n",
    "First, let's install the necessary libraries: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82b30c",
   "metadata": {},
   "source": [
    "### Import the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6af391a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from nlp import load_dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7bed02",
   "metadata": {},
   "source": [
    "### Loading the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31b0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='../../../resources/day_12/imdbs.csv', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fe1aa",
   "metadata": {
    "id": "eIVEcgbVfow6"
   },
   "source": [
    "\n",
    "Let us check the datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d57a2b62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-AySl-3fow7",
    "outputId": "3cfffc1d-85fe-4c0e-8e6b-02e97363ff4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlp.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30599e46",
   "metadata": {
    "id": "4zfFf433fow8"
   },
   "source": [
    "\n",
    "Next, let's split the dataset into train and test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c65ae",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d5706a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "516d4c06c201407fb9cda2275754a75a",
      "b0ac69e6604c4432b8c902e9de90ef24",
      "c8de8c830382483689b488143e12e032",
      "5493eb48f1444bf7ab0f8fbce84bd7c9",
      "e3a70370752e46ab989e4226df61051a",
      "e17f9de2dd074038b4924a5766645def",
      "677ec2f86c284108901b89c417395208",
      "1c774005109d45f79927341b16af8acd",
      "b5a12a00dfc141c2a542ffb4b2a6ec89",
      "d4517e94173b46a999a109fc7c508c73",
      "48bfa3980aef4c0d86b5e81c60d880b0",
      "532e132bcab54f86bb531e972849d721",
      "c540cf89aac2428d92d7cc0d99da7b12",
      "62406d3a5a544557acfc42a09709b136",
      "8a21de3ce41349f99587b4fa8d792f9a",
      "f26af917c7bf47d491ad8836269626f5"
     ]
    },
    "id": "MYwse-FTfow9",
    "outputId": "d4cfe55b-f0a2-492d-c831-2b27c017b371"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3519c",
   "metadata": {
    "id": "NGrz_f_cfow-"
   },
   "source": [
    "\n",
    "Let's print the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8180a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vd-gEqVcfow_",
    "outputId": "f8cdc898-10c7-4eee-9a8b-9836003f7bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 70),\n",
       " 'test': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 30)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33191f",
   "metadata": {
    "id": "17VmhoVKfoxA"
   },
   "source": [
    "\n",
    "Now, we create the  train and test sets:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "466b6dd4",
   "metadata": {
    "id": "zv5T9bX4foxA"
   },
   "outputs": [],
   "source": [
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c128fe",
   "metadata": {
    "id": "mGi62VtxfoxA"
   },
   "source": [
    "\n",
    "Next, let's download and load the pre-trained BERT model. In this example, we use the pre-trained bert-base-uncased model. As we can observe below, since we are performing sequence classification, we use the BertForSequenceClassification class: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcbe38",
   "metadata": {},
   "source": [
    "### Download and Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9595514",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "beaf2c9bccbf4e31a7996086a01f4f2d",
      "d71e10934f0c42c390128db32cbef024",
      "a8756cff3b5f494e83835bfc26f4ede0",
      "6ca4219854824f768eb2c9d3e76bf242",
      "7919b0395fd4426b9078f0f5338156e9",
      "436c82119ac3479cba0abebab0b4c148",
      "8a600d325aa142288ac2a5eebdfa8bc5",
      "b07d4bda9e114ba8b0fff74192e854f9",
      "f72eb680ea5a4efe83fcfc26276e86a6",
      "60ac8a0a19d04cba9fb1ead13333c916",
      "6d406280ef5b4231804ba097688f70a4",
      "14335ebb7b5a491ea802859eb13dc2fe",
      "7e1ca2a45aae4c6185d0efecca097dca",
      "727bb8d64ad8423cbdf417fae05ba35b",
      "08750cf9511d4ddfa4ce2f4320b42a02",
      "2609fc2778c24e40a4afcd86bfd96de2"
     ]
    },
    "id": "yNoC_WX2foxB",
    "outputId": "dbea248c-cfe4-468f-862e-0439380296bd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd8846",
   "metadata": {
    "id": "YW5wD22HfoxC"
   },
   "source": [
    "\n",
    "Next, we download and load the tokenizer which is used for pretraining the bert-base-uncased model.\n",
    "As we can observe, we create the tokenizer using the BertTokenizerFastclass instead of BertTokenizer. The BertTokenizerFast class has many advantages compared to BertTokenizer. We will learn about this in the next section: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6980a4cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "59e06d71c4804622b516265439f425ac",
      "0e00d7accc254270978f488e700f36dd",
      "f91f1d236e974e48a5b4ed03da1a2fc9",
      "9fe7874b450f4c57a6cf1454d782c5a2",
      "8e8426c76508436fa3bcc5e094109759",
      "a2e9f57a961943f3ba35ef88aa18be11",
      "85e3c9227f9d41d5850fad2b0bbbbe2a",
      "868f2625db62418a82f6002536b67746",
      "6c4b1a64780b4ff9a12fb9438825e9e7",
      "63eff4fc4c834b1297dd42a6bf8c88af",
      "08a117baf95c4de79c40954fcac26c2f",
      "011df17f256c47a59c4ded780e861940",
      "8d28bf50f43544fb92db0a60d1a8e9db",
      "61932d9012ab4364adc1c9aac108c06a",
      "37483f8b9545424383692a3cb1b20f54",
      "72fe04a8bd494983b8eaf33f6f3e7404"
     ]
    },
    "id": "szVRFKRpfoxC",
    "outputId": "a420b920-0712-49b3-e08f-16004324690c"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85f851",
   "metadata": {
    "id": "i95cBPtjfoxC"
   },
   "source": [
    "\n",
    "Now that we loaded the dataset and model, next let's preprocess the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336acce",
   "metadata": {
    "id": "poYE4BPPfoxD"
   },
   "source": [
    "### Preprocess the Dataset\n",
    "We can preprocess the dataset in a quicker way using our tokenizer. For example, consider the sentence: 'I love Paris'.  \n",
    "\n",
    "First, we tokenize the sentence and add the [CLS] token at the beginning and [SEP] token at the end as shown below: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649ebc7",
   "metadata": {
    "collapsed": true,
    "id": "dHfQQv6pfoxD"
   },
   "source": [
    "tokens = [ [CLS], I, love, Paris, [SEP] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f07fc",
   "metadata": {
    "id": "pHj9t1KrfoxE"
   },
   "source": [
    "\n",
    "Next, we map the tokens to the unique input ids (token ids). Suppose the following are the unique input ids (token ids):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94f863",
   "metadata": {
    "collapsed": true,
    "id": "FzduZUwOfoxE"
   },
   "source": [
    "input_ids = [101, 1045, 2293, 3000, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723fbce",
   "metadata": {
    "id": "CJpZf_A5foxE"
   },
   "source": [
    "Then, we need to add the segment ids (token type ids). Wait, what are segment ids? Suppose we have two sentences in the input. In that case, segment ids are used to distinguish one sentence from the other. All the tokens from the first sentence will be mapped to 0 and all the tokens from the second sentence will be mapped to 1. Since here we have only one sentence, all the tokens will be mapped to 0 as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c132f1",
   "metadata": {
    "collapsed": true,
    "id": "sLmIG0k3foxF"
   },
   "source": [
    "token_type_ids = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ead67",
   "metadata": {
    "id": "ai4H-92NfoxG"
   },
   "source": [
    "\n",
    "Now, we need to create the attention mask. We know that an attention mask is used to differentiate the actual tokens and [PAD] tokens. It will map all the actual tokens to 1 and the [PAD] tokens to 0. Suppose, our tokens length should be 5. Now, our tokens list has already 5 tokens. So, we don't have to add [PAD] token. Then our attention mask will become: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ef447",
   "metadata": {
    "collapsed": true,
    "id": "aixDEWsTfoxH"
   },
   "source": [
    "attention_mask = [1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2d34b",
   "metadata": {
    "id": "w6ULeBtifoxI"
   },
   "source": [
    "\n",
    "That's it. But instead of doing all the above steps manually, our tokenizer will do these steps for us. We just need to pass the sentence to the tokenizer as shown below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ebe3eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EujRhGsfoxJ",
    "outputId": "c4c4a6f0-4f64-4b6f-97b5-34f9e8b87047"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 3000, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('I love Paris')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72540996",
   "metadata": {
    "id": "by2HeGksfoxN"
   },
   "source": [
    "\n",
    "With the tokenizer, we can also pass any number of sentences and perform padding dynamically. To do that, we need to set padding to True and also the maximum sequence length. For instance, as shown below, we pass three sentences and we set the maximum sequence length, max_length to 5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21fcec1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwW-9n96foxN",
    "outputId": "50b80657-32e1-448b-db7e-95a6576603bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2293, 3000, 102], [101, 5055, 4875, 102, 0], [101, 4586, 2991, 102, 0]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0], [1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['I love Paris', 'birds fly','snow fall'], padding = True, max_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad82426",
   "metadata": {
    "id": "a2QyIyEefoxO"
   },
   "source": [
    "\n",
    "That's it, with the tokenizer, we can easily preprocess our dataset. So we define a function called preprocess for processing the dataset as shown below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9875d02",
   "metadata": {
    "id": "TYpvRZcnfoxP"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357c451",
   "metadata": {
    "id": "PwV4SLYDfoxP"
   },
   "source": [
    "\n",
    "Now, we preprocess the train and test set using the preprocess function: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec8a2b97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "a8f3cec178ab48f4a384e3f161b7a4b6",
      "630aceb512c14b3b8ebf582768cc92f7",
      "641bc0c6231148efba8c1cdd5b584c71",
      "2c1bbc546b91419ba42fb687d3381bd0",
      "e529489040b94f2e9b198161309403de",
      "c22a9d401f3440de8c18cb9f739c7363",
      "c30ca23fca0349b8bd285aab31980576",
      "1c5b94a7bb2a4ca1b20d8ddca992d435",
      "2e0f1746a3af4ae2bebdb89dbb7bf75f",
      "58409bb8ed944db2a19a6c0bbd71862a",
      "195c6a44a6b54024a4a718ed8f612b83",
      "7a4519d60caa47f498b8a39a61791492",
      "57d92a64d9984c0982fdb2f9670ef2e9",
      "9925167f7a6f43dab5be964467161275",
      "c7179f48b7eb4966987f1d7ee2ea550d",
      "7cd1ff43c45d4c79a39927af72b284a8"
     ]
    },
    "id": "MUyB2AXKfoxQ",
    "outputId": "5864dc11-56e5-490c-ea1d-f28e7d08769e"
   },
   "outputs": [],
   "source": [
    "train_set = train_set.map(preprocess, batched=True, batch_size=len(train_set))\n",
    "test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc220fd0",
   "metadata": {
    "id": "y0LaAN32foxQ"
   },
   "source": [
    "\n",
    "Next, we use the set_format function and select the columns which we need in our dataset and also in which format we need them as shown below:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "984e0b5d",
   "metadata": {
    "id": "EDdfEKT8foxR"
   },
   "outputs": [],
   "source": [
    "train_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90db36d",
   "metadata": {
    "id": "YZHwUC-NfoxR"
   },
   "source": [
    "That's it. Now that we have the dataset ready, let's train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f188bf3",
   "metadata": {
    "id": "Fv67exnifoxR"
   },
   "source": [
    "### Training the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62d516",
   "metadata": {
    "id": "ih63Z2wqfoxS"
   },
   "source": [
    "\n",
    "Define the batch size and epoch size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a6fe707",
   "metadata": {
    "id": "1NjA0cFGfoxT"
   },
   "outputs": [],
   "source": [
    "# You can use more batch size if you have more vram\n",
    "batch_size = 1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af198e6d",
   "metadata": {
    "id": "eSLvaWr6foxT"
   },
   "source": [
    "\n",
    "Define the warmup steps and weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af8628f0",
   "metadata": {
    "id": "6bLTXRGQfoxU"
   },
   "outputs": [],
   "source": [
    "warmup_steps = 500\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48854d1f",
   "metadata": {
    "id": "Brs5HIfufoxU"
   },
   "source": [
    "\n",
    "Define the training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8aae35a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06c9nd-DfoxU",
    "outputId": "4e517022-e880-479f-f483-6b866f8e5280"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72971bfd",
   "metadata": {
    "id": "VOqwu_F8foxV"
   },
   "source": [
    "\n",
    "\n",
    "Now define the trainer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfbb999a",
   "metadata": {
    "id": "EA70qZzsfoxV"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e159b",
   "metadata": {
    "id": "EjOdmJKDfoxV"
   },
   "source": [
    "Start training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eaf6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "638360dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "ce_yJaLwfoxW",
    "outputId": "4c1a20c6-099d-4e7e-99ca-c09d606d13a4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 70\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=0.6749564579554966, metrics={'train_runtime': 20.8098, 'train_samples_per_second': 6.728, 'train_steps_per_second': 6.728, 'total_flos': 36835547750400.0, 'train_loss': 0.6749564579554966, 'epoch': 2.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b66fc",
   "metadata": {
    "id": "at-qPxJIfoxW"
   },
   "source": [
    "\n",
    "After training we can evaluate the model using the evaluate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c7c5ec2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "u7sE13FDfoxW",
    "outputId": "dcddfb99-7016-46d7-9f93-1ad9554459ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6275491118431091,\n",
       " 'eval_runtime': 1.014,\n",
       " 'eval_samples_per_second': 29.587,\n",
       " 'eval_steps_per_second': 29.587,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e94d5",
   "metadata": {
    "id": "0hfs3hwcfoxX"
   },
   "source": [
    "\n",
    "In this way, we can finetune the pre-trained BERT. Now that we have learned how to finetune the BERT for the text classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fdcb1",
   "metadata": {
    "id": "WGH9m3aoAYXp"
   },
   "source": [
    "## Q&A with Finetuned BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e6eb7",
   "metadata": {},
   "source": [
    "In this section, let's learn how to perform question answering with a finetuned Q&A BERT. First, let us import the necessary modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd955b1",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a573f97e",
   "metadata": {
    "id": "0hpJg1FbAYXy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ac98a",
   "metadata": {
    "id": "lPJaYZzmAYX0"
   },
   "source": [
    "\n",
    "Now, we download and load the model. We use the bert-large-uncased-whole-word-masking-finetuned-squad model which is finetuned on the SQUAD (Stanford question answering dataset). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369f2e6",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c63abbf6",
   "metadata": {
    "id": "U4rF-fcHAYX2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262292a",
   "metadata": {
    "id": "4VIr9V1qAYX2"
   },
   "source": [
    "\n",
    "Next, we download and load the tokenizer which is used for pretraining the bert-large-uncased-whole-word-masking-finetuned-squad model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f049c211",
   "metadata": {
    "id": "gI6MVxD7AYX3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer_config.json from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\b9f8d92aa5a32cfe504c3524c173dc611dbe81d49392f40601286b94ee1e1169.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\9b7535fe1c0da28aa7cc66b7f34529d984f535c401be8352f6adeb25f7870def.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at C:\\Users\\tanch/.cache\\huggingface\\transformers\\402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadedbd4",
   "metadata": {
    "id": "jtIJnwohAYX3"
   },
   "source": [
    "\n",
    "Now that we downloaded the model and tokenizer, let's preprocess the input. \n",
    "\n",
    "### Preprocessing the Input\n",
    "First, we define the input to the BERT which is question and paragraph text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27c18dbd",
   "metadata": {
    "id": "c_M3V_kVAYX3"
   },
   "outputs": [],
   "source": [
    "question = \"What is the immune system?\"\n",
    "paragraph = \"The immune system is a system of many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988187fb",
   "metadata": {
    "id": "3w6MAGh_AYX4"
   },
   "source": [
    "Add [CLS] token to the beginning of the question and [SEP] token at the end of both the question and paragraph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a34f9047",
   "metadata": {
    "id": "hsn5nIiBAYX4"
   },
   "outputs": [],
   "source": [
    "question = '[CLS] ' + question + '[SEP]'\n",
    "paragraph = paragraph + '[SEP]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9b8e9",
   "metadata": {
    "id": "h8HkGXVSAYX4"
   },
   "source": [
    "\n",
    "Now, tokenize the question and paragraph: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d2a04ed",
   "metadata": {
    "id": "JpOn-vyHAYX4"
   },
   "outputs": [],
   "source": [
    "question_tokens = tokenizer.tokenize(question)\n",
    "paragraph_tokens = tokenizer.tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231dc85d",
   "metadata": {
    "id": "5QVFGvnZAYX5"
   },
   "source": [
    "\n",
    "\n",
    "Combine the question and paragraph tokens and convert them to input_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5417ab0f",
   "metadata": {
    "id": "y5Eo9QhAAYX5"
   },
   "outputs": [],
   "source": [
    "tokens = question_tokens + paragraph_tokens \n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87940c",
   "metadata": {
    "id": "55nyUd8CAYX5"
   },
   "source": [
    "\n",
    "\n",
    "Next, we define the segment_ids. The segment_ids will be 0 for all the tokens of question and it will be 1 for all the tokens of the paragraph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7846670a",
   "metadata": {
    "id": "bhqMUQxJAYX5"
   },
   "outputs": [],
   "source": [
    "segment_ids = [0] * len(question_tokens)\n",
    "segment_ids += [1] * len(paragraph_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6f2db",
   "metadata": {
    "id": "mssGDCivAYX6"
   },
   "source": [
    "\n",
    "Now we convert the input_ids and segment_ids to tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "971f53b2",
   "metadata": {
    "id": "RFTYxYotAYX6"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([input_ids])\n",
    "segment_ids = torch.tensor([segment_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d1f1a",
   "metadata": {
    "id": "e48VMzbCAYX6"
   },
   "source": [
    "\n",
    "\n",
    "Now that we processed the input. Let's feed them to the model and get the result. \n",
    "\n",
    "### Getting the Answer\n",
    "We feed the input_ids and segment_ids to the model which return the start score and end score for all of the tokens: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9337e8d",
   "metadata": {
    "id": "-dBxzkQsAYX6"
   },
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(input_ids, token_type_ids = segment_ids, return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcad79",
   "metadata": {
    "id": "d5zRrjE8AYX6"
   },
   "source": [
    "\n",
    "Now, we select the start_index which is the index of the token which has a maximum start score and end_index which is the index of the token which has a maximum end score: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb359d0c",
   "metadata": {
    "id": "9AUj2-jMAYX7"
   },
   "outputs": [],
   "source": [
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26d87d",
   "metadata": {
    "id": "rdePQpGIAYX7"
   },
   "source": [
    "\n",
    "That's it! Now, we print the text span between the start and end index as our answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d2f29d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8pNAGoYAYX7",
    "outputId": "b58dce9d-5a87-48dc-c7ba-60b62112e304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a system of many biological structures and processes within an organism that protects against disease\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokens[start_index:end_index+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e3855",
   "metadata": {
    "id": "JLMafolhAYX7"
   },
   "source": [
    "\n",
    "Now that we learned how to finetune BERT for the question answering task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4289029",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d103ddc",
   "metadata": {},
   "source": [
    "We started the chapter by looking at the simple implementation of the **pre-trained BERT model** provided by Google. Then, we learned that we can use the pre-trained BERT model in two ways: as a **feature extractor** by extracting embeddings, and by **fine-tuning the pre-trained BERT model** for downstream tasks such as text classification, question-answering, and more.\n",
    "\n",
    "Then, we learned how to **extract embeddings** from the pre-trained BERT model in detail. We also learned how to use Hugging Face's transformers library to generate embeddings. Then, we learned how to extract embeddings from all the encoder layers of BERT in detail. Moving on, we learned how to **fine-tune pre-trained BERT for downstream tasks**. We learned how to fine-tune BERT for **text classification** and **question-answering** in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Sudharsan Ravichandiran - Getting Started with Google BERT_ Build and train state-of-the-art natural language processing models using BERT-Packt Publishing Ltd (2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
