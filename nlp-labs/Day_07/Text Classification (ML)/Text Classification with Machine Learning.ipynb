{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c36e3d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is to demonstrate the implementation of text classification on news data using machine learning approach. The classifier are required to classify the text data into their corresponding categories in supervised manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b73d4",
   "metadata": {},
   "source": [
    "# What will we accomplish?\n",
    "\n",
    "Steps to implement text classifier in machine learning:\n",
    "\n",
    "> Step 1: Importing Libraries\n",
    "\n",
    "> Step 2: Loading Datasets & Exploratory Data Analysis\n",
    "\n",
    "> Step 3: Text Pre-processing\n",
    "\n",
    "> Step 4: Feature Extraction (Vectorization)\n",
    "\n",
    "> Step 5: Running ML algorithms\n",
    "\n",
    "> Step 6: Grid Search for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20126d44",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "<h5>Python version: 3.9.6 (Python 3 ++)</h5>\n",
    "\n",
    "### Library Required:\n",
    "    1. NLTK\n",
    "    2. sklearn\n",
    "    3. re # Regular Expression\n",
    "    4. genism\n",
    "    \n",
    "**Note**: Little bit of python and ML basics including text classification are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2297e",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\n",
    "\n",
    "* [Step 1: Importing Libraries](#Step-1:-Importing-Libraries)\n",
    "\n",
    "\n",
    "* [Step 2: Loading Datasets & EDA](#Step-2:-Loading-Datasets-&-EDA)\n",
    "\n",
    "\n",
    "* [Step 3: Text Pre-Processing](#Step-3:-Text-Pre-Processing)\n",
    "\n",
    "    * [Text Cleaning](#Text-Cleaning)\n",
    "    \n",
    "    * [Stopword Removal](#Stopword-Removal)\n",
    "    \n",
    "    * [Lemmatization](#Lemmatization)\n",
    "    \n",
    "    * [Full Text Preprocessing](#Full-Text-Preprocessing)\n",
    "    \n",
    "    * [Data Cleaning on Text Data](#Data-Cleaning-on-Text-Data)\n",
    "    \n",
    "    * [Pickling Cleaned Text Data](#Pickling-Cleaned-Text-Data)\n",
    "    \n",
    "    * [Loading Pickle File](#Loading-Pickle-File)\n",
    "    \n",
    "\n",
    "* [Step 4: Feature Extraction (Vectorization)](#Step-4:-Feature-Extraction-(Vectorization))\n",
    "    \n",
    "    * [Term Frequency-Inverse Document Frequencies (TF-IDF)](#Term-Frequency-Inverse-Document-Frequencies-(TF-IDF))\n",
    "    \n",
    "    * [Word2Vec](#Word2Vec)\n",
    "    \n",
    "    \n",
    "* [Step 5: Running ML algorithms](#Step-5:-Running-ML-algorithms)\n",
    "    \n",
    "    * [Naive Bayes (NB)](#Naive-Bayes-(NB))\n",
    "    \n",
    "    * [Support Vector Machine](#Support-Vector-Machine)\n",
    "\n",
    "\n",
    "* [Step 6: Grid Search](#Step-6:-Grid-Search)\n",
    "\n",
    "\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0b3d3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Document/Text classification** is one of the important and typical task in **supervised machine learning (ML)**. Assigning categories to documents, which can be a web page, library book, media articles, gallery etc. has many applications like e.g. **spam filtering**, **email routing**, **sentiment analysis** etc.\n",
    "\n",
    "![text-classification](../../../images/text-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a9ad4",
   "metadata": {},
   "source": [
    "# Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27750de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import fetch_20newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# For text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Pickling File\n",
    "import pickle\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Machine Learning algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For fine tuning\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e8751",
   "metadata": {},
   "source": [
    "# Step 2: Loading Datasets & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1311485",
   "metadata": {},
   "source": [
    "The dataset will be using for this tutorial is the famous “20 Newsgoup” dataset. About the data from the original [website](#http://qwone.com/~jason/20Newsgroups/):\n",
    "\n",
    "> The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "This data set is in-built in scikit, so we don’t need to download it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6194a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3cc14",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff8132",
   "metadata": {},
   "source": [
    "The datatypes of training set and test set are sklearn.utils.Bunch which the values can also access by the keys. For more information, you may refer to https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad5fa22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5baf6",
   "metadata": {},
   "source": [
    "To get all the target names (categories) of news data, you can use `.target_name` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc03e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3f5a",
   "metadata": {},
   "source": [
    "Let's get some intuition on how our data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c92d16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target label: rec.autos\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0])\n",
    "print(\"Target label:\", train.target_names[train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2383db5",
   "metadata": {},
   "source": [
    "# Step 3: Text Pre-Processing\n",
    "\n",
    "Before we move to model building, we need to preprocess our dataset by **removing punctuations** & **special characters**, **cleaning texts**, **removing stop words**, and **applying lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24f766",
   "metadata": {},
   "source": [
    "**Simple text cleaning processes**: \n",
    "Some of the common text cleaning process involves:\n",
    "* Removing punctuations, special characters, URLs & hashtags\n",
    "* Removing leading, trailing & extra white spaces/tabs\n",
    "* Typos, slangs are corrected, abbreviations are written in their long forms\n",
    "\n",
    "**Stop-word removal**: \n",
    "We can remove a list of generic stop words from the English vocabulary using nltk. A few such words are ‘i’,’you’,’a’,’the’,’he’,’which’ etc.\n",
    "\n",
    "**Stemming**: \n",
    "Refers to the process of slicing the end or the beginning of words with the intention of removing affixes(prefix/suffix)\n",
    "\n",
    "\n",
    "**Lemmatization**: \n",
    "It is the process of reducing the word to its base form\n",
    "\n",
    "\n",
    "![Stemming vs Lemmatization](../../../images/stemming_lemmatization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614725e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: irwin@cmptrc.lonestar.org (Irwin Arnstein)\n",
      "Subject: Re: Recommendation on Duc\n",
      "Summary: What's it worth?\n",
      "Distribution: usa\n",
      "Expires: Sat, 1 May 1993 05:00:00 GMT\n",
      "Organization: CompuTrac Inc., Richardson TX\n",
      "Keywords: Ducati, GTS, How much? \n",
      "Lines: 13\n",
      "\n",
      "I have a line on a Ducati 900GTS 1978 model with 17k on the clock.  Runs\n",
      "very well, paint is the bronze/brown/orange faded out, leaks a bit of oil\n",
      "and pops out of 1st with hard accel.  The shop will fix trans and oil \n",
      "leak.  They sold the bike to the 1 and only owner.  They want $3495, and\n",
      "I am thinking more like $3K.  Any opinions out there?  Please email me.\n",
      "Thanks.  It would be a nice stable mate to the Beemer.  Then I'll get\n",
      "a jap bike and call myself Axis Motors!\n",
      "\n",
      "-- \n",
      "-----------------------------------------------------------------------\n",
      "\"Tuba\" (Irwin)      \"I honk therefore I am\"     CompuTrac-Richardson,Tx\n",
      "irwin@cmptrc.lonestar.org    DoD #0826          (R75/6)\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = train.data[10]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e4913",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0abf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    # lowercase all the characters\n",
    "    text = text.lower()\n",
    "    # Remove leading, trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Using regular expression (re) to preprocess text\n",
    "    text = re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    # Return preprocessed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d2457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from irwin cmptrc lonestar org irwin arnstein subject re recommendation on duc summary what s it worth distribution usa expires sat may gmt organization computrac inc richardson tx keywords ducati gts how much lines i have a line on a ducati gts model with k on the clock runs very well paint is the bronze brown orange faded out leaks a bit of oil and pops out of st with hard accel the shop will fix trans and oil leak they sold the bike to the and only owner they want and i am thinking more like k any opinions out there please email me thanks it would be a nice stable mate to the beemer then i ll get a jap bike and call myself axis motors tuba irwin i honk therefore i am computrac richardson tx irwin cmptrc lonestar org dod r \n"
     ]
    }
   ],
   "source": [
    "print(cleaning(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d87372",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2b69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    filtered_words = [w for w in word_tokenize(text) if w not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1c6925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From : irwin @ cmptrc.lonestar.org ( Irwin Arnstein ) Subject : Re : Recommendation Duc Summary : What 's worth ? Distribution : usa Expires : Sat , 1 May 1993 05:00:00 GMT Organization : CompuTrac Inc. , Richardson TX Keywords : Ducati , GTS , How much ? Lines : 13 I line Ducati 900GTS 1978 model 17k clock . Runs well , paint bronze/brown/orange faded , leaks bit oil pops 1st hard accel . The shop fix trans oil leak . They sold bike 1 owner . They want $ 3495 , I thinking like $ 3K . Any opinions ? Please email . Thanks . It would nice stable mate Beemer . Then I 'll get jap bike call Axis Motors ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - '' Tuba '' ( Irwin ) `` I honk therefore I '' CompuTrac-Richardson , Tx irwin @ cmptrc.lonestar.org DoD # 0826 ( R75/6 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopword(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bd937",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "300d6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function that map NLTK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Working on lemmatization\n",
    "# Lemmatizer works well with part-of-speech tagging\n",
    "def lemmatization(text):\n",
    "    # Get NLTK position tags\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(text))\n",
    "    # Map the position tags to wordnet tags and lemmatize the word/token\n",
    "    processed_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in word_pos_tags]\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3711c391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From : irwin @ cmptrc.lonestar.org ( Irwin Arnstein ) Subject : Re : Recommendation on Duc Summary : What 's it worth ? Distribution : usa Expires : Sat , 1 May 1993 05:00:00 GMT Organization : CompuTrac Inc. , Richardson TX Keywords : Ducati , GTS , How much ? Lines : 13 I have a line on a Ducati 900GTS 1978 model with 17k on the clock . Runs very well , paint be the bronze/brown/orange fade out , leak a bit of oil and pop out of 1st with hard accel . The shop will fix trans and oil leak . They sell the bike to the 1 and only owner . They want $ 3495 , and I be think more like $ 3K . Any opinion out there ? Please email me . Thanks . It would be a nice stable mate to the Beemer . Then I 'll get a jap bike and call myself Axis Motors ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - '' Tuba '' ( Irwin ) `` I honk therefore I be '' CompuTrac-Richardson , Tx irwin @ cmptrc.lonestar.org DoD # 0826 ( R75/6 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n"
     ]
    }
   ],
   "source": [
    "print(lemmatization(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdc858",
   "metadata": {},
   "source": [
    "## Full Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed504d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    cleaned_text = cleaning(text)\n",
    "    filtered_text = remove_stopword(cleaned_text)\n",
    "    preprocessed_text = lemmatization(filtered_text)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f39c786f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irwin cmptrc lonestar org irwin arnstein subject recommendation duc summary worth distribution usa expire sat may gmt organization computrac inc richardson tx keywords ducati gts much line line ducati gts model k clock run well paint bronze brown orange fade leaks bit oil pop st hard accel shop fix trans oil leak sell bike owner want think like k opinion please email thanks would nice stable mate beemer get jap bike call axis motor tuba irwin honk therefore computrac richardson tx irwin cmptrc lonestar org dod r\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51341f",
   "metadata": {},
   "source": [
    "## Data Cleaning on Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23a7ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning train data\n",
    "# for idx, text in enumerate(train.data):\n",
    "#     train.data[idx] = preprocess(text)\n",
    "    \n",
    "# print(\"Training data is cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "372c5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, text in enumerate(test.data):\n",
    "#     test.data[idx] = preprocess(text)\n",
    "\n",
    "# print(\"Test data is cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca854aa2",
   "metadata": {},
   "source": [
    "## Pickling Cleaned Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458785e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"model/train.pkl\", 'wb') as file:\n",
    "#     pickle.dump(train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "213dae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"model/test.pkl\", 'wb') as file:\n",
    "#     pickle.dump(test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca2c5e",
   "metadata": {},
   "source": [
    "## Loading Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967fd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "with open(\"model/train.pkl\", 'rb') as file:\n",
    "    train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc7c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open(\"model/test.pkl\", 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec529e",
   "metadata": {},
   "source": [
    "# Step 4: Feature Extraction (Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c607d",
   "metadata": {},
   "source": [
    "It’s difficult to work with **text data** while building **Machine learning** models since these models need well-defined **numerical data**. The process to convert text data into numerical data/vector, is called **vectorization** or in the NLP world, **word embedding**. **Bag-of-Words(BoW)** and **Word Embedding (with Word2Vec)** are two well-known methods for converting text data to numerical data. In this notebook, we will be using Term Frequency-Inverse Document Frequencies(TF-IDF) and Word2Vec for feature extraction and vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf70c3",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequencies (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d656d",
   "metadata": {},
   "source": [
    "By using TF-IDF, the value of a word **increases proportionally** to count in the document, but it is **inversely proportional** to the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a11be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized tfidf vectorizer\n",
    "tf_idf = TfidfVectorizer(use_idf=True)\n",
    "X_train_tfidf = tf_idf.fit_transform(train.data)\n",
    "X_test_tfidf = tf_idf.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a0b025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 79397)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796fdb2",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33878aa8",
   "metadata": {},
   "source": [
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network which is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a82640dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(w) for w in train.data]  \n",
    "X_test_tok= [nltk.word_tokenize(w) for w in test.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9536f67",
   "metadata": {},
   "source": [
    "Let’s try to understand the hyperparameters of this model.\n",
    "\n",
    "* vector_size: The number of dimensions of the embeddings and the default is 100.\n",
    "* window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "* min_count: The minimum count of words to consider; words with occurrence less than this count will be * ignored. The default for min_count is 5.\n",
    "* workers: The number of partitions during training and the default workers is 3.\n",
    "* sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "\n",
    "After training the word2vec model, we can obtain the word embedding directly from the training model as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26e489ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "        self.word2vec = model.wv\n",
    "        \n",
    "        self.dim = model.vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9891d3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text_tok = [nltk.word_tokenize(i) for i in train.data]\n",
    "model = Word2Vec(clean_text_tok, min_count=1, vector_size=150, workers=8, window=8) \n",
    "\n",
    "modelw = MeanEmbeddingVectorizer(model)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_w2v = modelw.transform(X_train_tok)\n",
    "X_test_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4f1a1c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 150)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6813e2",
   "metadata": {},
   "source": [
    "# Step 5: Running ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5a9f0",
   "metadata": {},
   "source": [
    "It’s time to train a machine learning model on the vectorized dataset and test it. Now that we have converted the text data to numerical data, we can run ML models on ***X_train_tfidf*** & ***train.target***. We’ll test this model on ***X_test_tfidf*** to get ***test.target*** and further evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0662544",
   "metadata": {},
   "source": [
    "## Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f47de",
   "metadata": {},
   "source": [
    "### Using TFIDF-Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0708ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized Multinomial Naive Bayes\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b311329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Naive Bayes classifier according to X, y\n",
    "clf.fit(X_train_tfidf, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "912e4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "538b499b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-TFIDF Model Accuracy:\n",
      "0.8076208178438662\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predicted == test.target)\n",
    "\n",
    "print(\"NB-TFIDF Model Accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975954a4",
   "metadata": {},
   "source": [
    "### Using Word2vec Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0c20d",
   "metadata": {},
   "source": [
    "Since our word2vec data consist of negative values and MultinomialNB fails when features have negative values, therefore we will use GaussianNB instead to normalize features to [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9623815",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB().fit(X_train_w2v, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1775fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf325d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-W2V Model Accuracy:\n",
      "0.45419543281996816\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predicted == test.target)\n",
    "\n",
    "print(\"NB-W2V Model Accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f75e8a",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae034b5b",
   "metadata": {},
   "source": [
    "### Using TFIDF-Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a196be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "813c4729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.fit(X_train_tfidf, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9a35e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = SVM.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d98714bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-TFIDF Model Accuracy:\n",
      "0.8166489644184811\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predicted == test.target)\n",
    "\n",
    "print(\"SVM-TFIDF Model Accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a3c0c",
   "metadata": {},
   "source": [
    "### Using Word2vec Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a325a8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.fit(X_train_w2v, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c8cc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = SVM.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf00f83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-W2V Model Accuracy:\n",
      "0.6034253850238981\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predicted == test.target)\n",
    "\n",
    "print(\"SVM-W2V Model Accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc89111",
   "metadata": {},
   "source": [
    "As you can observe from our result obtained, both SVM and NB models product low accuracy when using word2vec data. The possible reason is because our word2vec is only trained on the train data which has limited amount of vocabulary (around 11000++) and therefore it is hardly to understand the semantical meaning of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40268b31",
   "metadata": {},
   "source": [
    "# Step 6: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab63fa",
   "metadata": {},
   "source": [
    "Almost all the classifiers will have various parameters which can be tuned to obtain optimal performance. Scikit gives an extremely useful tool ‘GridSearchCV’. In this section, let's fine tune our SVM model with TF-IDF text data. We are going to use pipeline and therefore there are some processings need to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af9554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cls = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                     ('cls', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8e2ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'cls__alpha': [1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a54d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_text_cls = GridSearchCV(text_cls, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7d5383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf', TfidfVectorizer()), ('cls', SGDClassifier(alpha=0.001))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(),\n",
       " 'cls': SGDClassifier(alpha=0.001),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 1),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'cls__alpha': 0.001,\n",
       " 'cls__average': False,\n",
       " 'cls__class_weight': None,\n",
       " 'cls__early_stopping': False,\n",
       " 'cls__epsilon': 0.1,\n",
       " 'cls__eta0': 0.0,\n",
       " 'cls__fit_intercept': True,\n",
       " 'cls__l1_ratio': 0.15,\n",
       " 'cls__learning_rate': 'optimal',\n",
       " 'cls__loss': 'hinge',\n",
       " 'cls__max_iter': 1000,\n",
       " 'cls__n_iter_no_change': 5,\n",
       " 'cls__n_jobs': None,\n",
       " 'cls__penalty': 'l2',\n",
       " 'cls__power_t': 0.5,\n",
       " 'cls__random_state': None,\n",
       " 'cls__shuffle': True,\n",
       " 'cls__tol': 0.001,\n",
       " 'cls__validation_fraction': 0.1,\n",
       " 'cls__verbose': 0,\n",
       " 'cls__warm_start': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cls.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "140f04d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_text_cls = gs_text_cls.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac865838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812091099152827"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model scores\n",
    "gs_text_cls.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d6a26a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__alpha': 0.001, 'tfidf__use_idf': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model parameters\n",
    "gs_text_cls.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4289029",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d103ddc",
   "metadata": {},
   "source": [
    "In this article, I demonstrated the basics of building a text classification model comparing **Bag-of-Words (with Tf-Idf)** and **Word Embedding with Word2Vec**. You can further enhance the performance of your model using this code by\n",
    "\n",
    "* using other classification algorithms like Support Vector Machines (SVM), XgBoost, Ensemble models, Neural networks etc.\n",
    "* using Gridsearch with more parameters to tune the hyperparameters of your model\n",
    "* using GloVe for word embeddings\n",
    "* using advanced word-embedding methods like GloVe and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "2. https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
