{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a23038e",
   "metadata": {},
   "source": [
    "> **Copyright (c) 2020 Skymind Holdings Berhad**<br><br>\n",
    "> **Copyright (c) 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    "<br>\n",
    "Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "<br>you may not use this file except in compliance with the License.\n",
    "<br>You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0/\n",
    "<br>\n",
    "<br>Unless required by applicable law or agreed to in writing, software\n",
    "<br>distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n",
    "<br>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "<br>See the License for the specific language governing permissions and\n",
    "<br>limitations under the License.\n",
    "<br>\n",
    "<br>\n",
    "**SPDX-License-Identifier: Apache-2.0**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e46dd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This handson is to demonstrate how to create a chatbot with PyTorch from scratch. There are a few parts in this section and you can run them and even edit the \"intents.json\" file (intents file is the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e97335",
   "metadata": {},
   "source": [
    "# What we will accomplish?\n",
    "\n",
    "1. Learn how chatbots work\n",
    "2. Get an overview of a chatbot structure\n",
    "3. Learn implementations of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fea2bc",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "Read the guide at each section and run them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46c1ef",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "## Important (Dataset)\n",
    "Our dataset is the \"intents.json\". This file contains the user's intent and possible queries (questions) that the user might ask. If you are familiar with json file, you should have no problem adding queries along with their tags and what the chatbot should answer.\n",
    "\n",
    "In the json file, there are intents, tag, patterns and responses\n",
    "\n",
    "Intents like the main branch in that contains tag, patterns and responses. You don't have to change anything here.\n",
    "\n",
    "If you want to add a query, you must add a tag, pattern and its reponses.\n",
    "\n",
    "Tag is the label that we are a going to give for the query, for example:\n",
    "\n",
    "patterns: \"hello\", \"hey\", \"hi\"\n",
    "\n",
    "tag: \"greetings\"\n",
    "\n",
    "Patterns like hello will obviously fall under greetings tag. You also need to add a response so that the bot will be able to answer it.\n",
    "\n",
    "Therefore, in the responses you can add \"Hi how may I help you\".\n",
    "\n",
    "Reminder: This intents.json must be formatted correctly, otherwise the model won't be able to train it.\n",
    "\n",
    "## Data Preprocessing\n",
    "In this next section is the part where we will preprocess the data. The process that will occur is tokenization, stemming and bag of words. Here, we have defined all the methods.\n",
    "\n",
    "If you have not installed nltk, do install it. If you have just install nltk and running for the first time, you might want to download punkt as well. You just have to uncomment the code the run them, and it will automatically download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# You might need to download punkt if you are running this the first time\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfa0a9",
   "metadata": {},
   "source": [
    "# Part 2: Neural Network Model\n",
    "Here we define our neural network model. This model has just one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc73557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac3568",
   "metadata": {},
   "source": [
    "# Part 3: Training the Model\n",
    "Next step is to train the model. This is the most crucial step and everytime you make changes to the intents.json file, you need to train the model again to see the changes when you run the chatbot. Once the model is trained, it will be saved as data.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345542c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 patterns\n",
      "8 tags: ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'price', 'thanks']\n",
      "81 unique stemmed words: [\"'s\", 'a', 'accept', 'and', 'anyon', 'anyth', 'are', 'buy', 'bye', 'can', 'card', 'cash', 'cost', 'credit', 'day', 'deliveri', 'do', 'doe', 'for', 'funni', 'get', 'good', 'goodby', 'greet', 'have', 'hello', 'help', 'here', 'hey', 'hi', 'how', 'hye', 'i', 'in', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'make', 'mastercard', 'me', 'much', 'my', 'of', 'onli', 'or', 'pay', 'payment', 'paypal', 'plushi', 'price', 'product', 'sale', 'see', 'sell', 'shall', 'ship', 'shop', 'someth', 'store', 't-shirt', 'take', 'tell', 'thank', 'that', 'the', 'there', 'they', 'thi', 'time', 'to', 'wassup', 'what', 'when', 'which', 'with', 'you']\n",
      "81 8\n",
      "Epoch [100/1000], Loss: 0.4613\n",
      "Epoch [200/1000], Loss: 0.0157\n",
      "Epoch [300/1000], Loss: 0.0034\n",
      "Epoch [400/1000], Loss: 0.0016\n",
      "Epoch [500/1000], Loss: 0.0010\n",
      "Epoch [600/1000], Loss: 0.0005\n",
      "Epoch [700/1000], Loss: 0.0001\n",
      "Epoch [800/1000], Loss: 0.0008\n",
      "Epoch [900/1000], Loss: 0.0001\n",
      "Epoch [1000/1000], Loss: 0.0002\n",
      "final loss: 0.0002\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from nltk_utils import bag_of_words, tokenize, stem\n",
    "# from model import NeuralNet\n",
    "\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # add to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # add to our words list\n",
    "        all_words.extend(w)\n",
    "        # add to xy pair\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# stem and lower each word\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)\n",
    "\n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)\n",
    "\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14438115",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "Finally, we can run the model and test it ourselves. Once you hit run, a terminal will pop up and we can enter our queries. Make sure you have gone through the intents.json so you have a general idea of what to ask the chatbot so it will be able to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720ac5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, How may I serve you?\n",
      "You: Hi\n",
      "Chat Bot: Hey :-)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "# from model import NeuralNet\n",
    "# from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Chat Bot\"\n",
    "print(\"Welcome, How may I serve you?\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720e5d8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This handson is a simple way of making a chatbot, though you may seen sometimes the model is not able to understand the query and could not provide a response. This could be due to the lack of data in the dataset and vanishing gradient problem in our model.\n",
    "\n",
    "Nevertheless, this may be a kick start to creating chatbots and we can make improvements to this chatbots by implementing RNN and better data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6529532",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "Author\n",
    "Pahvindran Raj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
