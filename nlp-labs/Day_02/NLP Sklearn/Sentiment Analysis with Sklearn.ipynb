{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407be03",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is to introduce the basic of Scikit-learn module for basic NLP task. Sklearn is a useful library especially in machine learning. This tutorial consists of two major parts, which are:\n",
    "\n",
    "1. Introduction to scikit-learn\n",
    "2. Using sklearn in sentiment analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7286a59",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [What is Scikit-Learn (Sklearn)](#What-is-Scikit-Learn-(Sklearn))\n",
    "\n",
    "\n",
    "* [Prerequisites](#Prerequisites)\n",
    "\n",
    "\n",
    "* [Installation](#Installation)\n",
    "    * [Using PIP](#Using-PIP)\n",
    "    * [Using conda](#Using-conda)\n",
    "\n",
    "\n",
    "* [Features](#Features)\n",
    "\n",
    "\n",
    "* [Sentiment Analysis](#Sentiment-Analysis)\n",
    "    * [Objective](#Objective)\n",
    "\n",
    "\n",
    "* [Get Started](#Let’s-Get-Started)\n",
    "    * [TF-IDF](#TF-IDF)\n",
    "    * [Support Vector Machine](#Support-Vector-Machine)\n",
    "    \n",
    "\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45b1dc",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"300\" height=\"300\" src=\"../../../images/sklearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff2676",
   "metadata": {},
   "source": [
    "# What is Scikit-Learn (Sklearn)\n",
    "\n",
    "Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon **NumPy**, **SciPy** and **Matplotlib**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230f988",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Before we start using scikit-learn latest release, we require the following:\n",
    "\n",
    "    + Python (>=3.5)\n",
    "\n",
    "    + NumPy (>= 1.11.0)\n",
    "\n",
    "    + Scipy (>= 0.17.0)li\n",
    "\n",
    "    + Joblib (>= 0.11)\n",
    "\n",
    "    + Matplotlib (>= 1.5.1) is required for Sklearn plotting capabilities.\n",
    "\n",
    "    + Pandas (>= 0.18.0) is required for some of the scikit-learn examples using data structure and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe67a8",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "If you already installed NumPy and Scipy, following are the two easiest ways to install scikit-learn:\n",
    "\n",
    "### Using PIP\n",
    "Following command can be used to install scikit-learn via pip:\n",
    "\n",
    "> `pip install -U scikit-learn`\n",
    "\n",
    "### Using conda\n",
    "Following command can be used to install scikit-learn via conda:\n",
    "\n",
    "> `conda install scikit-learn`\n",
    "\n",
    "On the other hand, if NumPy and Scipy is not yet installed on your Python workstation then, you can install them by using either **pip** or **conda**.\n",
    "\n",
    "Another option to use scikit-learn is to use Python distributions like **Canopy** and **Anaconda** because they both ship the latest version of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed4aba",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "Rather than focusing on loading, manipulating and summarising data, Scikit-learn library is focused on modeling the data. Some of the most popular groups of models provided by Sklearn are as follows:\n",
    "\n",
    "- **Unpervised Learning algorithms** − Almost all the popular supervised learning algorithms, like Linear Regression, Support Vector Machine (SVM), Decision Tree etc., are the part of scikit-learn.\n",
    "\n",
    "\n",
    "- **Unsupervised Learning algorithms** − On the other hand, it also has all the popular unsupervised learning algorithms from clustering, factor analysis, PCA (Principal Component Analysis) to unsupervised neural networks.\n",
    "\n",
    "\n",
    "- **Clustering** − This model is used for grouping unlabeled data.\n",
    "\n",
    "\n",
    "- **Cross Validation** − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "\n",
    "- **Dimensionality Reduction** − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "\n",
    "- **Ensemble methods** − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "\n",
    "- **Feature extraction** − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "\n",
    "- **Feature selection** − It is used to identify useful attributes to create supervised models.\n",
    "\n",
    "\n",
    "- **Open Source** − It is open source library and also commercially usable under BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bba8f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c5c63",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook we are going to perform a binary classification i.e. we will classify the sentiment as positive or negative according to the `Reviews’ column data of the IMDB dataset.  We will use TFIDF for text data vectorization and Linear Support Vector Machine for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1af9e3",
   "metadata": {},
   "source": [
    "`Natural Language Processing (NLP)` is a sub-field of artificial intelligence that deals understanding and processing human language. In light of new advancements in machine learning, many organizations have begun applying natural language processing for translation, chatbots and candidate filtering.\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly. Rather, the text must be converted into vectors of numbers. Then we use `TF-IDF` vectorizer approach. `TF-IDF` is a technique used for natural language processing, that transforms text to feature vectors that can be used as input to the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ebc40",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "- **Pandas**\n",
    "> `!pip install pandas`\n",
    "\n",
    "- **Numpy**\n",
    "> `!pip install numpy`\n",
    "\n",
    "- **Scikit-learn**\n",
    "> `!pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c7048",
   "metadata": {},
   "source": [
    "# Let’s Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f389a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7265e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"../../../resources/day_02/IMDB_train.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64d604",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd11272",
   "metadata": {},
   "source": [
    "![TF-IDF](../../../images/tf-idf.png)\n",
    "\n",
    "Some semantic information is preserved as uncommon words are given more importance than common words in TF-IDF.\n",
    "\n",
    "`E.g. 'She is beautiful', Here 'beautiful will have more importance than 'she' or 'is'.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f47d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c1962e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mere thoughts of \"Going Overboard\" (aka \"Babes...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why does this movie fall WELL below standards?...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow and I thought that any Steven Segal movie ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The story is seen before, but that does'n matt...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Sentiment\n",
       "0  When I first tuned in on this morning news, I ...       neg\n",
       "1  Mere thoughts of \"Going Overboard\" (aka \"Babes...       neg\n",
       "2  Why does this movie fall WELL below standards?...       neg\n",
       "3  Wow and I thought that any Steven Segal movie ...       neg\n",
       "4  The story is seen before, but that does'n matt...       neg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying top 5 rows of our dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636999d9",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), text preprocessing is the practice of cleaning and preparing text data. **NLTK** and **re** are common Python libraries used to handle many text preprocessing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877c314",
   "metadata": {},
   "source": [
    "Defining `get_clean` function which is taking argument as ‘Reviews’ column then after performing some steps:\n",
    "\n",
    "1. Lowering the letter then after replacing backward slash from nothing and underscore from space.\n",
    "\n",
    "2. Remove emails from the Reviews column.\n",
    "\n",
    "3. Removing html tags from the Reviews column.\n",
    "\n",
    "4. Removing special character.\n",
    "\n",
    "5. If you have multiple repeated character then it converted into single character and make meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a183dbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when i first tuned in on this morning news i t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mere thoughts of going overboard aka babes aho...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why does this movie fall well below standards ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow and i thought that any steven segal movie ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the story is seen before but that doesn matter...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Sentiment\n",
       "0  when i first tuned in on this morning news i t...       neg\n",
       "1  mere thoughts of going overboard aka babes aho...       neg\n",
       "2  why does this movie fall well below standards ...       neg\n",
       "3  wow and i thought that any steven segal movie ...       neg\n",
       "4  the story is seen before but that doesn matter...       neg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_clean(x):\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n",
    "    \n",
    "    # Remove Emails\n",
    "    x = re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\",  x)\n",
    "\n",
    "    # Remove Urls\n",
    "    x = re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
    "\n",
    "    # Remove Special Characters\n",
    "    x = re.sub(r'[^\\w ]+', \"\", x)\n",
    "    x = ' '.join(x.split())\n",
    "\n",
    "    x = re.sub(\"(.)\\{2,}\", \"'\\'\", x)\n",
    "    return x\n",
    "\n",
    "train_df['Reviews'] = train_df['Reviews'].apply(lambda x: get_clean(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f854119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF count vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = train_df['Reviews']\n",
    "y = train_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593f9ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2894437 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data into vectorizer and then transform it\n",
    "X = tfidf.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f457c5",
   "metadata": {},
   "source": [
    "Here, splitting the dataset into x and y column having **20%** is for `testing` and **80%** for `training` purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f53016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a8a19",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "`SVM` is a supervised machine learning algorithm that can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.\n",
    "\n",
    "![SVM](../../../images/SVM_1.png)\n",
    "\n",
    "![SVM](../../../images/SVM_2.png)\n",
    "\n",
    "The objective of a `Linear SVC` (Support Vector Classifier) is to fit the data you provide, returning a “best fit” hyperplane that divides, or categorizes your data. From there, after getting the hyperplane, you can then feed some features to your classifier to see what the “predicted” class is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c229ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221f72d",
   "metadata": {},
   "source": [
    "The `classification report` shows a representation of the main classification metrics on a per-class basis. This gives a deeper intuition of the classifier behavior over global accuracy which can mask functional weaknesses in one class of a multiclass problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c96a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.87      0.87      2480\n",
      "         pos       0.87      0.88      0.88      2520\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8cfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'this movie is really good. thanks a lot for making it'\n",
    "\n",
    "x = get_clean(x)\n",
    "vec = tfidf.transform([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78edf745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of input vector\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbe9b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction on input vector\n",
    "clf.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0d73f",
   "metadata": {},
   "source": [
    "Python `pickle` module is used for serializing and de-serializing python object structures. The process to converts any kind of python objects (list, dict, etc.) into byte streams (0s and 1s) is called `pickling` or `serialization` or `flattening` or `marshalling`. We can convert the byte stream (generated through pickling) back into python objects by a process called as `unpickling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb14793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('model/clf_model', 'wb'))\n",
    "pickle.dump(tfidf, open('model/tfidf', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06349f69",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Firstly, We have loaded the IMBD movie reviews dataset using the pandas dataframe.\n",
    "\n",
    "\n",
    "- Then define get_clean() function and removed unwanted emails, urls, Html tags and special character.\n",
    "\n",
    "\n",
    "- Convert the text into vectors with the help of the TF-IDF Vectorizer.\n",
    "\n",
    "\n",
    "- After that use a linear vector machine classifier algorithm.\n",
    "\n",
    "\n",
    "- We have fit the model on LinearSVC classifier for binary classification and predict the sentiment i.e. positive or negative on real data.\n",
    "\n",
    "\n",
    "- Lastly, Dump the clf and TF-IDF model with the help of the pickle library. In other words, it’s the process of converting a python object into a byte stream to store it in a file/database, maintain program state across sessions or transport data over the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Sklearn Tutorial](https://www.tutorialspoint.com/scikit_learn/index.htm)\n",
    "2. [Sentiment Analysis Using Sklearn](https://kgptalkie.com/sentiment-analysis-using-scikit-learn/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
