{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c36e3d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will use PyTorch to build a **sequence 2 sequence (encoder-decoder) model** with **simple dot product attention using GRU** and evaluate their **attention scores**. We will further look into metrics like — **BLEU**, **ROUGE** for evaluating our model.\n",
    "\n",
    "\n",
    "### Dataset used:\n",
    "We will work on the **wikihow dataset**. If you cannot find the dataset in the `datasets` folder, you may download from [here](https://ucsb.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358).\n",
    "\n",
    "\n",
    "This dataset is one of the large-scale datasets available for summarization with the length of articles varying considerably. It consists of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles which make the summarization problem more challenging and interesting.\n",
    "\n",
    "![wikihow](../../../images/wikihow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b73d4",
   "metadata": {},
   "source": [
    "# What will we accomplish?\n",
    "\n",
    "Steps to implement text summarizer using PyTorch Deep Learning:\n",
    "\n",
    "> Step 1: Data preprocessing using Pytorch\n",
    "\n",
    "> Step 2: Building network model\n",
    "\n",
    "> Step 3: Building model training and evaluation\n",
    "\n",
    "> Step 4: Inference and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a8fc0",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [Data Preparation](#Data-Preparation)\n",
    "    * [Import Libraries](#Import-Libraries)\n",
    "    * [Import Dataset](#Import-Dataset)\n",
    "    * [Data Preprocessing](#Data-Preprocessing)\n",
    "    * [Data Cleaning](#Data-Cleaning)\n",
    "    * [Data Exploratory Analysis](#Data-Exploratory-Analysis)\n",
    "    * [Train-Test Split](#Train-Test-Split)\n",
    "\n",
    "\n",
    "* [Deep Model Design](#Deep-Model-Design)\n",
    "    * [Seq2seq Model with Attention using GRU and Teacher Forcing](#Seq2seq-Model-with-Attention-using-GRU-and-Teacher-Forcing)\n",
    "    * [Prepare Vocab](#Prepare-Vocab)\n",
    "    * [Building Neural Network Model](#Building-Neural-Network-Model)\n",
    "        * [Encoder](#Encoder)\n",
    "        * [Decoder](#Decoder)\n",
    "        * [Attention Mechanism](#Attention-Mechanism)\n",
    "        * [Teacher Forcing](#Teacher-Forcing)\n",
    "    * [Configure Training and Evaluation](#Configure-Training-and-Evaluation)\n",
    "    * [Model Training](#Model-Training)\n",
    "    * [Pickle Trained Model](#Pickle-Trained-Model)\n",
    "    * [Load Trained Model](#Load-Trained-Model)\n",
    "    \n",
    "    \n",
    "* [Model Evaluation Metrics](#Model-Evaluation-Metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0758d5",
   "metadata": {},
   "source": [
    "# Implementation of Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585452b",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33576dd",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edaeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For text cleaning\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a823a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords corpus\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a48bbf",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e923ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wikihow dataset\n",
    "df = pd.read_csv('../../../resources/day_08/wikihowAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fcc8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\nKeep related supplies in the same area.,\\r...</td>\n",
       "      <td>How to Be an Organized Artist1</td>\n",
       "      <td>If you're a photographer, keep all the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\nCreate a sketch in the NeoPopRealist manne...</td>\n",
       "      <td>How to Create a Neopoprealist Art Work</td>\n",
       "      <td>See the image for how this drawing develops s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\nGet a bachelor’s degree.,\\r\\nEnroll in a s...</td>\n",
       "      <td>How to Be a Visual Effects Artist1</td>\n",
       "      <td>It is possible to become a VFX artist without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\nStart with some experience or interest in ...</td>\n",
       "      <td>How to Become an Art Investor</td>\n",
       "      <td>The best art investors do their research on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\r\\nKeep your reference materials, sketches, a...</td>\n",
       "      <td>How to Be an Organized Artist2</td>\n",
       "      <td>As you start planning for a project or work, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  \\r\\nKeep related supplies in the same area.,\\r...   \n",
       "1  \\r\\nCreate a sketch in the NeoPopRealist manne...   \n",
       "2  \\r\\nGet a bachelor’s degree.,\\r\\nEnroll in a s...   \n",
       "3  \\r\\nStart with some experience or interest in ...   \n",
       "4  \\r\\nKeep your reference materials, sketches, a...   \n",
       "\n",
       "                                    title  \\\n",
       "0          How to Be an Organized Artist1   \n",
       "1  How to Create a Neopoprealist Art Work   \n",
       "2      How to Be a Visual Effects Artist1   \n",
       "3           How to Become an Art Investor   \n",
       "4          How to Be an Organized Artist2   \n",
       "\n",
       "                                                text  \n",
       "0   If you're a photographer, keep all the necess...  \n",
       "1   See the image for how this drawing develops s...  \n",
       "2   It is possible to become a VFX artist without...  \n",
       "3   The best art investors do their research on t...  \n",
       "4   As you start planning for a project or work, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1834ad08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214547</td>\n",
       "      <td>215364</td>\n",
       "      <td>214294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>214096</td>\n",
       "      <td>215364</td>\n",
       "      <td>209178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\r\\nAcquire a pot.,\\r\\nGather the ingredients ...</td>\n",
       "      <td>How to Be an Organized Artist1</td>\n",
       "      <td>,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "count                                              214547   \n",
       "unique                                             214096   \n",
       "top     \\r\\nAcquire a pot.,\\r\\nGather the ingredients ...   \n",
       "freq                                                   11   \n",
       "\n",
       "                                 title    text  \n",
       "count                           215364  214294  \n",
       "unique                          215364  209178  \n",
       "top     How to Be an Organized Artist1      ,,  \n",
       "freq                                 1     524  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c00566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 215365\n",
      "Number of columns: 3\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df.shape\n",
    "\n",
    "print(\"Number of articles:\", rows)\n",
    "print(\"Number of columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615fd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid articles\n",
    "df = df[df['text'].isnull() == False]\n",
    "df = df[df['text'] != 'nan']\n",
    "\n",
    "df = df[df['headline'].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdac9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated articles\n",
    "df.drop_duplicates(subset=['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bacfdba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\nKeep related supplies in the same area.,\\r...</td>\n",
       "      <td>How to Be an Organized Artist1</td>\n",
       "      <td>If you're a photographer, keep all the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\nCreate a sketch in the NeoPopRealist manne...</td>\n",
       "      <td>How to Create a Neopoprealist Art Work</td>\n",
       "      <td>See the image for how this drawing develops s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\nGet a bachelor’s degree.,\\r\\nEnroll in a s...</td>\n",
       "      <td>How to Be a Visual Effects Artist1</td>\n",
       "      <td>It is possible to become a VFX artist without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\nStart with some experience or interest in ...</td>\n",
       "      <td>How to Become an Art Investor</td>\n",
       "      <td>The best art investors do their research on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\r\\nKeep your reference materials, sketches, a...</td>\n",
       "      <td>How to Be an Organized Artist2</td>\n",
       "      <td>As you start planning for a project or work, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215360</th>\n",
       "      <td>\\r\\nConsider changing the spelling of your nam...</td>\n",
       "      <td>How to Pick a Stage Name3</td>\n",
       "      <td>If you have a name that you like, you might f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215361</th>\n",
       "      <td>\\r\\nTry out your name.,\\r\\nDon’t legally chang...</td>\n",
       "      <td>How to Pick a Stage Name4</td>\n",
       "      <td>Your name might sound great to you when you s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215362</th>\n",
       "      <td>\\r\\nUnderstand the process of relief printing....</td>\n",
       "      <td>How to Identify Prints1</td>\n",
       "      <td>Relief printing is the oldest and most tradit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215363</th>\n",
       "      <td>\\r\\nUnderstand the process of intaglio printin...</td>\n",
       "      <td>How to Identify Prints2</td>\n",
       "      <td>Intaglio is Italian for \"incis­ing,\" and corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215364</th>\n",
       "      <td>\\r\\nUnderstand the different varieties of lith...</td>\n",
       "      <td>How to Identify Prints3</td>\n",
       "      <td>Lithography is a big term often used to refer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209178 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "0       \\r\\nKeep related supplies in the same area.,\\r...   \n",
       "1       \\r\\nCreate a sketch in the NeoPopRealist manne...   \n",
       "2       \\r\\nGet a bachelor’s degree.,\\r\\nEnroll in a s...   \n",
       "3       \\r\\nStart with some experience or interest in ...   \n",
       "4       \\r\\nKeep your reference materials, sketches, a...   \n",
       "...                                                   ...   \n",
       "215360  \\r\\nConsider changing the spelling of your nam...   \n",
       "215361  \\r\\nTry out your name.,\\r\\nDon’t legally chang...   \n",
       "215362  \\r\\nUnderstand the process of relief printing....   \n",
       "215363  \\r\\nUnderstand the process of intaglio printin...   \n",
       "215364  \\r\\nUnderstand the different varieties of lith...   \n",
       "\n",
       "                                         title  \\\n",
       "0               How to Be an Organized Artist1   \n",
       "1       How to Create a Neopoprealist Art Work   \n",
       "2           How to Be a Visual Effects Artist1   \n",
       "3                How to Become an Art Investor   \n",
       "4               How to Be an Organized Artist2   \n",
       "...                                        ...   \n",
       "215360               How to Pick a Stage Name3   \n",
       "215361               How to Pick a Stage Name4   \n",
       "215362                 How to Identify Prints1   \n",
       "215363                 How to Identify Prints2   \n",
       "215364                 How to Identify Prints3   \n",
       "\n",
       "                                                     text  \n",
       "0        If you're a photographer, keep all the necess...  \n",
       "1        See the image for how this drawing develops s...  \n",
       "2        It is possible to become a VFX artist without...  \n",
       "3        The best art investors do their research on t...  \n",
       "4        As you start planning for a project or work, ...  \n",
       "...                                                   ...  \n",
       "215360   If you have a name that you like, you might f...  \n",
       "215361   Your name might sound great to you when you s...  \n",
       "215362   Relief printing is the oldest and most tradit...  \n",
       "215363   Intaglio is Italian for \"incis­ing,\" and corr...  \n",
       "215364   Lithography is a big term often used to refer...  \n",
       "\n",
       "[209178 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11141ec8",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Pre-processing and cleaning is an important step because building a model on unclean and messy data will in turn produce messy results. We will apply the below cleaning techniques before feeding our data to the model:\n",
    "\n",
    "1. Converting all text to lower case for further processing\n",
    "\n",
    "\n",
    "2. Parsing HTML tags\n",
    "\n",
    "\n",
    "3. Removing text between ( ) and [ ]\n",
    "\n",
    "\n",
    "4. Contraction Mapping — Replacing shortened version of words (for e.g. can’t is replaced with cannot and so on)\n",
    "\n",
    "\n",
    "5. Removing apostrophe\n",
    "\n",
    "\n",
    "6. Removing punctuations and special characters\n",
    "\n",
    "\n",
    "7. Removing stop words using nltk library\n",
    "\n",
    "\n",
    "8. Retaining only long words, i.e. words with length > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d648a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text, num):\n",
    "    # Lowercase characters\n",
    "    txt = text.lower()\n",
    "    \n",
    "    # Parse HTML tags\n",
    "    txt = BeautifulSoup(txt, \"lxml\").text\n",
    "    \n",
    "    # Remove text between () & []\n",
    "    txt = re.sub(r'\\([^)]*\\)', '', txt)\n",
    "    \n",
    "    # Remove double quotation marks\n",
    "    txt = re.sub('\"','', txt)\n",
    "    \n",
    "    # Contraction mapping\n",
    "    txt = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in txt.split(\" \")])\n",
    "    \n",
    "    # Remove apostrophe\n",
    "    txt = re.sub(r\"'s\\b\",\"\", txt)\n",
    "    \n",
    "    # Remove numbers and special characters\n",
    "    txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "    \n",
    "    # Remove text with at least 2 consecutive same characters\n",
    "    txt = re.sub('[m]{2,}', 'mm', txt)\n",
    "    \n",
    "    if(num == 0):\n",
    "        txt = re.sub(r'\\.',' . ', txt)\n",
    "        tokens = [w for w in txt.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=txt.split()\n",
    "        \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i) > 1:          #removing short words\n",
    "            long_words.append(i)\n",
    "            \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6620e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\", \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \n",
    "                       \"it'd\": \"it would\",\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\"she'd\": \"she would\", \n",
    "                       \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                       \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                       \"they'd've\": \"they would have\",\"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                       \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\",\"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                       \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\",\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\",\"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa85604",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b829a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for t in df['text']:\n",
    "    clean_text.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff1ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summary = []\n",
    "for t in df['headline']:\n",
    "    clean_summary.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4e545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = clean_text\n",
    "df['headline'] = clean_summary\n",
    "\n",
    "df.replace('', np.nan, inplace=True)\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a3509",
   "metadata": {},
   "source": [
    "### Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9603ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_word_count = []\n",
    "headline_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['text']:\n",
    "    temp = i.split()\n",
    "    text_word_count.append(len(temp))\n",
    "\n",
    "for j in df['headline']:\n",
    "    temp1 = j.split()\n",
    "    headline_word_count.append(len(temp1))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'headline':headline_word_count})\n",
    "length_df.hist(bins=30,range=[0,1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e07599c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the graph\n",
    "# We can fix maximum length of text = 150 since most of the reviews have a length of 150 \n",
    "# and maximum headline length of 50, since maximum headlines are of size 50\n",
    "\n",
    "max_len_text = 150\n",
    "max_len_headline = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6024189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4835030549898167\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in df['text']:\n",
    "    if(len(i.split()) <= max_len_text):\n",
    "        cnt = cnt+1\n",
    "        \n",
    "print(cnt/len(df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1d296",
   "metadata": {},
   "source": [
    "Selecting text and headlines below the maximum lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e4843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(df['text'])\n",
    "headline = np.array(df['headline'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(len(headline[i].split()) <= max_len_headline and len(text[i].split()) <= max_len_text):\n",
    "        short_text.append(text[i])\n",
    "        short_summary.append(headline[i])\n",
    "        \n",
    "df = pd.DataFrame({'text':short_text, 'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "779a99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "guy like every facebook status update instagram picture post better yet frequently comment posts sign wants interact may indicate likes look see comments people posts well comments frequently may reveal avid social media user however rarely likes comments posts sign might feelings publicly comments one photos posts respond eager start conversation way may reveal likes least enjoys messaging example may say great photo could respond saying vancouver last week beautiful city guy met starts liking commenting old photos sig interested means spent time looking back old photos probably wants know better enjoys looking pictures guy likes want connect follow variety different social media platforms example may add facebook snapchat start following twitter instagram adding number social media sites likely sign wants look posts photos selfies attempt get know better\n",
      "\n",
      "Summary:\n",
      "see interacts posts reply comments notice comments old pictures posts check see added multiple social media platforms\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", df['text'][50], sep='\\n')\n",
    "\n",
    "print(\"\\nSummary:\", df['summary'][50], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e25954",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Here we using 70% for training set and 30% for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f958e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['summary'], test_size=0.3, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afb20178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:\n",
      "59676\n",
      "\n",
      "Number of testing samples:\n",
      "25576\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", len(X_train), sep=\"\\n\")\n",
    "\n",
    "print(\"\\nNumber of testing samples:\", len(X_test), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50387d4d",
   "metadata": {},
   "source": [
    "## Deep Model Design\n",
    "\n",
    "Before feeding the training data to our deep model, we will represent each word as a one-hot vector. We will then need a unique index per word to use it as the input and output of the network. In order to do so, we will create a helper class `Vocab` which has **(word-> index)** and **(index-> word)** mappings along with the count of each word.\n",
    "\n",
    "\n",
    "To read data in our model, we have created pairs of our input and output in the form of a list **(pairs-> (Input, Output))**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb09a7",
   "metadata": {},
   "source": [
    "### Seq2seq Model with Attention using GRU and Teacher Forcing\n",
    "\n",
    "We will be using a seq2seq(encoder-decoder architecture) model with a simple dot product attention. The underlying idea of choosing this architecture is that we have a many-to-many problem at hand(n number of words as input and m number of words as output). The figure below shows the detailed architecture diagram for this model.\n",
    "\n",
    "![Model Architecture](../../../images/GRUNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714795f",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea368edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66ce98a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Torch is using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496fcda",
   "metadata": {},
   "source": [
    "### Prepare Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "430ca0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70c16d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    text = np.array(text)\n",
    "    summary = np.array(summary)\n",
    "    pairs = [(text[i], summary[i]) for i in range(len(text))]\n",
    "\n",
    "    # Reverse pairs, make Vocab instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_vocab = Vocab(summary)\n",
    "        output_vocab = Vocab(text)\n",
    "    else:\n",
    "        input_vocab = Vocab(text)\n",
    "        output_vocab = Vocab(summary)\n",
    "\n",
    "    return input_vocab, output_vocab, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b7fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareVocab(text, summary, reverse=False):\n",
    "    input_vocab, output_vocab, pairs = readData(text, summary, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_vocab.addSentence(pair[0])\n",
    "        output_vocab.addSentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_vocab.name, input_vocab.n_words)\n",
    "    print(\"\\n\")\n",
    "    print(output_vocab.name, output_vocab.n_words)\n",
    "    \n",
    "    return input_vocab, output_vocab, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c45bec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 59676 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "['save hardware later use remove styrofoam lining necessary proper fit grind sand ensure perfect fit'\n",
      " 'food processor blender crumbs collect inside grater'\n",
      " 'discounts might include deals airfare ticket options seating airport transportation many websites agencies offer budget travel options students around world destinations like los angeles although many airlines longer offer plans still offer discounts flight coupons domestic international travel'\n",
      " ...\n",
      " 'icon looks like map located home screen located bottom map search box map zoom route starting location present first set directions menu pop display basic route information distance travel time eta see different icons gas stations dinner locations coffee shops etc depending location time day map present list nearby locations choose fit category currently method adding custom pit stop additional destination route multiple stops need set new route destination map display new route pit stop start first set directions resume original route simply tap resume route top screen'\n",
      " 'privacy standards prevent uber allowing riders see individual rating driver given however see average rating find uber app phone looking icon apps searching tap icon menu button displayed three horizontal lines upper left corner screen snapshot recent trip appear alongside menu options menu listing number issues appear toward top menu taken page describing passenger ratings thank window pop containing average trip rating'\n",
      " 'leave minutes turn toast side minutes heat long takes browned sides place squares bread half leave inch edge free chocolate melt area bake minutes chocolate starts melting glossy texture sprinkle small drops olive oil onto slice season pinch salt slice like thought oil salt chocolate skip however bring great flavor popular method enjoy hot chocolate fresh berries side bowl'] 60295\n",
      "\n",
      "\n",
      "['remove factory installed rear bumper trim panel line new body kit rear bumper make sure lines correctly use factory bumper template drill new holes new body kit rear bumper install onto car factory hardware'\n",
      " 'use box grater make bread crumbs cut loaf bread inch slices run slice bread large holes box grater continue grating bread slices gone entire loaf'\n",
      " 'locate airlines offer discounts specified ages occupations educators shop around deals students hunt discounted ticket offerings senior citizens'\n",
      " ...\n",
      " 'open maps app tap search box input final destination tap destination results tap directions tap go next desired route tap bottom screen tap pit stop category tap go next desired pit stop'\n",
      " 'understand cannot see individual rating open uber app tap menu button tap help tap account payment tap account settings ratings tap would like know rating tap submit'\n",
      " 'preheat oven place crusty bread slices baking sheet place oven toast cut slices half break chocolate squares put chocolate covered bread back oven remove oven serve immediately'] 35101\n"
     ]
    }
   ],
   "source": [
    "input_vocab, output_vocab, pairs = prepareVocab(X_train, y_train , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "075f6eeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('set counter toast long needed favorite level toasting bread toasting grab plate napkin get peanut butter jelly jam knife spoon spread bread toasted liking lay toast plate breadboard use separate knives spread avoid leaving traces spread opposite jar using butter knife scoop big hunk peanut butter place one slice toast spread melts bread cover toast thick layer peanut butter like use little much like evenly distribute two spreads press together form toasted sandwich enjoy snack', 'plug toaster grab ingredients take two slices bread put toaster wait toast done place toast preparation surface spread jelly jam toast overlapping peanut butter finished')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb135a2e",
   "metadata": {},
   "source": [
    "### Building Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec1ed3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bbc3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_vocab, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_vocab, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a0dbc",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder layer of the seq2seq model **extracts information** from the input text and encodes it into a single vector, that is a **context vector**. Basically, for each input word, the encoder generates a hidden state and a vector , using this hidden state for the next input word.\n",
    "\n",
    "We have used GRU(Gated Recurrent Unit) for the encoder layer in order to **capture long term dependencies** - mitigating the vanishing/exploding gradient problem encountered while working with vanilla RNNs. The GRU cell reads one word at a time and using the update and reset gate, computes the hidden state content and cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cae9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b48f0d",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The decoder layer of a seq2seq model uses the last hidden state of the encoder i.e. the **context vector** and generates the output words. The decoding process starts once the sentence has been encoded and the decoder is given a **hidden state** and an input token at each step/time.\n",
    "\n",
    "At the initial time stamp/state the first hidden state is the context vector and the input vector is **SOS(start-of-string)**. The decoding process ends when **EOS(end-of-sentence)** is reached. The SOS and EOS tokens are explicitly added at the start and end of each sentence respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9263e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec397261",
   "metadata": {},
   "source": [
    "#### Attention Mechanism\n",
    "\n",
    "Using the encoder-decoder architecture, the **encoded context vector** is passed on to the decoder to generate an output sentence. Now what if the input sentence is long and a single context vector cannot capture all the important information. **This is where attention comes into picture!!!**\n",
    "\n",
    "The main intuition of using attention is to allow the model to **focus/pay attention** on the most important part of the input text. As a blessing in disguise, it also helps to **overcome the vanishing gradient problem**. There are different types of attention — additive, multiplicative, however, we will use the basic dot product attention for our model.\n",
    "\n",
    "\n",
    "1. Attention scores are first calculated by computing the **dot product of the encoder(h) and decoder(s) hidden state**\n",
    "\n",
    "\n",
    "2. These attention scores are converted to a **distribution(α)** by passing them through the **Softmax layer**.\n",
    "\n",
    "\n",
    "3. Then the **weighted sum of the hidden states (z)** is computed.\n",
    "\n",
    "\n",
    "4. This z is then **concatenated with s** and fed through the softmax layer to generate the words using ‘**Greedy Algorithm**’ (by computing argmax)\n",
    "\n",
    "In this architecture, instead of directly using the output of last encoder’s hidden state, we are also feeding the **weighted combination of all the encoder hidden states**. This helps the model to pay attention to important words across long sequences.\n",
    "\n",
    "**Supporting Equations**\n",
    "\n",
    "![Attention Equations](../../../images/attentionEquation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31ee06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceddc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a72a3",
   "metadata": {},
   "source": [
    "#### Teacher Forcing\n",
    "\n",
    "In general, for recurrent neural networks, the output from a state is fed as an input to the next state. This process causes slow convergence thereby increasing the training time.\n",
    "\n",
    "\n",
    "\n",
    "#### What is Teacher Forcing\n",
    "\n",
    "Teacher forcing addresses this slow convergence problem by feeding the **actual value/ground truth** to the model. The basic intuition behind this technique is that instead of feeding the decoders predicted output as an input for the next state, the ground truth or the actual value is fed to the model. If the model predicts a wrong word it might lead to a condition wherein all the further words that are predicted are incorrect. Hence, teacher forcing feeds the actual value thereby **correcting the model** if it predicts/generates a wrong word.\n",
    "\n",
    "Teacher forcing is a fast and effective way to train RNNs, however, this approach may result in ***more fragile/unstable models*** when the generated sequences vary from what was seen during the training process.\n",
    "To deal with such an isssue, we will follow an approach that involves randomly choosing to use the ground truth output or the generated output from the previous time step as an input for current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2ed8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a51e1d",
   "metadata": {},
   "source": [
    "### Configure Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3736ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d5143da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a22ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print(\"Training....\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        if(input_length > 150):\n",
    "          #print(input_length)\n",
    "          continue\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e10deae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_vocab.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0655228",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c8d2e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "1m 46s (- 353m 27s) (500 0%) 5.8764\n",
      "1000 / 100001\n",
      "3m 20s (- 331m 22s) (1000 1%) 6.2445\n",
      "4m 56s (- 324m 2s) (1500 1%) 6.0801\n",
      "2000 / 100001\n",
      "6m 36s (- 323m 54s) (2000 2%) 6.3151\n",
      "8m 10s (- 318m 52s) (2500 2%) 6.0685\n",
      "3000 / 100001\n",
      "9m 48s (- 316m 58s) (3000 3%) 5.8522\n",
      "11m 21s (- 312m 57s) (3500 3%) 5.6258\n",
      "4000 / 100001\n",
      "12m 57s (- 310m 50s) (4000 4%) 5.7877\n",
      "14m 31s (- 308m 5s) (4500 4%) 5.6872\n",
      "5000 / 100001\n",
      "16m 7s (- 306m 19s) (5000 5%) 5.8692\n",
      "17m 43s (- 304m 31s) (5500 5%) 5.8990\n",
      "6000 / 100001\n",
      "19m 18s (- 302m 35s) (6000 6%) 5.7065\n",
      "20m 51s (- 300m 9s) (6500 6%) 5.5286\n",
      "7000 / 100001\n",
      "22m 28s (- 298m 40s) (7000 7%) 5.7503\n",
      "24m 8s (- 297m 49s) (7500 7%) 5.7814\n",
      "8000 / 100001\n",
      "25m 42s (- 295m 39s) (8000 8%) 5.6480\n",
      "27m 17s (- 293m 49s) (8500 8%) 5.3671\n",
      "9000 / 100001\n",
      "28m 50s (- 291m 34s) (9000 9%) 5.6408\n",
      "30m 23s (- 289m 32s) (9500 9%) 5.7285\n",
      "10000 / 100001\n",
      "31m 48s (- 286m 12s) (10000 10%) 5.6263\n",
      "33m 15s (- 283m 27s) (10500 10%) 5.9679\n",
      "11000 / 100001\n",
      "34m 42s (- 280m 49s) (11000 11%) 5.4814\n",
      "36m 15s (- 279m 5s) (11500 11%) 5.6427\n",
      "12000 / 100001\n",
      "37m 56s (- 278m 16s) (12000 12%) 5.6435\n",
      "39m 35s (- 277m 5s) (12500 12%) 5.7729\n",
      "13000 / 100001\n",
      "41m 12s (- 275m 49s) (13000 13%) 5.5847\n",
      "42m 48s (- 274m 18s) (13500 13%) 5.5354\n",
      "14000 / 100001\n",
      "44m 23s (- 272m 40s) (14000 14%) 5.5907\n",
      "46m 1s (- 271m 20s) (14500 14%) 5.5619\n",
      "15000 / 100001\n",
      "47m 30s (- 269m 11s) (15000 15%) 5.4497\n",
      "48m 58s (- 267m 1s) (15500 15%) 5.6241\n",
      "16000 / 100001\n",
      "50m 26s (- 264m 51s) (16000 16%) 5.7034\n",
      "51m 55s (- 262m 47s) (16500 16%) 5.7155\n",
      "17000 / 100001\n",
      "53m 25s (- 260m 48s) (17000 17%) 5.5438\n",
      "54m 53s (- 258m 45s) (17500 17%) 5.5949\n",
      "18000 / 100001\n",
      "56m 23s (- 256m 54s) (18000 18%) 5.5968\n",
      "57m 52s (- 254m 57s) (18500 18%) 5.5776\n",
      "19000 / 100001\n",
      "59m 21s (- 253m 3s) (19000 19%) 5.6471\n",
      "60m 50s (- 251m 10s) (19500 19%) 5.6568\n",
      "20000 / 100001\n",
      "62m 21s (- 249m 24s) (20000 20%) 5.5402\n",
      "63m 49s (- 247m 32s) (20500 20%) 5.5174\n",
      "21000 / 100001\n",
      "65m 19s (- 245m 45s) (21000 21%) 5.5132\n",
      "66m 49s (- 244m 1s) (21500 21%) 5.5846\n",
      "22000 / 100001\n",
      "68m 16s (- 242m 2s) (22000 22%) 5.5841\n",
      "69m 45s (- 240m 15s) (22500 22%) 5.5935\n",
      "23000 / 100001\n",
      "71m 12s (- 238m 24s) (23000 23%) 5.4856\n",
      "72m 42s (- 236m 42s) (23500 23%) 5.6774\n",
      "24000 / 100001\n",
      "74m 14s (- 235m 6s) (24000 24%) 5.5946\n",
      "75m 44s (- 233m 25s) (24500 24%) 5.5797\n",
      "25000 / 100001\n",
      "77m 13s (- 231m 40s) (25000 25%) 5.4969\n",
      "78m 41s (- 229m 53s) (25500 25%) 5.4741\n",
      "26000 / 100001\n",
      "80m 9s (- 228m 8s) (26000 26%) 5.4986\n",
      "81m 36s (- 226m 20s) (26500 26%) 5.1074\n",
      "27000 / 100001\n",
      "83m 6s (- 224m 41s) (27000 27%) 5.5099\n",
      "84m 34s (- 222m 59s) (27500 27%) 5.4516\n",
      "28000 / 100001\n",
      "86m 4s (- 221m 19s) (28000 28%) 5.6211\n",
      "87m 34s (- 219m 41s) (28500 28%) 5.4674\n",
      "29000 / 100001\n",
      "89m 5s (- 218m 7s) (29000 28%) 5.4063\n",
      "90m 37s (- 216m 33s) (29500 29%) 5.4964\n",
      "30000 / 100001\n",
      "92m 2s (- 214m 45s) (30000 30%) 5.5467\n",
      "93m 33s (- 213m 12s) (30500 30%) 5.5164\n",
      "31000 / 100001\n",
      "95m 1s (- 211m 31s) (31000 31%) 5.5642\n",
      "96m 28s (- 209m 48s) (31500 31%) 5.4753\n",
      "32000 / 100001\n",
      "97m 57s (- 208m 10s) (32000 32%) 5.3353\n",
      "99m 27s (- 206m 33s) (32500 32%) 5.4039\n",
      "33000 / 100001\n",
      "100m 57s (- 204m 58s) (33000 33%) 5.7200\n",
      "102m 23s (- 203m 16s) (33500 33%) 5.3548\n",
      "34000 / 100001\n",
      "103m 55s (- 201m 44s) (34000 34%) 5.5252\n",
      "105m 23s (- 200m 5s) (34500 34%) 5.3595\n",
      "35000 / 100001\n",
      "106m 55s (- 198m 35s) (35000 35%) 5.5214\n",
      "108m 29s (- 197m 6s) (35500 35%) 5.4946\n",
      "36000 / 100001\n",
      "109m 56s (- 195m 27s) (36000 36%) 5.3960\n",
      "111m 24s (- 193m 49s) (36500 36%) 5.2818\n",
      "37000 / 100001\n",
      "112m 54s (- 192m 15s) (37000 37%) 5.5124\n",
      "114m 26s (- 190m 43s) (37500 37%) 5.5086\n",
      "38000 / 100001\n",
      "115m 54s (- 189m 7s) (38000 38%) 5.4690\n",
      "117m 26s (- 187m 36s) (38500 38%) 5.5489\n",
      "39000 / 100001\n",
      "118m 54s (- 185m 59s) (39000 39%) 5.5446\n",
      "120m 24s (- 184m 26s) (39500 39%) 5.4412\n",
      "40000 / 100001\n",
      "121m 54s (- 182m 51s) (40000 40%) 5.6466\n",
      "123m 20s (- 181m 12s) (40500 40%) 5.3824\n",
      "41000 / 100001\n",
      "124m 49s (- 179m 36s) (41000 41%) 5.6657\n",
      "126m 18s (- 178m 2s) (41500 41%) 5.4551\n",
      "42000 / 100001\n",
      "127m 47s (- 176m 27s) (42000 42%) 5.3692\n",
      "129m 16s (- 174m 53s) (42500 42%) 5.4830\n",
      "43000 / 100001\n",
      "130m 45s (- 173m 20s) (43000 43%) 5.5280\n",
      "132m 15s (- 171m 47s) (43500 43%) 5.4853\n",
      "44000 / 100001\n",
      "133m 44s (- 170m 13s) (44000 44%) 5.2903\n",
      "135m 12s (- 168m 38s) (44500 44%) 5.4129\n",
      "45000 / 100001\n",
      "136m 45s (- 167m 8s) (45000 45%) 5.5404\n",
      "138m 14s (- 165m 34s) (45500 45%) 5.3764\n",
      "46000 / 100001\n",
      "139m 47s (- 164m 5s) (46000 46%) 5.5123\n",
      "141m 20s (- 162m 37s) (46500 46%) 5.5879\n",
      "47000 / 100001\n",
      "142m 51s (- 161m 5s) (47000 47%) 5.3406\n",
      "144m 26s (- 159m 38s) (47500 47%) 5.5910\n",
      "48000 / 100001\n",
      "145m 57s (- 158m 6s) (48000 48%) 5.2801\n",
      "147m 25s (- 156m 33s) (48500 48%) 5.3082\n",
      "49000 / 100001\n",
      "148m 56s (- 155m 1s) (49000 49%) 5.4445\n",
      "150m 26s (- 153m 28s) (49500 49%) 5.4479\n",
      "50000 / 100001\n",
      "151m 59s (- 151m 59s) (50000 50%) 5.4818\n",
      "153m 27s (- 150m 25s) (50500 50%) 5.3465\n",
      "51000 / 100001\n",
      "154m 52s (- 148m 48s) (51000 51%) 5.4108\n",
      "156m 21s (- 147m 14s) (51500 51%) 5.4960\n",
      "52000 / 100001\n",
      "157m 51s (- 145m 42s) (52000 52%) 5.4829\n",
      "159m 23s (- 144m 12s) (52500 52%) 5.4165\n",
      "53000 / 100001\n",
      "160m 53s (- 142m 40s) (53000 53%) 5.2618\n",
      "162m 23s (- 141m 8s) (53500 53%) 5.4523\n",
      "54000 / 100001\n",
      "163m 50s (- 139m 34s) (54000 54%) 5.1072\n",
      "165m 21s (- 138m 3s) (54500 54%) 5.5390\n",
      "55000 / 100001\n",
      "166m 53s (- 136m 32s) (55000 55%) 5.6190\n",
      "168m 24s (- 135m 1s) (55500 55%) 5.3846\n",
      "56000 / 100001\n",
      "169m 53s (- 133m 29s) (56000 56%) 5.4214\n",
      "171m 23s (- 131m 57s) (56500 56%) 5.3841\n",
      "57000 / 100001\n",
      "172m 53s (- 130m 25s) (57000 56%) 5.5000\n",
      "174m 25s (- 128m 55s) (57500 57%) 5.3796\n",
      "58000 / 100001\n",
      "175m 53s (- 127m 22s) (58000 57%) 5.3403\n",
      "177m 24s (- 125m 51s) (58500 58%) 5.3983\n",
      "59000 / 100001\n",
      "178m 58s (- 124m 22s) (59000 59%) 5.4566\n",
      "180m 26s (- 122m 49s) (59500 59%) 5.4065\n",
      "60000 / 100001\n",
      "181m 57s (- 121m 18s) (60000 60%) 5.3900\n",
      "183m 39s (- 119m 54s) (60500 60%) 5.4704\n",
      "61000 / 100001\n",
      "185m 12s (- 118m 24s) (61000 61%) 5.3079\n",
      "186m 50s (- 116m 57s) (61500 61%) 5.2586\n",
      "62000 / 100001\n",
      "188m 22s (- 115m 27s) (62000 62%) 5.2611\n",
      "189m 57s (- 113m 58s) (62500 62%) 5.3642\n",
      "63000 / 100001\n",
      "191m 33s (- 112m 30s) (63000 63%) 5.4657\n",
      "193m 4s (- 110m 59s) (63500 63%) 5.3672\n",
      "64000 / 100001\n",
      "194m 39s (- 109m 29s) (64000 64%) 5.4662\n",
      "196m 11s (- 107m 58s) (64500 64%) 5.3024\n",
      "65000 / 100001\n",
      "197m 39s (- 106m 26s) (65000 65%) 5.2189\n",
      "199m 5s (- 104m 51s) (65500 65%) 5.2336\n",
      "66000 / 100001\n",
      "200m 36s (- 103m 20s) (66000 66%) 5.4302\n",
      "202m 5s (- 101m 48s) (66500 66%) 5.2626\n",
      "67000 / 100001\n",
      "203m 36s (- 100m 17s) (67000 67%) 5.4464\n",
      "205m 7s (- 98m 45s) (67500 67%) 5.4556\n",
      "68000 / 100001\n",
      "206m 38s (- 97m 14s) (68000 68%) 5.2886\n",
      "208m 9s (- 95m 43s) (68500 68%) 5.3433\n",
      "69000 / 100001\n",
      "209m 44s (- 94m 14s) (69000 69%) 5.3493\n",
      "211m 23s (- 92m 46s) (69500 69%) 5.4566\n",
      "70000 / 100001\n",
      "212m 53s (- 91m 14s) (70000 70%) 5.1984\n",
      "214m 25s (- 89m 43s) (70500 70%) 5.3770\n",
      "71000 / 100001\n",
      "215m 54s (- 88m 11s) (71000 71%) 5.4239\n",
      "217m 23s (- 86m 39s) (71500 71%) 5.3740\n",
      "72000 / 100001\n",
      "218m 54s (- 85m 7s) (72000 72%) 5.3968\n",
      "220m 20s (- 83m 34s) (72500 72%) 5.4410\n",
      "73000 / 100001\n",
      "221m 51s (- 82m 3s) (73000 73%) 5.2915\n",
      "223m 24s (- 80m 32s) (73500 73%) 5.5023\n",
      "74000 / 100001\n",
      "224m 52s (- 79m 0s) (74000 74%) 5.2013\n",
      "226m 20s (- 77m 28s) (74500 74%) 5.4319\n",
      "75000 / 100001\n",
      "227m 50s (- 75m 56s) (75000 75%) 5.5368\n",
      "229m 20s (- 74m 25s) (75500 75%) 5.3859\n",
      "76000 / 100001\n",
      "230m 49s (- 72m 53s) (76000 76%) 5.6260\n",
      "232m 24s (- 71m 23s) (76500 76%) 5.4981\n",
      "77000 / 100001\n",
      "233m 54s (- 69m 52s) (77000 77%) 5.2731\n",
      "235m 24s (- 68m 20s) (77500 77%) 5.2640\n",
      "78000 / 100001\n",
      "236m 53s (- 66m 49s) (78000 78%) 5.3556\n",
      "238m 24s (- 65m 17s) (78500 78%) 5.3535\n",
      "79000 / 100001\n",
      "239m 54s (- 63m 46s) (79000 79%) 5.3897\n",
      "241m 29s (- 62m 16s) (79500 79%) 5.3073\n",
      "80000 / 100001\n",
      "243m 1s (- 60m 45s) (80000 80%) 5.3955\n",
      "244m 32s (- 59m 14s) (80500 80%) 5.2907\n",
      "81000 / 100001\n",
      "246m 5s (- 57m 43s) (81000 81%) 5.3739\n",
      "247m 39s (- 56m 12s) (81500 81%) 5.2968\n",
      "82000 / 100001\n",
      "249m 9s (- 54m 41s) (82000 82%) 5.2061\n",
      "250m 42s (- 53m 10s) (82500 82%) 5.3083\n",
      "83000 / 100001\n",
      "252m 12s (- 51m 39s) (83000 83%) 5.1833\n",
      "253m 44s (- 50m 8s) (83500 83%) 5.2015\n",
      "84000 / 100001\n",
      "255m 15s (- 48m 37s) (84000 84%) 5.4291\n",
      "256m 43s (- 47m 5s) (84500 84%) 5.3094\n",
      "85000 / 100001\n",
      "258m 11s (- 45m 33s) (85000 85%) 5.4384\n",
      "86000 / 100001\n",
      "261m 10s (- 42m 31s) (86000 86%) 10.6306\n",
      "262m 41s (- 40m 59s) (86500 86%) 5.3090\n",
      "87000 / 100001\n",
      "264m 10s (- 39m 28s) (87000 87%) 5.3341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265m 39s (- 37m 57s) (87500 87%) 5.2507\n",
      "88000 / 100001\n",
      "267m 11s (- 36m 26s) (88000 88%) 5.3485\n",
      "268m 41s (- 34m 54s) (88500 88%) 5.2585\n",
      "89000 / 100001\n",
      "270m 8s (- 33m 23s) (89000 89%) 5.1311\n",
      "271m 39s (- 31m 52s) (89500 89%) 5.3655\n",
      "90000 / 100001\n",
      "273m 10s (- 30m 21s) (90000 90%) 5.3830\n",
      "274m 38s (- 28m 49s) (90500 90%) 5.4955\n",
      "91000 / 100001\n",
      "276m 14s (- 27m 19s) (91000 91%) 5.3753\n",
      "277m 45s (- 25m 48s) (91500 91%) 5.3277\n",
      "92000 / 100001\n",
      "279m 22s (- 24m 17s) (92000 92%) 5.4159\n",
      "280m 56s (- 22m 46s) (92500 92%) 5.4032\n",
      "93000 / 100001\n",
      "282m 31s (- 21m 15s) (93000 93%) 5.3261\n",
      "284m 6s (- 19m 45s) (93500 93%) 5.2404\n",
      "94000 / 100001\n",
      "285m 42s (- 18m 14s) (94000 94%) 5.3669\n",
      "287m 15s (- 16m 43s) (94500 94%) 5.2596\n",
      "95000 / 100001\n",
      "288m 46s (- 15m 11s) (95000 95%) 5.4422\n",
      "290m 21s (- 13m 40s) (95500 95%) 5.3271\n",
      "96000 / 100001\n",
      "291m 50s (- 12m 9s) (96000 96%) 5.3535\n",
      "97000 / 100001\n",
      "294m 49s (- 9m 7s) (97000 97%) 10.3459\n",
      "296m 20s (- 7m 35s) (97500 97%) 5.2246\n",
      "98000 / 100001\n",
      "297m 49s (- 6m 4s) (98000 98%) 5.2454\n",
      "299m 20s (- 4m 33s) (98500 98%) 5.1660\n",
      "99000 / 100001\n",
      "300m 49s (- 3m 2s) (99000 99%) 5.4363\n",
      "302m 20s (- 1m 31s) (99500 99%) 5.1286\n",
      "100000 / 100001\n",
      "303m 48s (- 0m 0s) (100000 100%) 5.2914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(input_vocab.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_vocab.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "# This is just a tutorial, therefore we only use 3000 epochs here\n",
    "trainIters(encoder1, attn_decoder1, 100000, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ed57f",
   "metadata": {},
   "source": [
    "### Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97036b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1, \"model/encoder.model\")\n",
    "torch.save(attn_decoder1, \"model/decoder.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7cac7",
   "metadata": {},
   "source": [
    "### Load Trained Model\n",
    "This pretrained model is not optimized and you may expect low accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80264f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = torch.load(\"model/encoder.model\")\n",
    "attn_decoder1 = torch.load(\"model/decoder.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53457638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0284, -1.0606,  0.0629,  ..., -0.3006, -0.3855,  1.4225],\n",
      "        [ 0.2433,  0.2984,  1.3664,  ..., -0.0368,  0.0760,  0.3301],\n",
      "        [-0.1069, -0.9138,  1.7072,  ...,  1.6283,  0.5255,  0.7051],\n",
      "        ...,\n",
      "        [ 0.1800,  1.1256,  0.6946,  ..., -0.3186, -0.3207,  0.0827],\n",
      "        [ 1.5827,  0.5843,  1.1109,  ..., -0.3587, -0.0601,  0.6484],\n",
      "        [ 1.8425,  0.6422, -1.0052,  ...,  0.6684,  0.4895,  0.2602]],\n",
      "       device='cuda:0')\n",
      "tensor([[-5.5133e-02,  7.1059e-02,  6.2996e-02,  ...,  6.2187e-02,\n",
      "          1.8284e-02,  1.3226e-02],\n",
      "        [-3.5551e-02, -2.2046e-02, -4.2452e-02,  ...,  2.8883e-02,\n",
      "          4.2148e-02, -4.7835e-04],\n",
      "        [-9.0124e-02, -1.4641e-02,  1.2347e-02,  ..., -8.1382e-03,\n",
      "         -1.0085e-03, -4.6588e-02],\n",
      "        ...,\n",
      "        [ 4.2197e-01,  3.0737e-01,  4.7557e-04,  ..., -1.1926e-01,\n",
      "          5.3730e-02, -3.7813e-01],\n",
      "        [-1.8955e-01, -2.5875e-01, -5.6589e-02,  ...,  1.4348e-01,\n",
      "          1.0287e-01,  4.8600e-01],\n",
      "        [-2.9245e-01, -5.4736e-02,  1.2345e-01,  ...,  2.1021e-01,\n",
      "         -1.2508e-01,  5.2048e-01]], device='cuda:0')\n",
      "tensor([[ 0.1312, -0.1257,  0.0014,  ...,  0.0742,  0.0410,  0.0113],\n",
      "        [ 0.0587, -0.0632, -0.0255,  ...,  0.0057, -0.0079,  0.0272],\n",
      "        [-0.0583,  0.0283, -0.0144,  ..., -0.0178, -0.0179, -0.0544],\n",
      "        ...,\n",
      "        [-0.1148,  0.0192,  0.0939,  ...,  0.0758, -0.0559,  0.0149],\n",
      "        [ 0.1004, -0.0855, -0.0643,  ..., -0.0024,  0.0995, -0.0401],\n",
      "        [ 0.0469,  0.0269, -0.0100,  ..., -0.0316,  0.0387, -0.0571]],\n",
      "       device='cuda:0')\n",
      "tensor([-3.3211e-02,  2.7540e-02, -5.3305e-02,  6.8125e-03,  1.7290e-02,\n",
      "         1.0019e-01,  7.1418e-02,  1.1941e-02,  2.2669e-02, -9.9939e-02,\n",
      "         1.1414e-01,  2.2431e-02,  1.6083e-02, -1.5287e-02,  2.3067e-02,\n",
      "        -4.7035e-02,  6.4798e-02,  1.5091e-02, -9.0893e-02, -4.6488e-03,\n",
      "        -2.5313e-02, -5.0731e-02, -4.9437e-02, -7.3247e-02,  2.0424e-03,\n",
      "        -8.0063e-03, -1.5483e-02,  3.2143e-02, -8.3586e-02,  1.3996e-01,\n",
      "        -4.5137e-02, -2.7720e-02,  3.8117e-02,  2.3492e-02,  2.2097e-02,\n",
      "         7.0289e-02,  1.9566e-02,  1.9073e-02,  1.2269e-02, -6.8381e-03,\n",
      "         5.0375e-02, -3.3208e-02,  1.1314e-02, -8.4298e-02,  7.4151e-02,\n",
      "        -3.6840e-05,  2.0654e-02,  3.1061e-02,  2.1559e-02, -6.4381e-02,\n",
      "         6.9366e-02, -2.7601e-02, -3.9581e-02, -8.6379e-02,  6.0651e-02,\n",
      "         4.1577e-02,  4.4224e-02,  1.1046e-01, -2.8941e-02, -4.6372e-02,\n",
      "         1.2131e-01,  1.4844e-02, -8.2860e-02,  4.2027e-02, -2.3474e-02,\n",
      "        -2.5088e-03, -1.6339e-02,  4.7173e-02, -8.0875e-02,  1.1854e-02,\n",
      "         4.4184e-04,  1.5049e-01, -1.9813e-02, -8.1767e-02,  8.8513e-02,\n",
      "         4.7075e-02, -1.8600e-02,  5.5661e-02, -4.5593e-02, -3.5306e-02,\n",
      "        -7.9384e-02, -3.0790e-02,  1.0495e-01, -3.3840e-02, -4.2518e-02,\n",
      "         6.0443e-02,  1.0525e-01,  1.1643e-04,  2.0568e-02, -3.6085e-02,\n",
      "         6.2235e-02, -5.0744e-02, -5.3482e-03,  1.2206e-01,  4.3231e-02,\n",
      "        -1.8735e-02, -7.7047e-02,  6.5032e-02, -3.3506e-02, -5.9525e-02,\n",
      "        -1.7907e-02,  7.8795e-02,  5.9480e-02,  4.1035e-02, -7.6376e-02,\n",
      "        -2.6597e-03,  3.0595e-02, -9.9290e-04,  2.0142e-02, -3.2448e-02,\n",
      "         2.8345e-03, -3.4830e-02,  1.0624e-02,  3.2768e-02, -1.9278e-03,\n",
      "        -3.8433e-02,  4.9696e-02,  6.5286e-03,  5.1389e-02, -1.6745e-02,\n",
      "         1.5648e-02,  2.0220e-02, -6.7397e-02, -7.0733e-02,  8.5821e-03,\n",
      "         3.5936e-02, -1.0907e-01, -7.1245e-02,  2.9915e-02,  1.5195e-01,\n",
      "        -3.8399e-03, -3.2003e-02,  3.5408e-02,  1.4990e-01,  1.0695e-02,\n",
      "         1.0414e-01,  1.6899e-02,  3.7061e-02, -4.0841e-02, -2.0792e-02,\n",
      "         4.4171e-02, -1.3984e-02, -2.2517e-02,  1.1833e-01, -4.4360e-02,\n",
      "         3.7896e-03, -6.7901e-02,  8.8124e-02,  2.1806e-02, -2.2822e-03,\n",
      "         6.5852e-04,  3.3981e-02,  4.0773e-02,  3.9013e-02, -1.0453e-02,\n",
      "         1.1625e-02,  9.2984e-02,  1.2193e-01,  2.6949e-01,  4.1551e-03,\n",
      "         1.1633e-01, -2.7770e-02, -9.2812e-02, -9.5699e-02,  1.8478e-01,\n",
      "        -5.8019e-02, -4.4416e-02,  4.3826e-02,  9.6160e-02, -9.4020e-03,\n",
      "        -6.3143e-03,  6.9767e-02, -2.5777e-02,  2.0857e-02, -4.5140e-02,\n",
      "        -3.4152e-02,  1.5206e-01, -6.8526e-02,  1.1868e-03, -1.2983e-03,\n",
      "         1.6652e-01,  1.9572e-03, -5.0934e-02, -9.2298e-02, -6.7200e-02,\n",
      "         3.1234e-02, -3.2246e-02, -1.0144e-01, -3.4149e-03,  5.3247e-02,\n",
      "        -4.6981e-02,  1.9367e-02,  9.4201e-03, -3.3885e-02, -4.6152e-02,\n",
      "        -2.7578e-03, -6.7599e-02, -3.4345e-04,  1.6516e-02,  1.8580e-01,\n",
      "         5.2527e-02,  5.7781e-02, -5.0186e-02, -2.9764e-02, -2.8032e-02,\n",
      "         1.3524e-02, -2.7115e-02,  1.2964e-01, -2.8879e-02,  6.5821e-02,\n",
      "        -4.4507e-02,  6.8708e-03, -6.3990e-02,  5.8461e-03, -1.8475e-02,\n",
      "         2.7511e-01, -4.5042e-02, -2.7788e-02, -2.7539e-02, -5.8801e-02,\n",
      "         6.3577e-03, -6.4257e-02,  9.5501e-02, -8.7675e-03,  1.5658e-01,\n",
      "         2.7228e-02, -1.9666e-02, -1.0377e-02, -6.9553e-02,  1.0859e-02,\n",
      "        -2.0180e-02, -5.0911e-03, -1.2829e-02, -3.1734e-02, -3.2543e-02,\n",
      "         5.9883e-03, -4.7661e-02,  4.6469e-02, -3.1144e-02, -3.8961e-02,\n",
      "         5.9468e-02, -2.8855e-02, -3.0162e-02, -2.6340e-02,  1.9321e-02,\n",
      "         5.0293e-02, -8.4012e-02,  1.2627e-02, -3.8288e-02, -4.6288e-02,\n",
      "         9.3673e-02, -4.4164e-02, -2.4271e-03, -7.9405e-02, -8.6704e-03,\n",
      "        -8.3666e-03,  2.4670e-02,  1.2264e-01, -9.2197e-03,  1.6932e-02,\n",
      "         1.3430e-01, -5.3479e-02,  6.5804e-02,  1.7066e-02, -6.7932e-02,\n",
      "        -2.6496e-02,  1.4511e-01, -3.1186e-02,  2.9860e-02, -3.7835e-02,\n",
      "        -3.1114e-02, -5.3127e-03,  5.7756e-02,  2.2160e-02,  7.2143e-02,\n",
      "        -5.5396e-03, -3.5536e-02, -6.0422e-02, -9.4329e-02, -3.8401e-03,\n",
      "        -6.0334e-02, -1.9725e-02, -8.5742e-02,  2.8891e-02, -4.9873e-02,\n",
      "        -6.1124e-02,  1.7927e-01,  2.8679e-03, -2.7436e-02,  6.8734e-03,\n",
      "         2.0113e-02, -8.7378e-02,  2.2982e-02, -3.2306e-02, -6.3619e-02,\n",
      "        -6.2681e-02, -3.1804e-02, -1.8536e-02,  1.0219e-01, -4.7883e-02,\n",
      "         4.5277e-01,  3.3198e-01,  6.5122e-01,  2.6657e-01, -2.9742e-02,\n",
      "         6.4084e-01,  6.4859e-01,  4.9994e-02,  8.1543e-01,  2.7885e-01,\n",
      "         1.0927e+00,  1.8652e-01,  2.9752e-01,  8.0971e-01,  3.2459e-02,\n",
      "         5.8911e-01,  7.6649e-01,  7.9706e-01,  2.5736e-01,  2.0114e-01,\n",
      "         1.2241e-01, -6.2655e-02,  5.3396e-01,  2.8267e-01,  1.2518e+00,\n",
      "         5.9497e-01,  7.6396e-01,  6.4270e-01,  2.5549e-01,  4.9303e-01,\n",
      "         6.6874e-01,  4.3461e-02,  1.0815e+00,  1.3594e-01,  3.8458e-01,\n",
      "         3.2820e-01,  3.3011e-01,  1.9799e-01,  9.2563e-03,  2.3898e-01,\n",
      "         1.0556e+00,  4.5518e-01, -1.3051e-02,  4.5240e-01,  3.3134e-01,\n",
      "         6.7412e-01,  7.0886e-01,  8.2237e-01,  3.7637e-01,  4.9406e-01,\n",
      "         3.3510e-01,  3.1419e-01,  8.8913e-01,  3.0817e-02,  5.0044e-01,\n",
      "         2.5975e-01,  3.8289e-01, -2.6162e-01,  1.6405e-01,  6.5765e-01,\n",
      "         1.4300e+00,  8.7336e-01,  1.1599e+00,  1.1163e+00,  2.2910e-01,\n",
      "         1.2521e-01,  1.4721e-01,  5.5013e-01,  4.0075e-01,  8.1967e-01,\n",
      "         8.5296e-02,  1.2636e+00,  7.2610e-01,  2.7735e-03,  3.1576e-01,\n",
      "         7.1257e-01,  5.1429e-01, -8.6092e-02,  6.0783e-01,  8.9015e-01,\n",
      "         2.7544e-01,  6.0055e-01,  7.7204e-01,  8.2733e-01,  5.0234e-01,\n",
      "         5.6602e-01,  2.0566e-01,  6.1435e-01,  4.2726e-02,  2.7330e-01,\n",
      "         6.6612e-01,  4.1093e-01,  7.5359e-01,  7.0448e-01,  9.9598e-02,\n",
      "         6.0394e-01,  6.2990e-02,  2.4571e-01,  2.0223e-01,  4.5674e-01,\n",
      "         6.1276e-01,  5.2674e-02,  2.3619e-01,  1.1552e-01,  5.4557e-01,\n",
      "         8.3069e-01,  2.0408e-01,  3.4004e-01,  6.8486e-01,  3.4435e-01,\n",
      "         4.2573e-01,  6.3900e-01,  4.4101e-02,  1.2074e-01,  1.6890e-01,\n",
      "         5.2201e-01, -6.4399e-02,  5.2120e-02,  5.1357e-01,  5.0753e-01,\n",
      "         5.2460e-01,  6.4765e-01,  1.0704e-01,  5.5024e-01,  1.7687e-01,\n",
      "        -1.5241e-01,  7.6036e-02,  1.7831e-02,  9.4796e-01, -1.6875e-01,\n",
      "         5.5060e-01,  3.9510e-01,  8.0389e-02, -2.8124e-01,  7.1197e-01,\n",
      "         1.7992e-01,  9.8750e-01,  4.7407e-01,  1.7040e-01,  2.2853e-01,\n",
      "         4.6010e-02,  3.4589e-02,  5.5316e-01,  1.0316e+00,  1.2624e+00,\n",
      "         4.3707e-01,  3.8805e-01,  7.0140e-02,  1.6721e+00,  7.5140e-01,\n",
      "        -1.8207e-02,  6.1600e-01,  1.4033e-01,  3.8309e-01,  4.7596e-01,\n",
      "         4.4189e-01,  6.0217e-01,  2.8583e-01,  1.2584e+00,  9.9095e-01,\n",
      "         2.9409e-01,  4.0991e-01,  2.4890e-01,  1.5118e-01, -2.4939e-01,\n",
      "         4.2255e-01,  4.8098e-01,  6.6719e-01, -1.3520e-01,  4.8708e-01,\n",
      "         3.6783e-01,  2.5510e-01,  1.6688e-01,  3.1975e-01,  7.2430e-01,\n",
      "         3.3358e-01,  5.5341e-01,  9.7964e-02,  3.7098e-01,  2.2462e-01,\n",
      "         7.9071e-01,  2.0747e-01,  1.0422e+00,  5.9270e-01,  3.2888e-01,\n",
      "         6.5763e-01,  5.5723e-01,  4.4332e-01, -1.4971e-02,  1.6590e-01,\n",
      "         1.7931e-01,  8.0742e-01,  1.4893e-01,  5.8943e-02, -4.1679e-02,\n",
      "         6.5851e-01,  5.7302e-02,  1.9727e-02,  8.4503e-01, -1.2665e-01,\n",
      "         1.8311e-01,  6.7893e-01,  4.1652e-01,  6.9839e-02,  7.8667e-01,\n",
      "         3.3720e-02,  6.9792e-01, -3.9095e-01,  4.2599e-01,  7.2250e-01,\n",
      "         2.9697e-01,  5.0078e-01,  1.5679e-01, -3.5900e-02,  4.8934e-01,\n",
      "         1.3971e+00,  8.6704e-01,  9.0683e-02,  1.9214e-01,  7.7666e-01,\n",
      "         6.5274e-01,  4.9396e-01,  1.4717e+00,  6.3347e-01,  7.1382e-02,\n",
      "         2.5758e-01,  2.5610e-01,  2.7013e-01,  3.0199e-01,  7.3639e-01,\n",
      "         1.1233e-01,  1.7241e-01,  4.6219e-01,  3.6214e-01,  1.6455e-01,\n",
      "         1.2861e+00,  1.8695e-01,  3.2496e-01,  1.0762e+00,  4.6071e-01,\n",
      "         6.6988e-01,  2.4955e-03,  6.0766e-01,  7.1299e-01, -3.1415e-02,\n",
      "         2.9312e-01,  5.1927e-01,  2.1113e-01,  8.7533e-01,  1.9413e-02,\n",
      "         3.8088e-01,  3.8893e-01,  6.3111e-01,  3.0561e-01,  1.2987e-01,\n",
      "         8.4844e-01,  5.7743e-01,  4.5915e-02,  5.1916e-01,  9.4280e-01,\n",
      "         5.3281e-02,  1.7221e-01,  1.2763e-01,  1.2507e-01,  1.3207e-01,\n",
      "         4.1376e-02,  2.7035e-01,  6.0530e-01,  1.6082e-01,  3.7453e-01,\n",
      "         4.9023e-01,  3.4901e-01,  3.6216e-01,  1.7844e-01,  3.9472e-01,\n",
      "         4.0061e-01, -3.0652e-02,  5.9682e-01,  2.4259e-01,  1.5948e-01,\n",
      "         2.9824e-01,  7.4503e-02,  2.9100e-01,  3.0879e-01,  1.4227e-01,\n",
      "         2.9088e-01, -1.2620e-01,  3.5226e-01,  6.3945e-01,  3.4636e-02,\n",
      "         1.1528e+00,  1.0895e-01,  7.6333e-02,  1.7681e-01,  1.0888e-01,\n",
      "         3.9206e-01,  5.1485e-02,  1.2535e+00,  5.6342e-01,  9.0989e-01,\n",
      "         1.6996e-01, -5.9137e-01, -3.7166e-01,  2.5101e-01, -1.0833e+00,\n",
      "         2.2202e-01,  5.8854e-01, -1.4887e-01,  6.3376e-01, -1.2277e-01,\n",
      "        -3.8554e-01,  1.5429e-01, -4.6646e-01, -1.5314e-01, -5.3052e-01,\n",
      "         2.2644e-01,  5.7547e-01, -4.4973e-01,  2.1185e-01,  5.1729e-01,\n",
      "        -2.2237e-01, -6.0066e-01, -7.0373e-01, -3.6085e-02,  5.7454e-01,\n",
      "        -3.6196e-01, -2.1730e-01,  6.0890e-01,  2.2272e-01,  1.0017e+00,\n",
      "        -1.2933e-01,  1.7184e-01,  8.5397e-01,  4.3381e-01, -9.7509e-01,\n",
      "        -6.9945e-01,  5.5747e-01, -8.8741e-02, -2.3481e-01, -6.5689e-01,\n",
      "         6.6269e-01, -5.0623e-01, -1.7797e-01, -1.1132e-01,  1.1168e+00,\n",
      "         5.8720e-01, -6.3480e-02, -2.5944e-01,  4.2681e-01,  1.2407e-01,\n",
      "         5.8628e-01, -4.6168e-01, -5.2570e-01,  3.1701e-01, -6.4327e-01,\n",
      "         1.6862e-01, -5.4955e-01,  1.3192e+00, -7.2940e-01,  1.4220e-01,\n",
      "         5.0157e-01, -8.1712e-01,  2.7127e-01,  2.0132e-01,  6.9269e-01,\n",
      "         5.4972e-01,  5.2906e-01, -7.6719e-01,  7.1087e-02,  4.6820e-01,\n",
      "         5.2603e-01, -1.9001e-01,  5.4721e-01,  1.5543e-01,  6.6493e-01,\n",
      "        -2.9101e-01,  1.4146e-01,  8.2587e-01, -3.8386e-01, -2.4227e-01,\n",
      "        -1.0252e-01,  9.8453e-02, -5.6107e-01,  1.2733e-01, -5.0006e-01,\n",
      "        -8.1824e-01, -8.9601e-01,  2.5572e-01, -7.2832e-01, -2.2646e-01,\n",
      "        -6.4934e-01,  2.2726e-01, -4.4905e-01, -7.6845e-01,  2.7036e-01,\n",
      "        -1.6112e-01,  9.2672e-02,  1.6225e-01, -3.7947e-01,  6.6533e-03,\n",
      "        -1.0439e-01, -8.7820e-01,  6.9778e-02,  5.3286e-01, -6.2130e-01,\n",
      "        -8.2724e-01, -7.4524e-01, -5.8082e-02, -1.7325e-01, -2.8026e-01,\n",
      "        -2.0057e-01,  5.2139e-01,  7.2638e-01, -3.4052e-02,  4.9348e-01,\n",
      "         2.0988e-01, -6.7399e-01, -5.9804e-01,  4.6153e-01, -6.0650e-01,\n",
      "         2.3198e-01,  5.9559e-01, -2.6936e-01,  2.0812e-01,  2.3534e-01,\n",
      "        -9.9034e-01, -3.0859e-01, -1.6119e-01,  7.8356e-01,  1.3314e+00,\n",
      "        -1.1895e-01, -2.7529e-01, -9.6550e-01, -1.3292e+00, -5.2659e-01,\n",
      "         9.0600e-01, -3.0320e-01, -4.7035e-01, -2.8240e-01, -4.5168e-01,\n",
      "        -5.0257e-01, -2.8097e-02,  4.6903e-01,  4.3817e-01,  4.7002e-02,\n",
      "        -3.6119e-01,  1.4313e-01,  6.1169e-01,  2.3838e-01,  1.3917e-01,\n",
      "        -1.1057e+00, -7.2361e-01,  4.9524e-01,  7.9991e-01, -4.7121e-01,\n",
      "         3.0502e-02, -3.7883e-01,  2.3121e-01,  1.1505e+00,  3.1581e-01,\n",
      "        -8.0148e-01,  1.3269e-01,  3.3737e-02,  1.2487e-02,  1.1516e+00,\n",
      "        -1.8569e-01,  4.3228e-01,  5.9198e-01, -1.2288e+00, -4.2804e-02,\n",
      "         1.0862e-01, -7.8050e-01,  4.6060e-01, -7.0066e-02, -1.2521e-01,\n",
      "        -4.5662e-01, -3.3143e-01, -1.1458e-01,  1.6054e-01, -2.7927e-01,\n",
      "         8.0936e-01,  8.8080e-02,  2.4781e-01,  1.5984e-01,  7.7722e-01,\n",
      "         3.4574e-01, -1.2990e-01, -5.1302e-01,  1.3812e-01, -9.1917e-01,\n",
      "         4.6905e-01,  3.3237e-01, -1.3819e-01, -4.7867e-01,  3.6207e-01,\n",
      "        -4.0370e-01,  2.5272e-01,  1.4229e-01, -2.0642e-01, -1.2329e+00,\n",
      "         4.6948e-01, -6.9326e-01,  5.7373e-01, -6.9529e-01, -9.2977e-01,\n",
      "        -8.5440e-02,  6.3715e-01, -1.4373e+00,  9.2378e-02, -4.5182e-01,\n",
      "        -6.4927e-02,  2.3489e-01, -3.0447e-01, -7.7881e-01, -2.2128e-01,\n",
      "        -5.9117e-01,  5.7440e-02, -3.0553e-01,  3.9957e-02,  1.3418e-01,\n",
      "        -1.5773e-01,  8.8663e-02, -1.2606e-01, -1.0357e-01, -1.2145e+00,\n",
      "         5.4724e-01, -4.9498e-02, -1.5718e-01, -1.9358e-01, -6.2284e-01,\n",
      "        -6.3250e-01,  8.6486e-02,  2.7774e-01, -1.2349e-01,  7.8810e-01,\n",
      "         3.4613e-01, -6.7603e-01, -4.5186e-01, -4.4043e-01, -4.6945e-01,\n",
      "         6.1923e-01, -9.1642e-02, -1.5143e-01,  1.1416e-01,  7.3777e-01,\n",
      "        -7.3895e-01, -7.0404e-01, -4.8634e-01, -7.0345e-01, -5.4933e-01,\n",
      "        -9.1267e-01,  1.4367e-01, -1.9848e-01,  4.2795e-01,  1.0501e-01,\n",
      "        -8.4212e-02, -6.3259e-01, -1.2667e+00, -4.3450e-01,  3.2802e-01,\n",
      "        -9.7248e-01, -2.9417e-01, -1.1165e+00,  4.1480e-01, -8.1864e-02,\n",
      "        -5.3476e-01,  9.7952e-01,  1.6095e-01, -8.1562e-01,  3.4539e-02,\n",
      "        -3.5204e-01,  1.2539e-01,  3.1280e-01,  1.5554e-01,  7.9695e-01,\n",
      "        -2.3260e-01, -7.6519e-01,  8.9363e-03,  9.1337e-02,  5.4749e-01,\n",
      "        -1.6444e-01, -2.5907e-01,  1.6747e-01,  6.3652e-02, -3.0829e-01,\n",
      "        -1.7358e-01, -1.2279e+00, -7.3151e-01,  3.9856e-01, -6.6035e-01,\n",
      "         2.3447e-01, -1.7656e-01, -8.7841e-01,  1.2591e-01,  1.0927e-01,\n",
      "        -2.0081e-01, -3.7791e-01, -6.2858e-01,  7.3953e-01,  7.3857e-02],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.6646e-02,  1.1474e-01, -6.6032e-03, -3.8511e-02,  1.7487e-02,\n",
      "        -8.1848e-03,  1.7504e-02, -9.7324e-02,  6.9095e-02, -1.6052e-02,\n",
      "         3.7340e-02, -7.5788e-02,  7.7691e-03,  6.9178e-03,  1.7333e-02,\n",
      "        -1.3231e-02,  3.1757e-03,  1.0645e-02, -5.4142e-02, -4.4393e-02,\n",
      "         4.0253e-02, -5.9151e-02, -1.7486e-02, -9.2155e-02, -7.1564e-02,\n",
      "        -3.7541e-03,  8.6198e-03,  7.3597e-02, -4.9441e-02,  1.1707e-01,\n",
      "         5.9577e-03, -6.3402e-02,  9.3434e-02,  6.0238e-03,  8.7086e-02,\n",
      "        -1.8190e-02,  1.3497e-02,  1.0311e-02, -7.7909e-02,  2.0985e-02,\n",
      "         7.2763e-02, -3.2815e-02,  4.2623e-02, -7.4975e-02,  7.5302e-02,\n",
      "        -1.1392e-02,  3.0685e-02,  3.6488e-02, -4.4808e-02, -4.5440e-02,\n",
      "         6.1474e-02,  1.0399e-02, -1.2271e-02, -3.9612e-02,  4.2076e-02,\n",
      "        -3.5066e-02,  4.0348e-02,  1.5602e-01,  3.5547e-02, -5.2025e-02,\n",
      "         1.3524e-01,  6.3841e-02, -1.5306e-02,  1.7152e-02, -2.4856e-02,\n",
      "         2.4815e-02, -5.2088e-02,  2.9704e-02, -4.0585e-02, -8.7194e-02,\n",
      "        -6.0053e-02,  1.2123e-01, -3.5326e-02, -5.5797e-02,  4.9214e-02,\n",
      "         2.5131e-02, -2.1279e-02,  4.7347e-02,  1.2603e-02,  4.6609e-02,\n",
      "        -3.7582e-02, -3.9847e-02,  1.2460e-01,  2.2919e-02,  2.6867e-02,\n",
      "         4.0135e-02,  8.8734e-02,  3.3006e-02, -8.3030e-02,  7.2133e-03,\n",
      "         1.1613e-02,  1.9984e-02, -4.6211e-02,  2.4444e-02,  3.6173e-02,\n",
      "        -2.2417e-02, -8.1101e-02,  2.7151e-02, -5.9237e-02,  3.1947e-02,\n",
      "        -1.8258e-02,  6.1178e-03,  1.4501e-02, -1.4651e-02, -7.0834e-02,\n",
      "         2.3535e-02,  2.0385e-02, -6.6297e-03, -3.8367e-02, -8.1235e-03,\n",
      "        -3.5216e-02,  3.9056e-02,  3.7537e-02,  7.1128e-03, -2.7543e-03,\n",
      "         3.0839e-02,  2.9505e-02,  7.9713e-03,  7.6227e-02,  2.3031e-02,\n",
      "         1.5521e-02,  2.3396e-02,  2.2449e-03, -3.6016e-02, -1.7707e-02,\n",
      "        -4.5273e-03, -3.0386e-02,  5.5573e-03,  2.1826e-02,  1.2797e-01,\n",
      "        -3.4034e-03, -5.4763e-02,  1.7776e-02,  8.4757e-02,  4.2436e-02,\n",
      "         3.6638e-02, -4.2491e-03,  8.8004e-02, -5.9303e-03, -1.5839e-02,\n",
      "        -5.3525e-02, -4.9730e-02, -3.1650e-02,  1.2997e-01,  2.6309e-02,\n",
      "         2.2998e-02,  4.3704e-03,  6.4378e-03,  1.0438e-01, -8.7913e-02,\n",
      "         2.1132e-02,  1.2342e-02, -1.4820e-02,  7.5310e-02,  1.5426e-02,\n",
      "        -5.8710e-03,  9.7042e-02,  5.7716e-02,  2.3648e-01, -3.0572e-02,\n",
      "         1.2946e-01, -4.6654e-02, -3.3512e-02,  1.1559e-02,  1.6896e-01,\n",
      "        -4.1855e-02, -9.1133e-03,  5.2446e-02,  6.1750e-02, -1.5251e-02,\n",
      "         2.4206e-02,  1.1435e-02,  3.6029e-02, -1.0683e-02, -4.3860e-02,\n",
      "        -4.5568e-02,  8.3336e-02, -7.8762e-03, -3.6033e-02, -7.4169e-02,\n",
      "         2.1543e-01,  5.4837e-03, -5.5347e-02, -4.0262e-03,  1.7205e-02,\n",
      "         5.6511e-02, -3.5118e-03, -2.8213e-02, -6.8364e-02,  4.0943e-02,\n",
      "        -5.3030e-02, -4.5830e-02,  6.4513e-02,  1.3929e-02,  1.7774e-03,\n",
      "        -4.6507e-03, -1.1802e-01, -8.5266e-03,  1.1879e-01,  1.7659e-01,\n",
      "         5.3024e-03,  8.9206e-02,  1.2279e-02, -1.9644e-02, -2.2698e-03,\n",
      "        -4.5934e-02, -9.7173e-04,  1.5535e-01,  1.7079e-02,  5.3821e-02,\n",
      "        -4.4214e-02,  3.9350e-02,  9.7034e-03, -5.6927e-02,  5.3739e-02,\n",
      "         2.0102e-01,  5.8179e-02, -6.9484e-02, -3.7584e-02, -7.8963e-03,\n",
      "        -4.1322e-02, -2.5693e-02,  1.4436e-01, -1.0226e-01,  1.0372e-01,\n",
      "         2.1337e-02, -2.3416e-02, -9.3521e-02, -7.2525e-03,  5.1204e-02,\n",
      "         1.5142e-02,  8.5622e-03, -4.1402e-02, -4.8649e-02, -4.9736e-03,\n",
      "         2.3563e-02, -2.7896e-02, -8.0583e-03, -7.0354e-03, -3.2855e-02,\n",
      "         2.4183e-02, -5.9352e-02, -1.2891e-02,  3.9431e-02,  4.3009e-02,\n",
      "        -3.5373e-02, -4.0403e-02,  1.9152e-03,  2.2934e-02,  5.6622e-02,\n",
      "         1.1486e-01, -6.0227e-02, -1.0092e-01, -4.4323e-02, -7.2324e-02,\n",
      "         2.2591e-02,  1.6775e-02,  1.6370e-01, -3.6006e-02,  8.3300e-02,\n",
      "         1.2679e-01, -3.1396e-02,  2.1556e-02,  1.0220e-02,  2.7179e-02,\n",
      "        -5.0151e-02,  4.7016e-02,  9.3883e-04, -7.5909e-03, -7.2395e-02,\n",
      "         1.8118e-02,  2.0061e-02,  7.9034e-02, -6.5368e-02,  1.9480e-02,\n",
      "        -6.8183e-04,  2.1638e-02,  1.2198e-02, -9.4147e-02, -3.1915e-02,\n",
      "         5.5757e-04, -1.1975e-02, -8.9124e-02,  3.5648e-02,  2.9617e-02,\n",
      "        -8.7960e-03,  1.8467e-01, -4.3611e-02,  2.6118e-02,  4.5281e-02,\n",
      "        -1.6780e-02, -2.6301e-02,  2.6465e-02, -7.7893e-02, -9.4268e-03,\n",
      "        -1.7107e-02, -5.1640e-02, -6.9482e-02,  9.6947e-02,  2.6883e-02,\n",
      "         4.0109e-01,  4.0214e-01,  6.5871e-01,  2.7760e-01, -6.4489e-02,\n",
      "         6.6139e-01,  7.0073e-01,  1.0004e-01,  8.3976e-01,  2.7969e-01,\n",
      "         9.9721e-01,  1.5533e-01,  1.8463e-01,  7.3279e-01, -3.7354e-02,\n",
      "         5.6730e-01,  7.6604e-01,  8.8169e-01,  2.2264e-01,  1.3359e-01,\n",
      "         1.1295e-01, -4.4506e-02,  5.6390e-01,  2.3512e-01,  1.2916e+00,\n",
      "         6.3119e-01,  7.8550e-01,  5.9509e-01,  1.9316e-01,  4.9956e-01,\n",
      "         7.1015e-01,  1.0835e-01,  1.0562e+00,  1.4355e-01,  4.3973e-01,\n",
      "         2.6839e-01,  2.3619e-01,  1.4248e-01,  6.6926e-02,  2.4901e-01,\n",
      "         9.7800e-01,  4.7996e-01, -4.8235e-02,  3.8520e-01,  2.5662e-01,\n",
      "         7.2127e-01,  6.8235e-01,  8.6313e-01,  4.2624e-01,  4.2388e-01,\n",
      "         2.7357e-01,  3.1394e-01,  8.2287e-01, -2.6534e-02,  4.7178e-01,\n",
      "         2.0632e-01,  3.7527e-01, -2.5411e-01,  1.8078e-01,  7.2991e-01,\n",
      "         1.3901e+00,  8.6097e-01,  1.1856e+00,  1.1414e+00,  2.4786e-01,\n",
      "         1.3949e-01,  1.0893e-01,  4.6429e-01,  3.9837e-01,  8.5188e-01,\n",
      "         6.5815e-02,  1.2404e+00,  7.9037e-01,  2.1004e-02,  3.6639e-01,\n",
      "         6.8896e-01,  4.8110e-01, -7.8245e-02,  6.0761e-01,  8.8960e-01,\n",
      "         3.1568e-01,  6.4505e-01,  7.9992e-01,  8.2424e-01,  4.3015e-01,\n",
      "         5.8033e-01,  1.2521e-01,  6.7050e-01,  8.1659e-02,  2.1222e-01,\n",
      "         6.9415e-01,  4.4922e-01,  7.4127e-01,  7.3347e-01,  8.9720e-02,\n",
      "         5.9855e-01,  3.9416e-02,  2.8778e-01,  1.6748e-01,  3.9793e-01,\n",
      "         5.4286e-01,  5.0487e-03,  2.7355e-01,  7.1868e-02,  5.3221e-01,\n",
      "         8.8312e-01,  2.5700e-01,  3.1570e-01,  6.9487e-01,  3.8187e-01,\n",
      "         4.4580e-01,  6.2885e-01,  1.2656e-01,  1.2540e-01,  1.4360e-01,\n",
      "         5.4549e-01, -9.6745e-02,  3.9781e-02,  5.2473e-01,  4.5635e-01,\n",
      "         4.9549e-01,  6.8903e-01,  2.0999e-01,  5.6931e-01,  1.9775e-01,\n",
      "        -9.5142e-02,  6.1524e-02,  6.2623e-02,  8.8853e-01, -2.0241e-01,\n",
      "         5.7662e-01,  4.2923e-01,  1.2663e-02, -2.4452e-01,  7.4607e-01,\n",
      "         7.4884e-02,  9.3566e-01,  4.6098e-01,  1.2199e-01,  2.1640e-01,\n",
      "         2.2517e-02, -4.3559e-02,  5.7219e-01,  1.0680e+00,  1.1851e+00,\n",
      "         4.4816e-01,  3.2136e-01,  1.8694e-02,  1.6862e+00,  7.0561e-01,\n",
      "         5.0715e-02,  5.9734e-01,  1.0879e-01,  2.8219e-01,  4.5603e-01,\n",
      "         4.8420e-01,  6.3350e-01,  3.1841e-01,  1.2446e+00,  1.0035e+00,\n",
      "         3.4734e-01,  4.1207e-01,  2.6512e-01,  7.4421e-02, -2.8685e-01,\n",
      "         3.9843e-01,  4.5306e-01,  6.1551e-01, -2.3171e-01,  4.8858e-01,\n",
      "         4.0310e-01,  2.4685e-01,  1.6819e-01,  2.3654e-01,  7.6202e-01,\n",
      "         3.3504e-01,  5.5625e-01,  1.1987e-01,  3.4251e-01,  2.2567e-01,\n",
      "         8.0568e-01,  1.8870e-01,  1.1312e+00,  5.3240e-01,  3.9397e-01,\n",
      "         6.0301e-01,  5.2943e-01,  4.0751e-01, -8.4896e-02,  1.9306e-01,\n",
      "         1.7802e-01,  8.4723e-01,  2.2763e-01, -3.7374e-02, -9.3420e-03,\n",
      "         6.3519e-01,  4.7475e-02,  3.3199e-02,  8.3826e-01, -1.6702e-01,\n",
      "         9.0351e-02,  7.1312e-01,  4.0833e-01,  1.4023e-01,  7.1748e-01,\n",
      "        -6.2189e-02,  7.8657e-01, -3.4279e-01,  3.8068e-01,  6.7592e-01,\n",
      "         2.5186e-01,  4.6713e-01,  1.9433e-01, -4.0981e-02,  5.0984e-01,\n",
      "         1.3852e+00,  8.9476e-01,  1.4689e-01,  2.4100e-01,  7.0459e-01,\n",
      "         6.8453e-01,  5.7818e-01,  1.4128e+00,  6.9103e-01,  7.0984e-02,\n",
      "         2.6915e-01,  3.1188e-01,  3.4523e-01,  3.1459e-01,  7.8330e-01,\n",
      "         1.8135e-01,  2.7130e-01,  4.7703e-01,  3.1435e-01,  9.1823e-02,\n",
      "         1.3501e+00,  1.8877e-01,  2.8686e-01,  1.1289e+00,  4.5458e-01,\n",
      "         7.8076e-01,  4.0806e-02,  6.4492e-01,  7.6362e-01, -1.6611e-02,\n",
      "         2.9351e-01,  5.5899e-01,  1.4855e-01,  8.2603e-01,  4.2743e-02,\n",
      "         3.2879e-01,  4.2397e-01,  6.3345e-01,  2.9769e-01,  2.0769e-01,\n",
      "         8.8062e-01,  5.7630e-01,  3.4010e-02,  5.0782e-01,  9.1021e-01,\n",
      "         3.7768e-03,  9.1475e-02,  1.9447e-01,  1.1495e-01,  7.8180e-02,\n",
      "         1.3274e-01,  2.2304e-01,  6.5127e-01,  1.6438e-01,  3.6093e-01,\n",
      "         5.5682e-01,  3.7891e-01,  3.7410e-01,  2.2316e-01,  3.7720e-01,\n",
      "         3.7158e-01,  5.4475e-02,  5.9242e-01,  2.7383e-01,  1.4849e-01,\n",
      "         3.4760e-01,  1.6831e-01,  2.5314e-01,  2.5251e-01,  1.3720e-01,\n",
      "         2.8624e-01, -1.5159e-01,  3.5674e-01,  6.5184e-01,  6.0841e-02,\n",
      "         1.1150e+00,  9.9077e-02,  7.8457e-02,  1.7276e-01,  1.2581e-01,\n",
      "         3.7741e-01, -3.1595e-03,  1.1640e+00,  4.9799e-01,  9.1066e-01,\n",
      "         1.0761e-01, -3.1935e-01, -2.3097e-01,  9.3796e-02, -5.0069e-01,\n",
      "         5.8900e-02,  2.1235e-01, -1.1297e-01,  2.7144e-01,  5.5731e-02,\n",
      "        -2.5179e-01,  1.0727e-01, -2.2127e-01, -4.7619e-02, -2.3867e-01,\n",
      "         5.3880e-02,  1.5351e-01, -1.7210e-01,  8.3697e-02,  2.6442e-01,\n",
      "        -1.1202e-01, -2.5638e-01, -1.8241e-01,  6.6264e-02,  1.9662e-01,\n",
      "        -2.0192e-01, -1.9456e-01,  3.4989e-01,  1.6188e-01,  3.4762e-01,\n",
      "        -1.2415e-01,  8.5593e-02,  3.2532e-01,  1.4041e-01, -3.4219e-01,\n",
      "        -3.1173e-01,  2.4478e-01, -5.2672e-02, -4.3101e-02, -2.5349e-01,\n",
      "         1.5632e-01, -2.0690e-01, -7.6148e-02, -1.0533e-01,  5.3788e-01,\n",
      "         3.4231e-01, -1.1688e-01, -2.2711e-01,  2.1523e-01,  2.9170e-02,\n",
      "         2.8720e-01, -2.1543e-01, -1.3941e-01,  7.2365e-02, -3.6233e-01,\n",
      "         1.3925e-01, -2.1729e-01,  6.0115e-01, -2.8546e-01,  5.8128e-02,\n",
      "         1.9304e-01, -3.6487e-01,  9.9458e-02,  8.5509e-02,  3.0937e-01,\n",
      "         2.4675e-01,  1.6450e-01, -3.8411e-01,  1.9772e-03,  1.6184e-01,\n",
      "         2.4307e-01,  1.1533e-01,  1.4797e-01, -7.6005e-03,  2.8265e-01,\n",
      "        -1.0239e-01,  2.8972e-02,  3.3420e-01, -7.7711e-02, -9.4586e-02,\n",
      "         2.5239e-02,  4.5510e-02, -2.7532e-01,  4.5255e-02, -1.9840e-01,\n",
      "        -3.8937e-01, -4.8401e-01,  6.5089e-02, -2.7787e-01, -1.2213e-01,\n",
      "        -2.1746e-01,  1.0607e-01, -1.6826e-01, -3.1382e-01,  7.8317e-02,\n",
      "         8.8847e-03,  1.3171e-01,  2.7884e-02, -1.6255e-01,  1.0808e-01,\n",
      "         3.5772e-02, -4.3532e-01,  4.3896e-02,  2.9914e-01, -3.9222e-01,\n",
      "        -3.1151e-01, -2.7876e-01, -6.4276e-02, -6.7167e-02, -6.8427e-02,\n",
      "        -2.6114e-01,  1.6738e-01,  2.8729e-01, -4.9447e-02,  2.5293e-01,\n",
      "        -4.8502e-02, -3.4300e-01, -2.3957e-01,  2.0107e-01, -2.6434e-01,\n",
      "         1.0431e-01,  3.3671e-01, -1.1693e-01,  3.2150e-02,  1.1665e-01,\n",
      "        -4.0278e-01, -1.6846e-01, -5.7485e-02,  2.6335e-01,  6.2657e-01,\n",
      "        -7.3943e-02, -6.2011e-02, -4.7160e-01, -5.5239e-01, -2.3938e-01,\n",
      "         4.3574e-01, -1.0207e-01, -7.4554e-02, -9.5742e-02, -2.2412e-01,\n",
      "        -2.6694e-01,  3.4543e-02,  2.3037e-01,  1.6177e-01,  2.7801e-02,\n",
      "        -1.1432e-01,  8.1286e-02,  2.3506e-01,  1.8234e-01,  3.4246e-02,\n",
      "        -4.1260e-01, -3.3291e-01,  2.3983e-01,  3.6141e-01, -2.2778e-01,\n",
      "         7.1695e-02, -1.6491e-01,  1.9339e-01,  5.6184e-01,  1.3056e-01,\n",
      "        -3.8075e-01,  1.0460e-01,  3.6763e-02,  3.5812e-02,  5.5073e-01,\n",
      "        -1.5206e-01,  1.6230e-01,  2.0815e-01, -5.2062e-01,  4.3938e-02,\n",
      "         2.6011e-02, -3.7078e-01,  2.2953e-01, -5.6104e-02, -6.5393e-02,\n",
      "        -1.3299e-01, -8.3414e-02, -5.4439e-02,  2.6797e-02, -1.1245e-01,\n",
      "         4.0089e-01,  1.2178e-01,  8.6342e-02,  1.1320e-01,  3.2189e-01,\n",
      "         2.3757e-01, -1.7612e-01, -2.6550e-01,  2.1542e-02, -3.5691e-01,\n",
      "         1.8935e-01,  2.2312e-01, -6.4579e-02, -2.4918e-01,  1.7657e-01,\n",
      "        -2.1836e-01,  1.1917e-01,  9.3420e-02, -1.7501e-01, -5.5361e-01,\n",
      "         1.6582e-01, -3.7015e-01,  1.3043e-01, -3.2957e-01, -2.6450e-01,\n",
      "        -1.5950e-02,  2.5720e-01, -6.6303e-01,  1.2933e-01, -3.2406e-01,\n",
      "        -2.6244e-03,  1.9816e-01, -1.2109e-01, -3.5687e-01, -4.9239e-02,\n",
      "        -2.9765e-01,  1.1309e-01, -1.2996e-01,  2.4404e-02,  3.5371e-02,\n",
      "        -1.7336e-01,  5.3612e-02, -1.6611e-01, -1.2037e-01, -6.0663e-01,\n",
      "         2.6769e-01, -9.0691e-02, -1.3998e-01, -8.6208e-02, -3.0759e-01,\n",
      "        -1.9436e-01,  5.1520e-02,  4.1524e-02,  6.0612e-03,  3.0416e-01,\n",
      "         1.7818e-01, -2.6828e-01, -1.1528e-01, -1.8531e-01, -2.0600e-01,\n",
      "         2.6572e-01, -1.2197e-01,  2.1764e-02,  6.4748e-02,  3.4599e-01,\n",
      "        -3.0698e-01, -2.3229e-01, -2.9947e-01, -3.0253e-01, -2.8881e-01,\n",
      "        -5.1288e-01,  1.9001e-01, -1.1235e-02,  1.2945e-01,  8.9360e-02,\n",
      "        -1.6315e-01, -4.0220e-01, -6.4573e-01, -1.2530e-01,  1.5575e-01,\n",
      "        -4.3379e-01, -8.0903e-02, -4.5885e-01,  2.0452e-01, -2.6418e-02,\n",
      "        -2.1066e-01,  4.4004e-01,  9.1820e-02, -3.1358e-01, -9.2776e-03,\n",
      "        -1.3388e-01,  1.1176e-01,  1.0653e-01,  4.8795e-02,  3.4520e-01,\n",
      "         7.5204e-02, -3.4219e-01,  5.8729e-02, -5.3656e-02,  2.5019e-01,\n",
      "         1.0636e-02, -1.2827e-01,  1.3194e-01,  9.3822e-02, -2.2176e-01,\n",
      "        -1.6476e-02, -6.3745e-01, -2.2146e-01,  1.2660e-01, -2.8935e-01,\n",
      "         2.0136e-01, -1.9197e-02, -3.1718e-01,  6.8501e-02,  3.6979e-02,\n",
      "        -1.3442e-01, -2.1080e-01, -2.1374e-01,  3.4055e-01, -3.0370e-02],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for p in encoder1.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e338d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4535, -0.8212,  0.7829,  ...,  0.0507,  0.4352, -1.9595],\n",
      "        [ 0.3427,  2.7124, -0.2625,  ..., -0.1007,  1.1083, -0.0552],\n",
      "        [ 0.1731,  0.6942,  0.2450,  ...,  0.5388, -1.0690,  0.1895],\n",
      "        ...,\n",
      "        [-0.5880,  0.1559, -0.1740,  ..., -0.7984, -0.2060, -0.7826],\n",
      "        [-0.7874, -0.8534,  0.9255,  ...,  0.9693,  0.4033, -1.3736],\n",
      "        [-0.6242,  0.2627,  0.8259,  ...,  0.2791, -0.2974, -0.4235]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0266,  0.0106,  0.0733,  ...,  0.0305,  0.0089, -0.0266],\n",
      "        [ 0.0096,  0.0895,  0.0007,  ..., -0.0385, -0.0006, -0.0526],\n",
      "        [-0.0256,  0.0329,  0.0722,  ...,  0.1478,  0.0752,  0.0207],\n",
      "        ...,\n",
      "        [ 0.1107,  0.0652,  0.1063,  ...,  0.1272,  0.0135, -0.0240],\n",
      "        [-0.0261,  0.1426, -0.0542,  ...,  0.0939, -0.0175, -0.0344],\n",
      "        [ 0.1269,  0.0455,  0.0257,  ...,  0.1265,  0.0187, -0.0598]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.1040,  0.0864,  0.4179,  0.1586,  0.0366,  0.0591,  0.1475,  0.0431,\n",
      "         0.0790,  0.1679,  0.0041,  0.0751,  0.0433,  0.1036,  0.0269, -0.0286,\n",
      "         0.0718,  0.0017,  0.1408,  0.1034,  0.1008, -0.0086,  0.0480,  0.1979,\n",
      "         0.1253,  0.1305,  0.1150,  0.1008,  0.0530,  0.0672, -0.0055,  0.0176,\n",
      "         0.0417, -0.0028,  0.0746,  0.0366,  0.0532,  0.0802, -0.0172,  0.0520,\n",
      "         0.0363,  0.0072,  0.0241, -0.0125,  0.0061, -0.0038,  0.0353, -0.0104,\n",
      "         0.0343,  0.0136,  0.0022,  0.0093,  0.0428, -0.0045, -0.0022, -0.0279,\n",
      "        -0.0216,  0.0073,  0.0184, -0.0222,  0.0287, -0.0356, -0.0101, -0.0424,\n",
      "         0.0090, -0.0042,  0.0426, -0.0136, -0.0266, -0.0210, -0.0151,  0.0129,\n",
      "         0.0036, -0.0233, -0.0470, -0.0404,  0.0135, -0.0346, -0.0429, -0.0069,\n",
      "        -0.0397, -0.0303, -0.0417, -0.0243, -0.0231, -0.0203, -0.0815, -0.0466,\n",
      "        -0.0744, -0.0150, -0.0035, -0.0237, -0.0542, -0.0303, -0.0487, -0.0158,\n",
      "        -0.0269, -0.0359, -0.0652, -0.0283, -0.0703, -0.0368, -0.0528, -0.0622,\n",
      "        -0.0549, -0.0316, -0.0291, -0.0544, -0.0384, -0.0720, -0.0207, -0.0745,\n",
      "        -0.0353, -0.0721, -0.0895, -0.0212, -0.0673, -0.0150, -0.0218, -0.0840,\n",
      "        -0.0485, -0.0643, -0.0508, -0.0125, -0.0241, -0.0697, -0.0112, -0.0464,\n",
      "        -0.0696, -0.0272, -0.0381, -0.0018, -0.0231, -0.0257, -0.0426, -0.0098,\n",
      "        -0.0462, -0.0132, -0.0560,  0.0108, -0.0185, -0.0519,  0.0115, -0.0202,\n",
      "        -0.0162,  0.0563,  0.0080, -0.1294, -0.1624, -0.0102], device='cuda:0')\n",
      "tensor([[ 0.2081,  0.0479,  0.2028,  ..., -0.0081, -0.0532, -0.0509],\n",
      "        [ 0.1870,  0.1876,  0.1693,  ...,  0.0835, -0.1163,  0.0245],\n",
      "        [ 0.2232, -0.1946,  0.1258,  ...,  0.1803, -0.0587, -0.0926],\n",
      "        ...,\n",
      "        [ 0.2933, -0.2216,  0.0351,  ...,  0.0419, -0.0748, -0.0906],\n",
      "        [ 0.1008, -0.2913, -0.1141,  ...,  0.0869, -0.2542, -0.0780],\n",
      "        [ 0.0466, -0.0013, -0.0028,  ...,  0.2416, -0.1562,  0.0818]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5406, -0.4217, -0.7554, -0.5937, -0.5219, -0.4678, -0.6595, -0.5432,\n",
      "        -0.5807, -0.5280, -0.5422, -0.9486, -0.8303, -0.5703, -0.7345, -0.6439,\n",
      "        -0.4204, -0.5300, -0.8157, -0.6678, -0.5142, -0.6645, -0.6155, -0.5570,\n",
      "        -0.8047, -0.6969, -0.5513, -0.6550, -0.6674, -0.7063, -0.8310, -0.8538,\n",
      "        -0.3821, -0.5983, -0.3711, -0.7829, -0.5650, -0.5687, -0.5850, -0.6875,\n",
      "        -0.5032, -0.7750, -0.6537, -0.5906, -0.5181, -0.6405, -0.4965, -0.5695,\n",
      "        -0.8139, -0.5922, -0.5379, -0.5229, -0.5613, -0.9801, -0.7132, -0.5677,\n",
      "        -0.7715, -0.6057, -0.6577, -0.8100, -0.5694, -0.4580, -0.6973, -0.5116,\n",
      "        -0.6535, -0.8379, -0.4831, -0.7668, -0.6646, -0.5807, -0.6465, -0.8381,\n",
      "        -0.7210, -0.4460, -0.6752, -0.6116, -0.5341, -0.5441, -0.5887, -0.5294,\n",
      "        -0.7056, -0.7887, -0.5980, -0.8022, -0.9565, -0.6460, -0.5725, -0.3850,\n",
      "        -0.6044, -0.3921, -0.6816, -0.5539, -0.5556, -0.7051, -0.8455, -0.7270,\n",
      "        -1.1068, -0.9593, -0.5652, -0.7859, -0.5336, -0.6401, -0.5094, -0.6552,\n",
      "        -0.6621, -0.9619, -0.4979, -0.8339, -0.6902, -0.6037, -0.6048, -0.5683,\n",
      "        -0.6084, -0.5138, -1.0257, -0.7018, -0.9051, -0.4839, -0.6004, -0.5879,\n",
      "        -0.5937, -0.6979, -0.5627, -0.6525, -0.6552, -0.9294, -0.7162, -0.5570,\n",
      "        -0.7282, -0.5662, -0.5753, -0.7553, -0.6761, -0.5622, -0.5996, -0.7715,\n",
      "        -0.5098, -0.8768, -0.7863, -0.4806, -0.7385, -0.6874, -0.6228, -0.7493,\n",
      "        -0.6655, -0.7011, -0.7974, -0.4414, -0.5805, -0.5517, -0.6645, -0.6220,\n",
      "        -0.6855, -0.5139, -0.7380, -0.5518, -0.8679, -0.4358, -0.6293, -0.5738,\n",
      "        -0.6888, -0.9379, -0.6752, -0.6675, -0.4367, -0.7229, -0.5234, -0.8425,\n",
      "        -0.6629, -0.6120, -0.4324, -0.6422, -0.8130, -0.7165, -0.5654, -0.5086,\n",
      "        -0.5671, -0.7183, -0.6599, -0.5519, -0.8600, -0.6226, -0.7529, -0.5746,\n",
      "        -0.3558, -0.5336, -0.8263, -0.5138, -0.5801, -0.4166, -0.6744, -0.6424,\n",
      "        -0.6105, -0.5947, -0.6155, -0.5972, -0.6033, -0.6969, -0.5602, -0.5253,\n",
      "        -0.6448, -0.9751, -0.7725, -0.6715, -0.9535, -0.5072, -0.6863, -0.7625,\n",
      "        -0.8189, -0.5725, -0.7130, -0.7318, -0.6424, -0.7030, -0.7405, -0.7534,\n",
      "        -0.6286, -0.8097, -0.5213, -0.6713, -0.5501, -0.5920, -0.6636, -0.6433,\n",
      "        -0.5431, -0.7893, -0.5529, -0.6386, -0.6921, -0.6288, -0.7509, -0.7529,\n",
      "        -0.4583, -0.5883, -0.6016, -0.6845, -0.6987, -0.6020, -0.5598, -0.5033,\n",
      "        -0.9914, -0.6327, -0.8713, -0.6448, -0.4963, -0.5772, -0.5592, -0.6538,\n",
      "        -0.5646, -0.6259, -0.4732, -0.4642, -0.6299, -0.6379, -0.4621, -0.5549,\n",
      "        -0.4965, -0.6419, -0.4920, -0.5829, -0.5336, -0.5843, -0.6019, -0.5048,\n",
      "        -0.6286, -0.3780, -0.7992, -0.7922, -0.5301, -0.9332, -0.5592, -0.6613,\n",
      "        -0.5902, -0.5963, -0.8136, -0.5878, -0.6452, -0.5591, -0.5985, -0.6025,\n",
      "        -0.4391, -0.9707, -0.8098, -0.7009, -0.5958, -0.6263, -0.4469, -0.7154,\n",
      "        -0.5860, -0.7589, -0.4938, -0.6688, -0.6123, -0.7896, -0.5840, -0.6528,\n",
      "        -0.7349, -0.6523, -0.7157, -0.6051], device='cuda:0')\n",
      "tensor([[-0.0757, -0.0137, -0.0799,  ..., -0.1139,  0.0735,  0.1705],\n",
      "        [-0.0694, -0.0313, -0.0146,  ...,  0.0526,  0.0132, -0.0485],\n",
      "        [-0.0188,  0.0174,  0.0100,  ...,  0.0024, -0.0579,  0.0335],\n",
      "        ...,\n",
      "        [-0.0143, -0.0360, -0.0114,  ..., -0.0267,  0.0638, -0.1181],\n",
      "        [ 0.2450, -0.2202,  0.0277,  ...,  0.1181, -0.0094,  0.1143],\n",
      "        [ 0.1266,  0.0321,  0.1600,  ...,  0.0949, -0.0558, -0.0235]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.2156, -0.0246,  0.0270,  ...,  0.0652, -0.1024,  0.2734],\n",
      "        [-0.0424, -0.0361,  0.0493,  ...,  0.0059, -0.0896, -0.0237],\n",
      "        [ 0.0462,  0.0252,  0.0516,  ...,  0.0061, -0.0666, -0.0127],\n",
      "        ...,\n",
      "        [ 0.0514,  0.0372, -0.0131,  ..., -0.0272,  0.0909, -0.0146],\n",
      "        [-0.0390,  0.0424, -0.1150,  ...,  0.0700, -0.0548, -0.0894],\n",
      "        [-0.0767,  0.0680,  0.0117,  ..., -0.0850, -0.0726, -0.0871]],\n",
      "       device='cuda:0')\n",
      "tensor([-1.1630e-01, -6.3343e-02,  1.4530e-02, -4.1277e-02, -9.6494e-02,\n",
      "        -1.1274e-01,  7.3445e-02, -1.3120e-01,  2.7603e-02, -3.1021e-02,\n",
      "        -3.9592e-02, -1.0452e-01,  2.6283e-02, -1.2329e-01, -1.2700e-01,\n",
      "        -4.5905e-02,  1.2555e-01, -1.3356e-01, -2.1302e-02, -1.1898e-01,\n",
      "        -1.4291e-01, -4.0892e-02, -5.9389e-02, -4.9111e-02, -7.7937e-02,\n",
      "        -7.2386e-02,  8.0768e-03, -4.9213e-02, -4.9489e-02, -1.7749e-02,\n",
      "        -6.4746e-02, -1.9592e-01, -1.1613e-01, -5.0651e-02, -8.2802e-02,\n",
      "        -1.4811e-02, -4.0318e-02, -1.5085e-01, -8.7815e-02,  2.1865e-02,\n",
      "        -8.3305e-02, -2.3888e-02, -5.9754e-02, -3.3129e-02, -2.2320e-01,\n",
      "        -6.1654e-02, -6.3248e-02, -5.2078e-02,  9.7564e-03, -6.7452e-02,\n",
      "        -3.5045e-02, -1.1615e-01, -2.8891e-02, -7.2102e-02, -8.1291e-02,\n",
      "        -7.7879e-02, -9.1099e-02, -3.7122e-02, -2.0430e-02, -1.1930e-01,\n",
      "        -3.5073e-02, -1.7855e-01, -7.4508e-02, -5.9946e-02, -8.4564e-02,\n",
      "        -5.9899e-02, -1.1406e-01, -1.5523e-02,  1.2019e-02, -7.3024e-02,\n",
      "        -1.0054e-01, -3.0561e-02,  3.3174e-02, -1.2972e-01, -1.0134e-01,\n",
      "        -1.4030e-03, -1.0437e-01, -9.8104e-02, -1.3661e-02, -9.2827e-02,\n",
      "        -4.7088e-02, -1.4241e-01, -7.0751e-02, -1.9619e-02, -4.0449e-02,\n",
      "        -1.0103e-01, -1.1271e-01, -1.5117e-01, -8.6312e-02, -1.3084e-01,\n",
      "        -4.2066e-02, -1.4029e-01,  3.2332e-02, -4.6742e-02,  8.4265e-02,\n",
      "        -6.4319e-02,  6.8617e-02, -3.4539e-02, -3.7827e-02, -1.0333e-01,\n",
      "        -8.7853e-02, -1.3554e-01, -1.4540e-01, -5.3964e-02, -1.1247e-01,\n",
      "        -1.7626e-02, -1.4229e-02,  1.6820e-03, -6.0247e-02, -1.2521e-01,\n",
      "        -6.3470e-02, -6.5577e-02, -7.7642e-03, -8.9199e-02, -4.0823e-02,\n",
      "        -1.4460e-02, -1.0239e-01, -8.9458e-02, -5.8013e-02, -1.3496e-01,\n",
      "        -1.3976e-01, -8.3153e-02,  3.4458e-03,  1.9379e-02,  1.2068e-06,\n",
      "         1.1342e-02,  2.4060e-03, -1.0949e-01,  9.5904e-03, -1.0670e-01,\n",
      "         1.5405e-02, -7.0687e-02, -6.2338e-02, -3.6525e-02, -2.5358e-03,\n",
      "        -4.4367e-02, -6.2158e-02, -1.2604e-01, -1.0632e-01, -1.0862e-01,\n",
      "        -1.6205e-01, -9.5490e-02, -1.9058e-02, -1.3882e-01, -1.4115e-01,\n",
      "        -3.1886e-02, -1.3917e-01, -7.0112e-02, -9.6176e-02,  3.3751e-02,\n",
      "        -9.9431e-02, -8.4774e-03, -1.1797e-02, -3.8410e-02, -7.6045e-02,\n",
      "        -1.4118e-01, -7.5883e-04, -6.6887e-02, -5.1523e-02, -9.4193e-02,\n",
      "         1.0696e-02, -5.1553e-02, -3.8974e-02, -1.2227e-01, -6.8483e-02,\n",
      "        -9.8559e-02, -4.6422e-02, -8.3472e-02, -5.4039e-02, -7.4287e-02,\n",
      "        -2.3519e-02, -1.1859e-01, -2.5347e-02, -1.0857e-01, -7.2754e-02,\n",
      "        -9.8005e-02, -6.6378e-03, -9.5538e-02, -2.8229e-03,  1.2353e-02,\n",
      "        -6.3799e-02, -9.1815e-02,  1.2045e-01, -9.8080e-02, -3.3342e-02,\n",
      "        -6.2023e-02, -6.6053e-02, -7.4995e-02, -7.3124e-02, -1.2479e-01,\n",
      "        -1.0248e-01,  1.6625e-03, -4.1889e-02, -1.5247e-01, -3.1144e-02,\n",
      "        -6.1838e-03, -4.1576e-02, -1.7972e-01, -9.4626e-02, -1.4380e-01,\n",
      "        -1.0224e-01, -6.8810e-02, -2.3611e-02, -1.1198e-01, -1.3421e-01,\n",
      "        -1.4079e-01, -8.1118e-02, -1.7798e-01, -1.2252e-02, -9.7745e-02,\n",
      "        -3.0623e-02, -1.0107e-01, -8.2036e-02, -1.3381e-01, -3.5774e-02,\n",
      "         1.7944e-01, -1.6853e-02, -7.5185e-02,  7.5183e-02, -1.3796e-01,\n",
      "        -6.0887e-02, -7.4124e-02, -9.6561e-02, -1.0734e-01,  5.1255e-02,\n",
      "        -8.7475e-02, -6.4521e-02, -1.0012e-02, -4.1406e-02, -8.6906e-02,\n",
      "        -1.5334e-01, -9.4332e-02, -9.1864e-02, -5.0282e-02, -5.8668e-02,\n",
      "        -1.3259e-01, -1.5444e-01, -1.3451e-02, -4.5596e-02, -9.0153e-02,\n",
      "        -3.5672e-02, -1.1718e-01,  1.1780e-01,  3.1857e-02, -5.6357e-04,\n",
      "         6.6787e-02, -4.8214e-02, -1.0122e-01, -1.3171e-01, -1.2463e-01,\n",
      "        -2.0555e-01, -6.1410e-02,  5.0098e-03, -8.4163e-02, -9.6917e-02,\n",
      "        -1.6177e-02, -3.7754e-02, -1.7121e-01, -1.0558e-01, -7.2979e-02,\n",
      "        -4.7169e-02, -8.8144e-02, -1.1158e-01, -1.6440e-02,  7.7780e-02,\n",
      "         3.6294e-02, -1.2485e-01, -5.4482e-02,  1.9422e-02, -1.0347e-01,\n",
      "        -6.8119e-02, -2.4039e-03, -6.2241e-02, -5.4707e-02, -4.4652e-02,\n",
      "         1.3803e-01, -1.0105e-01, -1.0202e-02, -2.3474e-02, -5.2038e-02,\n",
      "         5.1270e-03, -1.0387e-01, -9.8661e-02, -1.0579e-01, -2.4720e-02,\n",
      "        -1.2268e-01, -5.5778e-02,  1.9050e-02, -7.9162e-02, -8.4984e-02,\n",
      "        -9.7761e-02, -1.0846e-01, -6.6372e-02, -7.2773e-02, -1.2558e-01,\n",
      "        -5.5824e-02, -1.6613e-01, -9.1080e-02, -6.9082e-02, -4.3229e-02,\n",
      "        -1.4820e-01,  2.1449e-02,  3.4325e-01,  2.6421e-01,  7.9827e-02,\n",
      "         2.5437e-01,  3.0879e-01,  1.1562e-01,  4.0931e-01,  1.4717e-01,\n",
      "         2.8420e-01,  1.6496e-01,  2.8322e-01,  3.4859e-01, -2.7072e-02,\n",
      "         4.3842e-01,  2.5542e-01,  2.9625e-01,  1.8015e-01, -8.8131e-02,\n",
      "         1.9736e-01,  1.8006e-02,  4.0301e-01,  4.3803e-01,  2.2464e-01,\n",
      "         3.3551e-01,  1.7783e-01,  8.2320e-01,  3.3881e-01,  4.2539e-01,\n",
      "         2.9316e-01,  1.3514e-01,  2.3624e-01,  1.1796e-01,  3.4306e-01,\n",
      "         1.0873e-01,  7.5532e-02,  6.8941e-02,  7.1106e-02,  2.9512e-01,\n",
      "         4.3826e-01,  2.4417e-01,  3.4971e-03,  2.7848e-01, -2.9732e-01,\n",
      "        -8.4364e-02,  3.3636e-01,  2.9314e-01,  3.1156e-01,  1.8385e-01,\n",
      "         4.4096e-01,  2.0388e-01,  1.5385e-01, -1.3209e-02,  2.7914e-01,\n",
      "         3.6295e-01,  3.9577e-01,  1.4559e-02,  5.0957e-01,  3.5278e-01,\n",
      "         4.5906e-01,  3.2259e-01,  3.5092e-01,  2.0477e-01,  3.8107e-01,\n",
      "        -3.3483e-02,  2.7411e-01,  6.1780e-01,  3.1074e-01,  3.3811e-01,\n",
      "         8.9641e-02,  2.9347e-01,  1.1923e-01,  2.5775e-02, -2.3233e-02,\n",
      "         3.7129e-01,  3.3548e-01,  3.9101e-02,  5.4382e-01,  3.3704e-01,\n",
      "         1.9706e-01,  2.7308e-01,  5.1681e-01,  2.5640e-01,  2.4170e-01,\n",
      "         3.6221e-01,  4.3652e-02,  2.2562e-01,  1.9647e-01,  9.5269e-02,\n",
      "         3.6361e-01,  1.6827e-01,  4.0649e-01,  3.3009e-01,  1.1251e-01,\n",
      "         3.2041e-01,  8.9367e-02,  2.5120e-01,  1.4215e-01,  2.2545e-01,\n",
      "         3.2601e-01,  4.0344e-02,  2.8951e-01,  3.5782e-02,  2.8167e-01,\n",
      "         4.5210e-01, -4.4635e-02,  3.4255e-01,  3.3661e-01,  8.9249e-02,\n",
      "         3.8178e-01,  1.6715e-01,  1.2878e-01, -1.0151e-02,  1.9735e-01,\n",
      "         2.4361e-01, -6.3403e-02,  6.0371e-02,  2.7952e-01,  2.1079e-02,\n",
      "        -9.3986e-02,  3.2870e-01,  9.0546e-02,  1.5876e-01,  1.7539e-01,\n",
      "        -7.7967e-02,  2.0440e-01,  1.6598e-02,  4.5351e-01, -5.1579e-02,\n",
      "         4.5783e-01,  3.0180e-01,  3.1966e-02, -3.0009e-02,  2.6446e-01,\n",
      "        -1.1362e-01,  3.8314e-01,  2.8773e-01, -3.9793e-02,  3.2422e-01,\n",
      "         1.5521e-01,  1.5602e-01,  2.3604e-01,  3.1317e-01,  3.5024e-01,\n",
      "         2.5977e-01,  5.9994e-01,  2.0599e-01,  2.9870e-01,  2.4494e-01,\n",
      "         1.6965e-02,  3.3649e-01,  1.0968e-02,  2.8531e-01,  2.1254e-01,\n",
      "         3.0641e-01,  3.7224e-01,  3.6783e-01,  3.7760e-01,  3.0557e-01,\n",
      "         1.5907e-01,  3.2181e-01,  1.3983e-01, -4.8539e-02,  1.7815e-01,\n",
      "         3.3472e-01,  5.2791e-01,  3.1229e-01, -1.2031e-02,  2.0625e-01,\n",
      "         2.3603e-01,  3.2433e-01,  1.6973e-01,  2.7183e-01,  3.3767e-01,\n",
      "         2.8692e-01,  2.8832e-01,  5.0498e-02,  2.4239e-01,  1.8277e-01,\n",
      "         2.3693e-01,  6.0973e-02,  3.9559e-01,  2.5963e-01,  3.7363e-01,\n",
      "         4.2321e-01, -2.5580e-01,  3.5044e-01, -4.5737e-02,  3.1626e-01,\n",
      "         2.8124e-01,  3.8338e-01, -1.8956e-01,  1.4276e-01,  9.9260e-03,\n",
      "         2.2873e-01,  4.3333e-01,  4.7329e-01,  2.6555e-01, -1.0756e-01,\n",
      "         7.0527e-01,  4.2220e-01,  3.1555e-01, -4.6579e-02,  1.7981e-01,\n",
      "        -1.4900e-01,  5.0459e-01, -3.2178e-01,  2.3887e-01,  3.9119e-01,\n",
      "         2.3132e-01,  2.6478e-01,  2.6147e-01,  9.7915e-02,  3.3435e-01,\n",
      "         1.5400e-01,  3.3579e-01,  9.8440e-02,  1.0663e-01,  3.4874e-01,\n",
      "         2.5020e-01, -5.3222e-02,  2.5627e-01,  2.6051e-01,  4.1953e-01,\n",
      "         1.7051e-01,  2.0068e-01,  1.8672e-01,  2.8921e-01,  2.6228e-01,\n",
      "        -8.6595e-03, -7.9957e-02, -1.5747e-01, -1.6997e-01,  2.6379e-01,\n",
      "         3.2097e-01,  3.2107e-01,  4.5806e-01,  3.1027e-01,  3.8755e-01,\n",
      "         3.8166e-01, -7.7055e-02, -3.1001e-01,  2.3906e-01,  1.7492e-02,\n",
      "         1.4250e-01,  1.7127e-01,  1.9911e-01,  4.0187e-01, -5.2323e-02,\n",
      "        -1.0347e-01,  2.0028e-01,  3.3069e-01,  4.2425e-01,  2.0877e-01,\n",
      "         3.4526e-01,  9.3935e-02,  1.9759e-01,  6.3455e-02,  2.4175e-01,\n",
      "         3.6476e-01,  2.4210e-01,  5.9469e-01,  3.3423e-02,  4.3853e-01,\n",
      "        -1.9915e-01,  4.1322e-02,  1.5964e-01,  1.4967e-01, -6.1316e-02,\n",
      "         3.9566e-01,  2.3226e-01,  2.7087e-01,  1.4058e-01,  2.5891e-02,\n",
      "        -2.2590e-01,  2.6533e-02,  4.9704e-02,  2.8584e-01,  9.3685e-02,\n",
      "        -1.5396e-01,  9.1436e-02,  2.0370e-01,  3.2096e-01,  1.3492e-01,\n",
      "         1.2539e-01,  3.8978e-01,  2.8519e-01,  4.6640e-02,  2.4369e-01,\n",
      "         2.9913e-01,  1.0244e-01,  2.8744e-01,  2.3782e-01, -9.1271e-02,\n",
      "         6.1530e-02, -1.1481e-02,  3.3748e-01,  4.2068e-01,  3.3718e-01,\n",
      "         1.8580e-01,  1.2020e-01, -2.7767e-01, -2.2356e-01,  2.2675e-01,\n",
      "         3.6352e-02, -5.6475e-02, -1.6942e-01,  2.8853e-02,  1.5660e-01,\n",
      "        -1.2381e-01,  7.2692e-02, -1.6450e-01, -8.2477e-02,  2.1720e-01,\n",
      "         2.4399e-01,  2.9213e-02, -1.3391e-01,  4.1878e-01, -1.8547e-01,\n",
      "        -6.8936e-02,  2.6474e-02, -3.5424e-02, -3.1808e-01, -1.0219e-01,\n",
      "        -7.6763e-02, -2.6358e-01, -1.6422e-01,  1.8520e-01, -2.7198e-02,\n",
      "        -4.1273e-01,  1.4436e-03,  1.4938e-01, -5.9742e-02, -6.6773e-02,\n",
      "         1.4514e-02, -3.8187e-02,  3.0143e-02,  1.5924e-01,  3.2963e-01,\n",
      "        -5.5054e-02,  7.3124e-02,  8.4938e-02,  2.3767e-01,  4.6880e-02,\n",
      "         2.4863e-01, -1.8615e-01,  1.5914e-01, -1.5767e-02, -3.0331e-01,\n",
      "        -2.5265e-02,  1.6132e-01,  5.8412e-01, -4.0698e-02, -8.5529e-02,\n",
      "         1.5027e-01,  3.4229e-02, -4.6164e-02, -5.0817e-02, -1.2583e-01,\n",
      "        -1.6617e-01, -5.9773e-02, -1.7933e-02, -1.7610e-01,  2.1690e-01,\n",
      "         1.8593e-01, -1.3572e-01,  1.1890e-01, -1.1189e-01,  8.7981e-02,\n",
      "        -1.0621e-01,  1.2978e-01,  4.4623e-01,  9.7346e-02,  2.2061e-02,\n",
      "        -9.5122e-02,  6.7931e-02, -2.3341e-01,  3.4719e-02, -3.5214e-01,\n",
      "         1.8966e-01, -2.4203e-01, -4.1215e-02,  2.4539e-01,  4.7885e-02,\n",
      "        -4.9224e-02,  8.8567e-02,  6.4866e-02, -7.2710e-02, -1.4524e-01,\n",
      "        -1.9269e-01,  8.5119e-02,  1.5616e-02, -5.4423e-03,  2.0082e-01,\n",
      "        -6.6736e-02,  6.9866e-02, -2.7499e-01, -1.8418e-01, -1.0836e-01,\n",
      "         1.4523e-01,  1.6600e-02,  8.9171e-02,  1.7467e-01,  6.9146e-03,\n",
      "         2.4801e-02, -1.7825e-01,  2.1887e-01,  2.5817e-01,  1.0195e-01,\n",
      "         2.4596e-01, -7.6872e-02, -6.0508e-02, -3.3488e-01,  2.0994e-01,\n",
      "         2.0037e-01, -1.4405e-01, -1.4380e-01, -1.0247e-01, -2.4591e-01,\n",
      "         2.0449e-01,  2.5630e-03, -3.8976e-01,  3.4804e-01,  2.4434e-01,\n",
      "        -1.9007e-01,  3.4725e-01, -2.0780e-01, -2.0616e-01, -1.4670e-01,\n",
      "        -2.7191e-01, -4.3187e-02, -1.1338e-01,  1.4602e-01, -2.7393e-01,\n",
      "         1.6013e-01, -1.0501e-01, -1.4249e-01, -5.0004e-01, -7.2748e-02,\n",
      "         2.0532e-01, -1.1867e-01,  3.0475e-01,  2.5999e-01,  6.8836e-02,\n",
      "        -2.7224e-01, -8.3181e-02, -1.9014e-02, -2.1199e-01,  3.7943e-01,\n",
      "        -5.7622e-01, -2.5745e-01, -1.9775e-02, -5.4483e-02, -9.0927e-02,\n",
      "         1.7437e-01,  2.9996e-01, -1.2014e-01,  2.4472e-01,  1.6589e-01,\n",
      "         3.1981e-01,  1.1227e-01,  2.0708e-01,  3.9506e-02, -3.3566e-02,\n",
      "        -1.7690e-02,  9.5388e-02,  8.8876e-02,  7.7866e-02,  1.9281e-01,\n",
      "         2.4371e-01,  3.2144e-01, -4.0533e-01,  9.0584e-02, -1.4114e-02,\n",
      "         6.2878e-02,  3.3507e-01,  7.0979e-02, -7.6256e-02, -3.2844e-01,\n",
      "        -4.0609e-01,  1.3304e-01,  2.8792e-01, -2.6470e-01,  1.9769e-01,\n",
      "        -1.9462e-01, -2.9399e-01, -2.2394e-01, -1.9709e-01,  2.7501e-02,\n",
      "        -1.8764e-01, -1.1651e-01, -4.1427e-01, -2.5300e-01,  5.5837e-02,\n",
      "        -3.1373e-01,  2.4903e-01,  2.5331e-01, -1.9984e-01,  9.5581e-03,\n",
      "         1.7883e-01,  6.4487e-02,  2.5213e-01,  1.3443e-02,  2.5269e-01,\n",
      "        -1.8665e-01,  1.1316e-01,  3.8251e-02, -3.2906e-01,  2.1381e-02,\n",
      "        -5.3511e-01,  2.7554e-01,  2.6002e-01,  3.9344e-01,  1.9799e-01,\n",
      "        -2.5399e-01,  1.9762e-02,  2.0537e-01, -2.9367e-01, -1.8149e-02,\n",
      "        -2.7067e-01, -4.1172e-01,  1.2385e-01, -3.2943e-01, -3.8904e-01,\n",
      "         1.0577e-01,  2.3626e-01,  2.4399e-01, -7.4599e-03, -3.1036e-01,\n",
      "        -2.5927e-01, -1.5744e-01, -3.2593e-01,  4.1696e-01, -8.4722e-02,\n",
      "        -5.0939e-02,  1.1672e-01, -1.5496e-01,  2.4732e-01,  1.2070e-01,\n",
      "         2.3403e-01,  2.2323e-01,  6.6099e-01,  4.8560e-01,  5.4455e-02,\n",
      "         2.5332e-01,  3.7258e-01,  1.1978e-01, -1.5360e-01, -4.1791e-01,\n",
      "        -2.4430e-01, -3.3765e-01,  1.2981e-01,  8.5530e-02, -2.0380e-01,\n",
      "        -9.4382e-02,  9.6686e-02, -1.0210e-01, -1.3274e-01,  3.8346e-01,\n",
      "        -3.9995e-02, -6.3762e-02,  3.7205e-03, -2.2575e-01,  1.6893e-01,\n",
      "        -2.7789e-02,  1.3236e-01,  8.1690e-02, -3.3793e-01,  1.5251e-01,\n",
      "        -2.8744e-02,  2.8198e-01,  6.4816e-02,  1.7994e-01, -1.1363e-01,\n",
      "        -3.5954e-01, -8.5521e-02, -4.3444e-01,  9.4003e-02, -1.6061e-01,\n",
      "        -4.8992e-01,  1.7208e-01, -1.9784e-01, -2.0109e-01,  3.7182e-01,\n",
      "         7.2392e-02, -1.1655e-02, -2.8110e-01,  6.0573e-02, -1.3502e-01,\n",
      "        -2.0711e-02,  3.3604e-01,  8.5544e-02, -3.9416e-01,  1.1007e-01,\n",
      "        -2.2622e-01,  3.9333e-01, -2.4310e-01, -1.1724e-02, -4.6623e-02],\n",
      "       device='cuda:0')\n",
      "tensor([-2.0003e-01, -5.5375e-02, -1.1578e-02, -2.8234e-02, -4.5866e-03,\n",
      "        -3.4780e-02,  7.6664e-02, -5.7906e-02, -5.1736e-02, -1.1008e-01,\n",
      "        -2.0527e-02, -1.2408e-01, -7.1418e-02, -1.1961e-01, -1.1179e-01,\n",
      "        -4.4916e-02,  1.8426e-01, -3.0387e-02, -4.8425e-02, -2.2369e-02,\n",
      "        -6.9720e-02,  4.3039e-03, -3.1993e-02, -1.0027e-02, -6.1076e-02,\n",
      "        -1.8363e-02,  7.9065e-02, -6.7462e-03, -1.4813e-02,  7.3722e-02,\n",
      "        -5.4689e-02, -1.7310e-01, -1.1766e-01, -1.2281e-01, -3.4303e-02,\n",
      "         2.4616e-03, -6.7099e-02, -9.4043e-02, -6.1887e-02, -1.4011e-02,\n",
      "        -1.5398e-01, -8.8724e-02, -5.9623e-02, -6.1098e-02, -2.3713e-01,\n",
      "        -9.5767e-02, -2.6927e-02, -7.7453e-02,  6.6763e-02, -1.6644e-01,\n",
      "        -1.4410e-02, -1.3830e-01, -1.6726e-02, -1.0250e-01, -7.2296e-02,\n",
      "        -1.6725e-02, -2.1863e-02, -1.1175e-01, -7.7705e-02, -4.1685e-02,\n",
      "        -8.3168e-02, -1.2676e-01, -1.5092e-01, -1.3583e-02, -6.9370e-02,\n",
      "         6.7793e-04, -3.5214e-02,  5.9027e-03, -4.4541e-02, -2.7010e-02,\n",
      "        -1.0444e-01, -6.7668e-02,  1.5930e-02, -9.0103e-02, -9.6081e-02,\n",
      "        -2.8423e-02, -6.1366e-02, -6.3441e-02, -2.9701e-02, -4.5956e-02,\n",
      "        -2.3311e-02, -9.3482e-02, -8.2508e-02, -3.3581e-02, -6.0437e-02,\n",
      "        -1.1313e-01, -4.5422e-02, -1.2835e-01, -1.2081e-01, -1.7024e-01,\n",
      "        -3.4610e-02, -1.3855e-01,  1.3098e-02, -6.6666e-02,  4.5786e-02,\n",
      "        -3.3846e-03,  1.2465e-01, -5.5646e-03, -7.7872e-02, -5.5117e-02,\n",
      "        -7.4398e-02, -9.0684e-02, -1.0348e-01, -1.3851e-02, -8.8421e-02,\n",
      "        -1.8004e-03, -1.9198e-03, -6.8284e-02, -4.9260e-02, -8.1844e-02,\n",
      "        -5.7168e-02, -6.5599e-02, -7.8024e-02, -9.7612e-02,  3.8621e-03,\n",
      "         9.4834e-03, -8.0295e-02, -1.6428e-01, -6.8341e-02, -1.4300e-01,\n",
      "        -1.0623e-01, -6.0638e-02,  4.7087e-02, -7.9799e-04, -6.0861e-02,\n",
      "        -3.7033e-02, -2.8218e-02, -1.5619e-01,  2.5888e-02, -9.2585e-02,\n",
      "        -4.0796e-02, -1.0463e-01, -6.4916e-02, -1.0276e-01,  5.3977e-02,\n",
      "        -1.0000e-01, -6.9669e-02, -1.1468e-01, -1.0156e-01, -1.2369e-01,\n",
      "        -7.3405e-02, -6.5090e-02, -2.5717e-03, -2.7231e-02, -2.7791e-02,\n",
      "         9.5960e-03, -6.6029e-02, -7.8298e-02, -1.1844e-01,  4.2355e-02,\n",
      "        -1.8108e-01, -3.2562e-02, -4.3584e-02, -1.0612e-01,  1.3259e-03,\n",
      "        -1.2499e-01, -4.2715e-03, -2.0128e-02, -5.7061e-02, -1.0722e-01,\n",
      "         3.5106e-02, -8.5286e-02, -5.2243e-02, -2.6768e-02, -1.6072e-01,\n",
      "        -1.0450e-01,  6.9236e-03, -1.1389e-01, -5.7450e-02, -1.0623e-01,\n",
      "        -9.1938e-02, -1.1512e-01,  2.7777e-02, -4.6040e-02, -4.0354e-02,\n",
      "        -9.5991e-02,  4.7300e-03, -1.0497e-01, -3.1775e-02,  8.1833e-03,\n",
      "        -4.8770e-02, -7.5748e-02,  3.3234e-02, -1.7201e-02, -9.7272e-02,\n",
      "        -1.4092e-01, -6.9591e-02, -1.0554e-01, -8.8874e-02, -8.9844e-02,\n",
      "         1.1303e-03,  4.6152e-02,  1.0983e-02, -1.3812e-01, -1.4311e-02,\n",
      "        -5.6374e-02, -2.3473e-02, -1.7375e-01, -1.1772e-01, -1.7559e-01,\n",
      "        -8.8451e-02, -6.3423e-02, -9.0151e-02, -2.9624e-02, -1.0230e-01,\n",
      "        -9.0692e-02, -3.8854e-02, -1.3278e-01, -3.4178e-03, -6.4698e-03,\n",
      "        -1.2727e-02, -4.4808e-02, -4.8944e-02, -1.2192e-01, -7.2329e-02,\n",
      "         1.2660e-01, -8.0828e-02, -7.0893e-02, -1.3310e-02, -1.1657e-01,\n",
      "        -1.6694e-02, -4.4126e-02, -9.4865e-02, -1.5419e-02,  1.0372e-02,\n",
      "        -7.7047e-02, -7.0176e-02, -7.6945e-02, -6.7223e-02, -4.6747e-02,\n",
      "        -1.1104e-01, -5.2158e-02, -3.5226e-02, -2.5402e-02, -1.0851e-01,\n",
      "        -2.5054e-02, -1.5661e-01, -4.0598e-02, -2.0681e-02, -4.3607e-02,\n",
      "        -2.7923e-02, -1.0446e-01,  8.0602e-02, -1.0267e-02, -1.7839e-02,\n",
      "         8.4536e-02, -8.6994e-02, -9.0866e-02, -1.1149e-01, -1.4958e-01,\n",
      "        -2.0236e-01, -2.9868e-02,  1.7224e-02, -7.4347e-02, -1.0292e-01,\n",
      "        -6.3111e-02, -6.4115e-02, -1.1581e-01, -2.0855e-01,  2.3362e-02,\n",
      "        -3.0679e-02, -9.0527e-02, -2.2371e-02, -5.6335e-02,  1.2768e-01,\n",
      "        -4.3387e-02, -1.2367e-01, -1.3414e-01, -3.8608e-03, -1.0054e-01,\n",
      "        -8.1409e-02, -2.1425e-02, -7.1106e-02, -1.2416e-01, -3.0462e-02,\n",
      "         8.6911e-02, -7.8431e-02, -9.4569e-02, -9.9804e-02, -5.2498e-02,\n",
      "         2.2337e-02, -8.8058e-02, -9.5259e-02, -2.3038e-02,  4.0353e-02,\n",
      "        -1.4702e-01,  2.9145e-04, -7.8404e-02,  8.0845e-03, -9.4252e-02,\n",
      "        -3.0517e-02, -1.1659e-01, -5.5012e-02,  1.9360e-02, -3.4437e-02,\n",
      "        -8.6107e-03, -2.0615e-01, -2.0797e-02, -1.4123e-01, -7.0887e-02,\n",
      "        -1.3412e-01,  3.0306e-02,  3.5288e-01,  2.6213e-01,  1.1273e-01,\n",
      "         2.6706e-01,  3.4177e-01,  9.5996e-02,  4.3642e-01,  2.3095e-01,\n",
      "         2.9880e-01,  7.6110e-02,  3.0407e-01,  3.2769e-01, -1.3115e-02,\n",
      "         4.2576e-01,  2.4315e-01,  3.2085e-01,  1.9183e-01, -5.3813e-02,\n",
      "         1.6971e-01,  3.4893e-02,  3.9134e-01,  3.5857e-01,  2.4532e-01,\n",
      "         3.3679e-01,  2.0899e-01,  8.2553e-01,  3.1444e-01,  3.4520e-01,\n",
      "         2.4794e-01,  1.8045e-01,  2.6037e-01,  1.2237e-01,  3.1708e-01,\n",
      "         7.3596e-02,  6.4038e-03,  1.4704e-01,  1.0692e-01,  2.8010e-01,\n",
      "         4.8040e-01,  2.6817e-01,  8.5405e-02,  2.3687e-01, -3.0692e-01,\n",
      "        -2.3733e-02,  2.6871e-01,  3.4712e-01,  2.5268e-01,  2.3986e-01,\n",
      "         4.5110e-01,  2.3342e-01,  1.2133e-01, -5.9396e-02,  2.8167e-01,\n",
      "         4.3473e-01,  4.0051e-01, -3.1329e-02,  4.3248e-01,  3.3344e-01,\n",
      "         3.4905e-01,  3.9278e-01,  3.5609e-01,  2.7481e-01,  3.7385e-01,\n",
      "        -8.7120e-02,  2.4336e-01,  6.0076e-01,  3.6271e-01,  3.2563e-01,\n",
      "         1.3241e-01,  3.1368e-01,  7.1063e-02,  8.1262e-02, -2.4472e-02,\n",
      "         3.9933e-01,  2.5381e-01, -1.4425e-02,  5.7187e-01,  3.1997e-01,\n",
      "         2.0777e-01,  2.8041e-01,  4.7566e-01,  2.6576e-01,  2.7924e-01,\n",
      "         3.6709e-01,  9.3732e-03,  3.0516e-01,  2.9073e-01,  9.4595e-02,\n",
      "         4.1928e-01,  1.0280e-01,  3.4689e-01,  3.7516e-01,  1.6127e-02,\n",
      "         3.6437e-01,  5.9935e-02,  2.1550e-01,  1.9066e-01,  3.0871e-01,\n",
      "         3.0050e-01,  7.4497e-02,  2.2476e-01, -2.0923e-02,  3.1090e-01,\n",
      "         4.5557e-01, -4.8795e-02,  3.3233e-01,  2.3683e-01,  1.2835e-01,\n",
      "         3.8281e-01,  1.5562e-01,  1.5794e-01,  2.1629e-02,  2.0359e-01,\n",
      "         2.8919e-01, -5.4128e-02,  1.1037e-01,  3.0405e-01,  7.5902e-02,\n",
      "        -7.8936e-02,  3.1097e-01,  1.4498e-01,  2.1825e-01,  1.6069e-01,\n",
      "        -7.7472e-02,  2.2342e-01, -4.5775e-02,  4.6645e-01, -2.7753e-02,\n",
      "         4.0911e-01,  2.8716e-01, -2.9223e-04, -7.9161e-02,  2.7045e-01,\n",
      "        -1.2202e-01,  3.7690e-01,  2.6555e-01, -8.3759e-02,  2.8984e-01,\n",
      "         1.2421e-01,  6.4455e-02,  2.6901e-01,  2.7220e-01,  3.0829e-01,\n",
      "         3.4196e-01,  5.8248e-01,  2.6937e-01,  3.5690e-01,  2.6584e-01,\n",
      "        -4.2416e-02,  3.7576e-01,  3.1679e-02,  2.9341e-01,  2.5887e-01,\n",
      "         2.9959e-01,  3.5640e-01,  3.6983e-01,  2.7809e-01,  3.0959e-01,\n",
      "         1.1623e-01,  2.8398e-01,  1.3689e-01, -3.9153e-02,  1.9372e-01,\n",
      "         3.0935e-01,  4.3746e-01,  3.8657e-01, -3.8238e-02,  1.3819e-01,\n",
      "         2.5217e-01,  2.4368e-01,  9.9965e-02,  1.9115e-01,  3.0494e-01,\n",
      "         2.8982e-01,  2.6531e-01,  4.6404e-02,  2.2613e-01,  2.0650e-01,\n",
      "         2.7195e-01,  1.1316e-01,  3.2628e-01,  2.6016e-01,  3.2107e-01,\n",
      "         4.9833e-01, -2.6635e-01,  3.0802e-01,  1.2287e-02,  3.2867e-01,\n",
      "         2.4085e-01,  4.3249e-01, -2.7352e-01,  1.1062e-01,  4.4033e-02,\n",
      "         1.9379e-01,  3.7849e-01,  4.9769e-01,  3.3189e-01, -1.6071e-01,\n",
      "         7.6951e-01,  4.4600e-01,  2.3252e-01, -3.5738e-02,  2.3177e-01,\n",
      "        -1.4754e-01,  4.5831e-01, -3.2954e-01,  2.5957e-01,  4.1172e-01,\n",
      "         2.9509e-01,  3.2318e-01,  1.8307e-01,  6.9662e-02,  2.9986e-01,\n",
      "         1.6779e-01,  3.9868e-01,  1.0157e-01,  5.7210e-02,  3.7988e-01,\n",
      "         2.3105e-01, -6.7379e-02,  2.8831e-01,  3.6393e-01,  3.9820e-01,\n",
      "         1.6015e-01,  2.7691e-01,  1.5827e-01,  2.3640e-01,  3.0113e-01,\n",
      "         9.5293e-04, -6.8035e-02, -1.8632e-01, -1.3945e-01,  3.0457e-01,\n",
      "         4.1079e-01,  2.5004e-01,  4.3469e-01,  3.2477e-01,  3.3136e-01,\n",
      "         3.8450e-01, -7.2442e-02, -2.2438e-01,  1.3986e-01, -1.0360e-02,\n",
      "         1.3163e-01,  2.1318e-01,  1.7701e-01,  3.2666e-01, -8.6056e-02,\n",
      "        -8.2843e-02,  2.0524e-01,  3.9259e-01,  3.5134e-01,  2.0086e-01,\n",
      "         3.0829e-01,  1.0663e-01,  2.5215e-01,  1.1989e-01,  1.5973e-01,\n",
      "         2.9589e-01,  2.0308e-01,  5.5268e-01,  6.2839e-02,  4.6188e-01,\n",
      "        -1.9078e-01, -1.5908e-02,  1.1146e-01,  1.4625e-01, -1.4381e-02,\n",
      "         4.0674e-01,  2.1753e-01,  1.7631e-01,  1.6746e-01,  1.0760e-02,\n",
      "        -2.1319e-01,  2.0563e-02,  6.2524e-02,  2.8260e-01,  1.3792e-01,\n",
      "        -1.0935e-01,  7.7527e-02,  2.0046e-01,  2.7059e-01,  1.6221e-01,\n",
      "         7.8744e-02,  3.7049e-01,  2.2701e-01,  9.0544e-02,  2.5200e-01,\n",
      "         2.9186e-01,  4.7796e-02,  2.1256e-01,  2.0279e-01, -1.1195e-01,\n",
      "         8.2030e-02, -2.0837e-02,  2.6917e-01,  3.9663e-01,  3.3666e-01,\n",
      "         1.3155e-01,  1.1899e-01, -2.3856e-01, -3.0802e-01,  3.0944e-01,\n",
      "         1.0219e-01,  6.2489e-02, -5.6633e-02,  3.2542e-02,  1.5035e-01,\n",
      "        -2.3078e-02, -1.7614e-01, -1.7251e-01, -2.8682e-02,  2.1381e-01,\n",
      "         1.8555e-01,  1.2577e-01, -3.0777e-02,  2.1248e-01, -1.7835e-01,\n",
      "        -3.9316e-02, -2.2604e-01,  1.5236e-02, -1.4712e-01,  4.1122e-02,\n",
      "        -1.4867e-01, -1.6698e-01,  3.2329e-01,  1.5811e-01,  2.8804e-01,\n",
      "         2.4965e-02, -3.1602e-03,  1.2496e-01, -6.3328e-02, -1.2944e-01,\n",
      "         9.4257e-02,  2.6452e-01,  1.6036e-01, -1.6816e-01,  1.6058e-01,\n",
      "         1.0155e-01, -1.7612e-01,  1.3339e-01,  7.3917e-02,  4.1767e-01,\n",
      "         1.1479e-01, -3.6583e-02,  1.2545e-01,  1.3439e-02, -1.5192e-01,\n",
      "         3.5108e-01,  1.9533e-01,  1.1624e-01,  3.8446e-02, -2.0418e-01,\n",
      "         2.3864e-01, -1.1914e-01, -1.5968e-01, -2.8624e-01, -1.4756e-03,\n",
      "        -7.2122e-02, -4.8502e-02,  1.1190e-01, -4.9312e-03,  9.2838e-02,\n",
      "         2.6308e-01, -1.1831e-01, -2.1002e-01, -6.2683e-02,  1.4482e-02,\n",
      "         6.0101e-02,  9.8145e-02,  1.8291e-01,  1.4898e-01, -1.9904e-01,\n",
      "        -7.5426e-02,  9.2097e-03,  2.3135e-02, -1.0423e-01, -3.6050e-02,\n",
      "         9.5654e-02, -8.7972e-02, -7.3262e-02,  8.6306e-02, -6.7260e-02,\n",
      "        -8.2303e-02,  1.8785e-01,  1.2080e-01, -2.4854e-01, -4.3008e-02,\n",
      "        -1.3390e-01,  8.2946e-02, -2.9950e-02, -4.4089e-02,  3.9845e-01,\n",
      "        -9.8761e-02,  3.5421e-01, -2.5693e-01, -1.2153e-01, -1.2116e-01,\n",
      "         1.1156e-01, -1.0210e-01,  8.1816e-02,  2.0096e-01, -5.5948e-02,\n",
      "         1.4510e-02, -1.6189e-01,  7.1754e-02,  1.2179e-01,  5.5603e-02,\n",
      "         1.3467e-01, -5.0143e-02, -3.1426e-02, -1.6900e-01,  2.1070e-01,\n",
      "         1.6351e-02, -1.8882e-01, -8.6733e-02, -3.1602e-02, -1.7960e-01,\n",
      "         2.0460e-01, -4.9045e-02, -3.2208e-01,  2.1344e-01,  9.2138e-02,\n",
      "        -1.0753e-01,  2.5539e-01, -4.2675e-02, -7.8672e-02, -1.4348e-01,\n",
      "        -2.4280e-01, -2.6981e-02, -7.6275e-02,  3.0260e-02, -7.9444e-02,\n",
      "        -6.2093e-02,  8.3577e-02, -1.2643e-01,  1.4936e-01, -7.3054e-02,\n",
      "         3.9078e-02, -2.3725e-01,  1.1208e-01,  1.5797e-01,  7.9630e-02,\n",
      "        -1.3259e-01, -3.7383e-01,  3.8218e-02, -1.9410e-02,  2.7409e-01,\n",
      "        -1.8870e-01, -1.5440e-01,  6.3889e-02,  2.5098e-02,  4.7417e-04,\n",
      "         2.6955e-02,  7.2774e-02,  7.2907e-02,  1.4183e-01,  6.6102e-02,\n",
      "         7.0876e-02,  6.7858e-03, -9.8087e-03,  1.5485e-03, -1.1572e-01,\n",
      "        -1.7263e-01,  3.4770e-01,  6.7416e-02, -7.7063e-02,  6.5769e-02,\n",
      "         2.1676e-01, -4.1826e-02, -2.5761e-01, -8.6365e-02, -8.1392e-03,\n",
      "        -1.8885e-01,  1.3244e-01,  1.1535e-01, -9.6740e-02, -1.5293e-01,\n",
      "        -7.6227e-02,  7.7551e-02,  1.7660e-01, -1.1213e-01,  2.3233e-01,\n",
      "         6.1620e-02, -2.7663e-01, -1.6507e-01,  2.9106e-03,  6.6911e-03,\n",
      "        -7.7984e-02,  1.9647e-02, -2.6358e-01, -2.6940e-01,  2.6764e-01,\n",
      "        -8.6312e-02, -2.9752e-01, -2.5509e-01, -1.0590e-01, -6.5351e-02,\n",
      "         1.8160e-01, -1.3983e-01,  1.4152e-01,  6.3947e-02,  4.2251e-02,\n",
      "        -3.4273e-01,  1.7741e-01, -2.0652e-01, -1.6881e-01,  2.1060e-02,\n",
      "        -1.4637e-01,  8.8312e-02,  1.7314e-01,  1.4907e-01,  1.1012e-01,\n",
      "        -2.6216e-01, -4.2944e-02, -4.9879e-02, -2.7647e-01,  4.4283e-04,\n",
      "        -2.1121e-01, -2.0353e-01,  5.1000e-02, -2.0453e-01, -3.4814e-02,\n",
      "         1.9685e-01,  3.7692e-02,  2.0733e-01, -1.1983e-01, -1.4398e-01,\n",
      "        -1.2621e-01, -1.7408e-01, -2.7325e-01,  1.6391e-01,  5.9569e-02,\n",
      "        -2.2667e-02, -1.2713e-01, -1.8591e-01,  1.9703e-01, -5.5476e-02,\n",
      "         8.6366e-02,  2.5422e-01,  2.1397e-01,  1.8524e-01, -1.2483e-01,\n",
      "         1.0409e-01,  1.6925e-01,  4.0220e-02, -9.5552e-02, -1.4715e-01,\n",
      "        -1.8798e-01, -2.1895e-01,  2.2117e-02,  1.5368e-01, -1.6121e-01,\n",
      "        -1.1861e-01,  1.3668e-02,  4.5662e-02, -1.1567e-01,  1.4127e-01,\n",
      "        -2.7942e-01, -1.5292e-01, -1.7911e-01, -3.2694e-01,  5.1772e-01,\n",
      "         2.7835e-01,  1.2317e-01, -3.2789e-03, -3.4983e-01, -7.9231e-02,\n",
      "         1.5537e-01,  1.6642e-01,  6.9869e-02, -7.5821e-02, -1.6838e-02,\n",
      "        -2.3324e-01, -2.2981e-01, -2.5032e-01,  2.9020e-02, -2.7916e-02,\n",
      "        -1.5314e-01,  7.8514e-02, -1.9741e-01, -9.5195e-02,  1.6242e-01,\n",
      "         3.8419e-02, -3.6239e-01, -9.9694e-02,  2.0209e-01, -1.5562e-01,\n",
      "         5.7810e-02,  1.3632e-02, -2.6938e-01, -3.5650e-01,  2.2875e-01,\n",
      "        -4.8769e-02,  1.8736e-01, -1.1909e-01,  1.3218e-01, -8.2738e-03],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0166, -0.0390,  0.0753,  ..., -0.0304,  0.0412, -0.0334],\n",
      "        [ 0.3195, -0.2127, -0.2586,  ..., -0.0173, -0.1247,  0.0660],\n",
      "        [ 0.2063,  0.1578, -0.2601,  ..., -0.2730,  0.0436,  0.0845],\n",
      "        ...,\n",
      "        [-0.0010, -0.0595,  0.0713,  ..., -0.0200, -0.0295,  0.0500],\n",
      "        [-0.0368, -0.0438,  0.0319,  ...,  0.0059, -0.0332, -0.0285],\n",
      "        [ 0.0575, -0.0035, -0.0347,  ...,  0.0478,  0.0066, -0.0371]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.8911e-02,  2.2901e+00,  7.8043e-01,  ..., -1.6430e-02,\n",
      "        -8.3252e-04, -3.8392e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for p in attn_decoder1.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1bb9d",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cee32e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "    text = list()\n",
    "    headline = list()\n",
    "    pred_headline = list()\n",
    "    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        \n",
    "        if(len(pair[0].split())>=150):\n",
    "            continue\n",
    "        else:\n",
    "            if(i%1000 == 0):\n",
    "                print(i*100/n,\"% complete\")\n",
    "\n",
    "            # print('>', pair[0])\n",
    "            text.append(pair[0])\n",
    "            # print('=', pair[1])\n",
    "            headline.append(pair[1])\n",
    "            output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "            output_sentence = ' '.join(output_words)\n",
    "            pred_headline.append(output_sentence)\n",
    "            # print('<', output_sentence)\n",
    "            # print('')\n",
    "            \n",
    "    return(text,headline,pred_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06d70c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % complete\n",
      "6.666666666666667 % complete\n",
      "13.333333333333334 % complete\n",
      "20.0 % complete\n",
      "26.666666666666668 % complete\n",
      "33.333333333333336 % complete\n",
      "40.0 % complete\n",
      "46.666666666666664 % complete\n",
      "53.333333333333336 % complete\n",
      "60.0 % complete\n",
      "66.66666666666667 % complete\n",
      "73.33333333333333 % complete\n",
      "80.0 % complete\n",
      "86.66666666666667 % complete\n",
      "93.33333333333333 % complete\n"
     ]
    }
   ],
   "source": [
    "text,headline,pred_headline = evaluateRandomly(encoder1, attn_decoder1, 15000)\n",
    "\n",
    "pred_df_GRU = pd.DataFrame()\n",
    "\n",
    "pred_df_GRU['text'] = text\n",
    "pred_df_GRU['headline'] = headline\n",
    "pred_df_GRU['pred_headline'] = pred_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e38a79b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "      <th>pred_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ends drying subjecting drying saliva containin...</td>\n",
       "      <td>lick lips constantly use high quality lip glos...</td>\n",
       "      <td>get good sized bowl mix honey water bowl mix a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hair loose classic way wear prevents lot damag...</td>\n",
       "      <td>wear hair straight wear curly get wavy hair ma...</td>\n",
       "      <td>brush hair brush brush brush brush brush brush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>draw cross section inside circle facing right ...</td>\n",
       "      <td>draw small circle tail line draw stick figures...</td>\n",
       "      <td>draw circle head head head one two circle one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wash roma tomato faucet turning necessary wash...</td>\n",
       "      <td>wash tomato remove top cut tomato half cut tom...</td>\n",
       "      <td>place half cut place cut half place place &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conducted plumpy nut challenge increase awaren...</td>\n",
       "      <td>contact merlin charity uk sign plumpy nut chal...</td>\n",
       "      <td>check whether check check &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14904</th>\n",
       "      <td>uber available selected location redirected ci...</td>\n",
       "      <td>open uber cities website click find city field...</td>\n",
       "      <td>open uber app tap tap tap location tap tap loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14905</th>\n",
       "      <td>dispenser usually shoot farthest however items...</td>\n",
       "      <td>see far shoot get average items remember dropp...</td>\n",
       "      <td>go find find find find go find find find go fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14906</th>\n",
       "      <td>stiff skin syndrome characterized development ...</td>\n",
       "      <td>check surface body hard thick skin test joint ...</td>\n",
       "      <td>get water water water water water get water wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>stand straight heels together toes pointed out...</td>\n",
       "      <td>stand correctly bring elbow knee together brin...</td>\n",
       "      <td>try leg leg leg leg forward leg one leg &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14908</th>\n",
       "      <td>get together study groups commit exchanging id...</td>\n",
       "      <td>form atmosphere cooperation studious kind peer...</td>\n",
       "      <td>avoid like like like talk questions &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14909 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      ends drying subjecting drying saliva containin...   \n",
       "1      hair loose classic way wear prevents lot damag...   \n",
       "2      draw cross section inside circle facing right ...   \n",
       "3      wash roma tomato faucet turning necessary wash...   \n",
       "4      conducted plumpy nut challenge increase awaren...   \n",
       "...                                                  ...   \n",
       "14904  uber available selected location redirected ci...   \n",
       "14905  dispenser usually shoot farthest however items...   \n",
       "14906  stiff skin syndrome characterized development ...   \n",
       "14907  stand straight heels together toes pointed out...   \n",
       "14908  get together study groups commit exchanging id...   \n",
       "\n",
       "                                                headline  \\\n",
       "0      lick lips constantly use high quality lip glos...   \n",
       "1      wear hair straight wear curly get wavy hair ma...   \n",
       "2      draw small circle tail line draw stick figures...   \n",
       "3      wash tomato remove top cut tomato half cut tom...   \n",
       "4      contact merlin charity uk sign plumpy nut chal...   \n",
       "...                                                  ...   \n",
       "14904  open uber cities website click find city field...   \n",
       "14905  see far shoot get average items remember dropp...   \n",
       "14906  check surface body hard thick skin test joint ...   \n",
       "14907  stand correctly bring elbow knee together brin...   \n",
       "14908  form atmosphere cooperation studious kind peer...   \n",
       "\n",
       "                                           pred_headline  \n",
       "0      get good sized bowl mix honey water bowl mix a...  \n",
       "1      brush hair brush brush brush brush brush brush...  \n",
       "2      draw circle head head head one two circle one ...  \n",
       "3        place half cut place cut half place place <EOS>  \n",
       "4                        check whether check check <EOS>  \n",
       "...                                                  ...  \n",
       "14904  open uber app tap tap tap location tap tap loc...  \n",
       "14905  go find find find find go find find find go fi...  \n",
       "14906  get water water water water water get water wa...  \n",
       "14907      try leg leg leg leg forward leg one leg <EOS>  \n",
       "14908          avoid like like like talk questions <EOS>  \n",
       "\n",
       "[14909 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b70f43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_GRU.to_csv('model/Attention_GRU_Pred.csv',sep='\\t',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77346b9",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics\n",
    "\n",
    "For our text summarization problem, there can be multiple correct answers and as we do not have a single correct output, we can evaluate our model using different parameters like — Recall, Precision, F-score. Below are some of the metrics:\n",
    "\n",
    "* **BLEU(Bilingual Evaluation Understudy)**: The cornerstone of this metric is precision having values between [0,1], wherein 1 represents a perfect match and 0 represents a complete mismatch. This metric is basically calculated by comparing the number of machine-generated words that are a part of the reference sentence with respect to the total number of words in the machine-generated output.\n",
    "\n",
    "\n",
    "* **ROUGE (Recall-Oriented Understanding for Gisting Evaluation)**: A recall-oriented measure that works by comparing the number of machine-generated words that are a part of the reference sentence with respect to the total number of words in the reference sentence.\n",
    "\n",
    "    This metric is more intuitive in the sense that every time we add a reference to the pool, we expand our space of alternating summaries. Hence, this metric should be preferred when we have multiple references.\n",
    "    \n",
    "    ROUGE implementation is pretty similar to BELU, however, there are other underlying implementations like LCS (Longest common subsequence) and skip-gram, etc. We can directly use the ROUGE-N implementation using the python library ROUGE.\n",
    "    \n",
    "\n",
    "In this tutorial, we will use ROUGE to evaluate the model.\n",
    "\n",
    "\n",
    "**Recall, r**\n",
    "The recall counts the number of overlapping n-grams found in both the model output and reference — then divides this number by the total number of n-grams in the reference. This is great for ensuring our model is capturing all of the information contained in the reference — but this isn’t so great at ensuring our model isn’t just pushing out a huge number of words to game the recall score.\n",
    "\n",
    "\n",
    "**Precision, p**\n",
    "To avoid this we use the precision metric — which is calculated in almost the exact same way, but rather than dividing by the reference n-gram count, we divide by the model n-gram count.\n",
    "\n",
    "\n",
    "**F1-Score, f**\n",
    "Now that we both the recall and precision values, we can use them to calculate our ROUGE F1 score like so:\n",
    "![F1_score](../../../images/F1.png)\n",
    "\n",
    "That gives us a reliable measure of our model performance that relies not only on the model capturing as many words as possible (recall) but doing so without outputting irrelevant words (precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f970d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.08824770650530915,\n",
       "  'p': 0.234780419508386,\n",
       "  'f': 0.11945047228398172},\n",
       " 'rouge-2': {'r': 0.013882968414462099,\n",
       "  'p': 0.027146369772616856,\n",
       "  'f': 0.016841815053443242},\n",
       " 'rouge-l': {'r': 0.08238563805616168,\n",
       "  'p': 0.21928321945281865,\n",
       "  'f': 0.11142691640129991}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install rouge\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge.get_scores(pred_df_GRU['pred_headline'], pred_df_GRU['headline'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae06020",
   "metadata": {},
   "source": [
    "# Extension\n",
    "\n",
    "The current model is only used for demo purpose and can be optimized by further tuning the hyperparameters (learning rate, optimizer, loss function, hidden layers, momentum, iterations, etc.)\n",
    "\n",
    "You also can try different algorithms and neural network architectures:\n",
    "\n",
    "1. Bi-directional/stacked GRU cells can be used for improving performance\n",
    "\n",
    "\n",
    "2. Implementing different kinds of attention-mechanism — multiplicative attention, multi-head attention, etc.\n",
    "\n",
    "\n",
    "3. Output words can be selected using beam search instead of greedy-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Text Summarization Using Deep Neural Networks](https://towardsdatascience.com/text-summarization-using-deep-neural-networks-e7ee7521d804)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
