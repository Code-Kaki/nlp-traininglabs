{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "> **Copyright (c) 2020 Skymind Holdings Berhad**<br><br>\n",
    "> **Copyright (c) 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    "<br>\n",
    "Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "<br>you may not use this file except in compliance with the License.\n",
    "<br>You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0/\n",
    "<br>\n",
    "<br>Unless required by applicable law or agreed to in writing, software\n",
    "<br>distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n",
    "<br>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "<br>See the License for the specific language governing permissions and\n",
    "<br>limitations under the License.\n",
    "<br>\n",
    "<br>\n",
    "**SPDX-License-Identifier: Apache-2.0**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407be03",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the handson part for the text representation. You will learn the various implementation of text representation from scratch as well as using `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7286a59",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [One-Hot Encoding](#One-Hot-Encoding)\n",
    "\n",
    "\n",
    "* [Bag-of-Words (BoW)](#Bag-of-Words-(BoW))\n",
    "\n",
    "\n",
    "* [Count Vectorizer](#Count-Vectorizer)\n",
    "\n",
    "\n",
    "* [TF-IDF](#TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7d06d",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6474133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re # Regular Expression\n",
    "import nltk\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a31d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data:\n",
      "What Is Artificial Intelligence (AI)?\n",
      "Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
      "\n",
      "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal. A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans. Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
      "\n",
      "When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth.\n",
      "\n",
      "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity. Researchers and developers in the field are making surprisingly rapid strides in mimicking activities such as learning, reasoning, and perception, to the extent that these can be concretely defined. Some believe that innovators may soon be able to develop systems that exceed the capacity of humans to learn or reason out any subject. But others remain skeptical because all cognitive activity is laced with value judgments that are subject to human experience.\n",
      "\n",
      "As technology advances, previous benchmarks that defined artificial intelligence become outdated. For example, machines that calculate basic functions or recognize text through optical character recognition are no longer considered to embody artificial intelligence, since this function is now taken for granted as an inherent computer function.\n",
      "\n",
      "AI is continuously evolving to benefit many different industries. Machines are wired using a cross-disciplinary approach based on mathematics, computer science, linguistics, psychology, and more.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "\n",
    "with open(\"../../../resources/day_03/sample_text.txt\") as file:\n",
    "    text = \"\".join(file.readlines())\n",
    "\n",
    "print(\"Text Data:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800564b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence List:\n",
      "['What Is Artificial Intelligence (AI)?', 'Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.', 'The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.', 'The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.', 'A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans.', 'Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.', 'When most people hear the term artificial intelligence, the first thing they usually think of is robots.', \"That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth.\", 'But nothing could be further from the truth.', 'Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex.', 'The goals of artificial intelligence include mimicking human cognitive activity.', 'Researchers and developers in the field are making surprisingly rapid strides in mimicking activities such as learning, reasoning, and perception, to the extent that these can be concretely defined.', 'Some believe that innovators may soon be able to develop systems that exceed the capacity of humans to learn or reason out any subject.', 'But others remain skeptical because all cognitive activity is laced with value judgments that are subject to human experience.', 'As technology advances, previous benchmarks that defined artificial intelligence become outdated.', 'For example, machines that calculate basic functions or recognize text through optical character recognition are no longer considered to embody artificial intelligence, since this function is now taken for granted as an inherent computer function.', 'AI is continuously evolving to benefit many different industries.', 'Machines are wired using a cross-disciplinary approach based on mathematics, computer science, linguistics, psychology, and more.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentence List:\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8394385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Sentences:\n",
      "['what is artificial intelligence ai', 'artificial intelligence ai refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions', 'the term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problemsolving', 'the ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal', 'a subset of artificial intelligence is machine learning which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans', 'deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text images or video', 'when most people hear the term artificial intelligence the first thing they usually think of is robots', 'thats because bigbudget films and novels weave stories about humanlike machines that wreak havoc on earth', 'but nothing could be further from the truth', 'artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks from the most simple to those that are even more complex', 'the goals of artificial intelligence include mimicking human cognitive activity', 'researchers and developers in the field are making surprisingly rapid strides in mimicking activities such as learning reasoning and perception to the extent that these can be concretely defined', 'some believe that innovators may soon be able to develop systems that exceed the capacity of humans to learn or reason out any subject', 'but others remain skeptical because all cognitive activity is laced with value judgments that are subject to human experience', 'as technology advances previous benchmarks that defined artificial intelligence become outdated', 'for example machines that calculate basic functions or recognize text through optical character recognition are no longer considered to embody artificial intelligence since this function is now taken for granted as an inherent computer function', 'ai is continuously evolving to benefit many different industries', 'machines are wired using a crossdisciplinary approach based on mathematics computer science linguistics psychology and more']\n"
     ]
    }
   ],
   "source": [
    "processed_sentences = list(map(lambda s: re.sub(r'[^\\w\\s]','', s.lower()), sentences))\n",
    "\n",
    "print(\"Cleaned Sentences:\")\n",
    "print(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3fbddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List:\n",
      "['what', 'is', 'artificial', 'intelligence', 'ai', 'artificial', 'intelligence', 'ai', 'refers', 'to', 'the', 'simulation', 'of', 'human', 'intelligence', 'in', 'machines', 'that', 'are', 'programmed', 'to', 'think', 'like', 'humans', 'and', 'mimic', 'their', 'actions', 'the', 'term', 'may', 'also', 'be', 'applied', 'to', 'any', 'machine', 'that', 'exhibits', 'traits', 'associated', 'with', 'a', 'human', 'mind', 'such', 'as', 'learning', 'and', 'problemsolving', 'the', 'ideal', 'characteristic', 'of', 'artificial', 'intelligence', 'is', 'its', 'ability', 'to', 'rationalize', 'and', 'take', 'actions', 'that', 'have', 'the', 'best', 'chance', 'of', 'achieving', 'a', 'specific', 'goal', 'a', 'subset', 'of', 'artificial', 'intelligence', 'is', 'machine', 'learning', 'which', 'refers', 'to', 'the', 'concept', 'that', 'computer', 'programs', 'can', 'automatically', 'learn', 'from', 'and', 'adapt', 'to', 'new', 'data', 'without', 'being', 'assisted', 'by', 'humans', 'deep', 'learning', 'techniques', 'enable', 'this', 'automatic', 'learning', 'through', 'the', 'absorption', 'of', 'huge', 'amounts', 'of', 'unstructured', 'data', 'such', 'as', 'text', 'images', 'or', 'video', 'when', 'most', 'people', 'hear', 'the', 'term', 'artificial', 'intelligence', 'the', 'first', 'thing', 'they', 'usually', 'think', 'of', 'is', 'robots', 'thats', 'because', 'bigbudget', 'films', 'and', 'novels', 'weave', 'stories', 'about', 'humanlike', 'machines', 'that', 'wreak', 'havoc', 'on', 'earth', 'but', 'nothing', 'could', 'be', 'further', 'from', 'the', 'truth', 'artificial', 'intelligence', 'is', 'based', 'on', 'the', 'principle', 'that', 'human', 'intelligence', 'can', 'be', 'defined', 'in', 'a', 'way', 'that', 'a', 'machine', 'can', 'easily', 'mimic', 'it', 'and', 'execute', 'tasks', 'from', 'the', 'most', 'simple', 'to', 'those', 'that', 'are', 'even', 'more', 'complex', 'the', 'goals', 'of', 'artificial', 'intelligence', 'include', 'mimicking', 'human', 'cognitive', 'activity', 'researchers', 'and', 'developers', 'in', 'the', 'field', 'are', 'making', 'surprisingly', 'rapid', 'strides', 'in', 'mimicking', 'activities', 'such', 'as', 'learning', 'reasoning', 'and', 'perception', 'to', 'the', 'extent', 'that', 'these', 'can', 'be', 'concretely', 'defined', 'some', 'believe', 'that', 'innovators', 'may', 'soon', 'be', 'able', 'to', 'develop', 'systems', 'that', 'exceed', 'the', 'capacity', 'of', 'humans', 'to', 'learn', 'or', 'reason', 'out', 'any', 'subject', 'but', 'others', 'remain', 'skeptical', 'because', 'all', 'cognitive', 'activity', 'is', 'laced', 'with', 'value', 'judgments', 'that', 'are', 'subject', 'to', 'human', 'experience', 'as', 'technology', 'advances', 'previous', 'benchmarks', 'that', 'defined', 'artificial', 'intelligence', 'become', 'outdated', 'for', 'example', 'machines', 'that', 'calculate', 'basic', 'functions', 'or', 'recognize', 'text', 'through', 'optical', 'character', 'recognition', 'are', 'no', 'longer', 'considered', 'to', 'embody', 'artificial', 'intelligence', 'since', 'this', 'function', 'is', 'now', 'taken', 'for', 'granted', 'as', 'an', 'inherent', 'computer', 'function', 'ai', 'is', 'continuously', 'evolving', 'to', 'benefit', 'many', 'different', 'industries', 'machines', 'are', 'wired', 'using', 'a', 'crossdisciplinary', 'approach', 'based', 'on', 'mathematics', 'computer', 'science', 'linguistics', 'psychology', 'and', 'more']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for s in processed_sentences:\n",
    "    words += word_tokenize(s)\n",
    "\n",
    "print(\"Word List:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a9719e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words:\n",
      "{'humans', 'which', 'innovators', 'skeptical', 'problemsolving', 'mimicking', 'mind', 'with', 'specific', 'hear', 'evolving', 'traits', 'longer', 'develop', 'intelligence', 'reasoning', 'many', 'previous', 'enable', 'since', 'programmed', 'any', 'activities', 'a', 'for', 'embody', 'through', 'function', 'concretely', 'some', 'industries', 'actions', 'reason', 'soon', 'experience', 'ideal', 'techniques', 'novels', 'robots', 'granted', 'become', 'absorption', 'rapid', 'adapt', 'easily', 'exceed', 'this', 'its', 'simple', 'automatically', 'and', 'have', 'term', 'crossdisciplinary', 'benefit', 'able', 'recognition', 'considered', 'outdated', 'perception', 'text', 'computer', 'are', 'goals', 'about', 'automatic', 'amounts', 'may', 'can', 'also', 'include', 'researchers', 'of', 'usually', 'films', 'truth', 'concept', 'or', 'chance', 'extent', 'laced', 'advances', 'character', 'ai', 'such', 'way', 'continuously', 'take', 'those', 'on', 'surprisingly', 'what', 'benchmarks', 'approach', 'now', 'but', 'could', 'exhibits', 'first', 'programs', 'weave', 'subject', 'cognitive', 'principle', 'even', 'taken', 'their', 'nothing', 'like', 'refers', 'ability', 'learn', 'most', 'mathematics', 'humanlike', 'simulation', 'images', 'judgments', 'believe', 'others', 'without', 'making', 'calculate', 'new', 'defined', 'field', 'out', 'machine', 'machines', 'be', 'in', 'different', 'being', 'the', 'these', 'applied', 'bigbudget', 'it', 'all', 'inherent', 'characteristic', 'think', 'psychology', 'artificial', 'that', 'wreak', 'value', 'recognize', 'tasks', 'rationalize', 'stories', 'science', 'havoc', 'huge', 'data', 'execute', 'to', 'best', 'strides', 'goal', 'video', 'functions', 'is', 'they', 'by', 'based', 'associated', 'assisted', 'earth', 'further', 'example', 'an', 'thats', 'systems', 'developers', 'achieving', 'complex', 'people', 'linguistics', 'capacity', 'mimic', 'thing', 'optical', 'unstructured', 'technology', 'using', 'from', 'human', 'as', 'deep', 'wired', 'learning', 'activity', 'because', 'basic', 'subset', 'more', 'no', 'remain', 'when'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(words)\n",
    "\n",
    "print(\"Unique Words:\")\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cde5ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Dictionary:\n",
      "{'humans': 0, 'which': 1, 'innovators': 2, 'skeptical': 3, 'problemsolving': 4, 'mimicking': 5, 'mind': 6, 'with': 7, 'specific': 8, 'hear': 9, 'evolving': 10, 'traits': 11, 'longer': 12, 'develop': 13, 'intelligence': 14, 'reasoning': 15, 'many': 16, 'previous': 17, 'enable': 18, 'since': 19, 'programmed': 20, 'any': 21, 'activities': 22, 'a': 23, 'for': 24, 'embody': 25, 'through': 26, 'function': 27, 'concretely': 28, 'some': 29, 'industries': 30, 'actions': 31, 'reason': 32, 'soon': 33, 'experience': 34, 'ideal': 35, 'techniques': 36, 'novels': 37, 'robots': 38, 'granted': 39, 'become': 40, 'absorption': 41, 'rapid': 42, 'adapt': 43, 'easily': 44, 'exceed': 45, 'this': 46, 'its': 47, 'simple': 48, 'automatically': 49, 'and': 50, 'have': 51, 'term': 52, 'crossdisciplinary': 53, 'benefit': 54, 'able': 55, 'recognition': 56, 'considered': 57, 'outdated': 58, 'perception': 59, 'text': 60, 'computer': 61, 'are': 62, 'goals': 63, 'about': 64, 'automatic': 65, 'amounts': 66, 'may': 67, 'can': 68, 'also': 69, 'include': 70, 'researchers': 71, 'of': 72, 'usually': 73, 'films': 74, 'truth': 75, 'concept': 76, 'or': 77, 'chance': 78, 'extent': 79, 'laced': 80, 'advances': 81, 'character': 82, 'ai': 83, 'such': 84, 'way': 85, 'continuously': 86, 'take': 87, 'those': 88, 'on': 89, 'surprisingly': 90, 'what': 91, 'benchmarks': 92, 'approach': 93, 'now': 94, 'but': 95, 'could': 96, 'exhibits': 97, 'first': 98, 'programs': 99, 'weave': 100, 'subject': 101, 'cognitive': 102, 'principle': 103, 'even': 104, 'taken': 105, 'their': 106, 'nothing': 107, 'like': 108, 'refers': 109, 'ability': 110, 'learn': 111, 'most': 112, 'mathematics': 113, 'humanlike': 114, 'simulation': 115, 'images': 116, 'judgments': 117, 'believe': 118, 'others': 119, 'without': 120, 'making': 121, 'calculate': 122, 'new': 123, 'defined': 124, 'field': 125, 'out': 126, 'machine': 127, 'machines': 128, 'be': 129, 'in': 130, 'different': 131, 'being': 132, 'the': 133, 'these': 134, 'applied': 135, 'bigbudget': 136, 'it': 137, 'all': 138, 'inherent': 139, 'characteristic': 140, 'think': 141, 'psychology': 142, 'artificial': 143, 'that': 144, 'wreak': 145, 'value': 146, 'recognize': 147, 'tasks': 148, 'rationalize': 149, 'stories': 150, 'science': 151, 'havoc': 152, 'huge': 153, 'data': 154, 'execute': 155, 'to': 156, 'best': 157, 'strides': 158, 'goal': 159, 'video': 160, 'functions': 161, 'is': 162, 'they': 163, 'by': 164, 'based': 165, 'associated': 166, 'assisted': 167, 'earth': 168, 'further': 169, 'example': 170, 'an': 171, 'thats': 172, 'systems': 173, 'developers': 174, 'achieving': 175, 'complex': 176, 'people': 177, 'linguistics': 178, 'capacity': 179, 'mimic': 180, 'thing': 181, 'optical': 182, 'unstructured': 183, 'technology': 184, 'using': 185, 'from': 186, 'human': 187, 'as': 188, 'deep': 189, 'wired': 190, 'learning': 191, 'activity': 192, 'because': 193, 'basic': 194, 'subset': 195, 'more': 196, 'no': 197, 'remain': 198, 'when': 199}\n"
     ]
    }
   ],
   "source": [
    "vocab = {word:idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "print(\"Vocabulary Dictionary:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db99470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab: 200\n"
     ]
    }
   ],
   "source": [
    "# Size of vocab\n",
    "m = len(vocab)\n",
    "\n",
    "print(\"Size of vocab:\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58613c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(sentence):\n",
    "    encoded_vec = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        vec = [0] * m\n",
    "        vec[vocab[word]] = 1\n",
    "        encoded_vec.append(vec)\n",
    "    return np.array(encoded_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5191a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: what is artificial intelligence ai\n",
      "\n",
      "After One Hot Encoding:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "Size of encoded vector:\n",
      "(5, 200)\n"
     ]
    }
   ],
   "source": [
    "# Encode the first sentence\n",
    "sentence = processed_sentences[0]\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "encoded_sentence = oneHotEncode(sentence)\n",
    "print(\"\\nAfter One Hot Encoding:\")\n",
    "print(encoded_sentence)\n",
    "print(\"\\nSize of encoded vector:\")\n",
    "print(encoded_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd546af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e89e94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193  99  21  98  11  21  98  11 146 183 174 152 125  88  98  93 110 172\n",
      "  20 137 183 179 106  90  16 115 175   6 174 170 114  13  29  18 183  17\n",
      " 109 172  70 184  24 197   0  88 117 162  22 105  16 136 174  91  45 125\n",
      "  21  98  99 101   1 183 141  16 165   6 172  84 174  36  43 125   5   0\n",
      " 157  81   0 161 125  21  98  99 109 105 195 146 183 174  49 172  48 138\n",
      "  41  26 104  77  16   9 183 120  55 198  32  23  39  90  56 105 168  64\n",
      " 180  25 105 182 174   4 125  87  14 125 186  55 162  22 171  92 128 190\n",
      " 194 119 132  86 174 170  21  98 174  75 178 177 188 179 125  99 149 173\n",
      "  30  37  74  16 123 192 158   3  89 110 172 199  85 126  61  38 122  53\n",
      "  29  80  77 174 185  21  98  99  27 126 174 135 172  88  98  41  29  57\n",
      "  93   0 191 172   0 109  41  62 115 100  16  69 167  77 174 119 151 183\n",
      " 181 172  20  65 118  47 174  82 125  21  98  94 116  88  46   8 148  16\n",
      "  59  93 174  73  20 111 163 140 159  93 116   7 162  22 105 143  16 133\n",
      " 183 174  72 172 176  41  29  50  57 155  33 172  97 114 156  29   2 183\n",
      "  58 164 172  68 174  42 125  90 183 104 128 142 130  17 160  38 129 147\n",
      " 154  30  12  46   8  99 103 197 189 102 172  20 160 183  88  71  22 169\n",
      "  10 134  34 172  57  21  98  31 131  76  67 110 172  40  28  79 128 145\n",
      " 171 182 127  44 144  20 121 108  51 183  63  21  98 153 180  78  99 124\n",
      " 166  76  83  22  15  96  48  78  11  99  52  66 183  35 112  60  95 110\n",
      "  20 196 187   0  54  19  27 126 113  48 150 107 139  16 118]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "int_words = label_encoder.fit_transform(words)\n",
    "\n",
    "print(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae7af90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 193)\t1.0\n",
      "  (1, 99)\t1.0\n",
      "  (2, 21)\t1.0\n",
      "  (3, 98)\t1.0\n",
      "  (4, 11)\t1.0\n",
      "  (5, 21)\t1.0\n",
      "  (6, 98)\t1.0\n",
      "  (7, 11)\t1.0\n",
      "  (8, 146)\t1.0\n",
      "  (9, 183)\t1.0\n",
      "  (10, 174)\t1.0\n",
      "  (11, 152)\t1.0\n",
      "  (12, 125)\t1.0\n",
      "  (13, 88)\t1.0\n",
      "  (14, 98)\t1.0\n",
      "  (15, 93)\t1.0\n",
      "  (16, 110)\t1.0\n",
      "  (17, 172)\t1.0\n",
      "  (18, 20)\t1.0\n",
      "  (19, 137)\t1.0\n",
      "  (20, 183)\t1.0\n",
      "  (21, 179)\t1.0\n",
      "  (22, 106)\t1.0\n",
      "  (23, 90)\t1.0\n",
      "  (24, 16)\t1.0\n",
      "  :\t:\n",
      "  (332, 11)\t1.0\n",
      "  (333, 99)\t1.0\n",
      "  (334, 52)\t1.0\n",
      "  (335, 66)\t1.0\n",
      "  (336, 183)\t1.0\n",
      "  (337, 35)\t1.0\n",
      "  (338, 112)\t1.0\n",
      "  (339, 60)\t1.0\n",
      "  (340, 95)\t1.0\n",
      "  (341, 110)\t1.0\n",
      "  (342, 20)\t1.0\n",
      "  (343, 196)\t1.0\n",
      "  (344, 187)\t1.0\n",
      "  (345, 0)\t1.0\n",
      "  (346, 54)\t1.0\n",
      "  (347, 19)\t1.0\n",
      "  (348, 27)\t1.0\n",
      "  (349, 126)\t1.0\n",
      "  (350, 113)\t1.0\n",
      "  (351, 48)\t1.0\n",
      "  (352, 150)\t1.0\n",
      "  (353, 107)\t1.0\n",
      "  (354, 139)\t1.0\n",
      "  (355, 16)\t1.0\n",
      "  (356, 118)\t1.0\n"
     ]
    }
   ],
   "source": [
    "oneHot_encoder = OneHotEncoder()\n",
    "\n",
    "print(oneHot_encoder.fit_transform(np.array(words).reshape(len(words), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857c795a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: what is artificial intelligence ai\n",
      "\n",
      "After One Hot Encoding:\n",
      "  (0, 193)\t1.0\n",
      "  (0, 99)\t1.0\n",
      "  (0, 21)\t1.0\n",
      "  (0, 98)\t1.0\n",
      "  (0, 11)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Encode the first sentence\n",
    "sentence = processed_sentences[0]\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "encoded_sentence = oneHotEncode(sentence)\n",
    "print(\"\\nAfter One Hot Encoding:\")\n",
    "for w in word_tokenize(processed_sentences[0]):\n",
    "    print(oneHot_encoder.transform([[w]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6441b68",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3787ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3de52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f653823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da16bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefaf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc29a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfa4c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n"
     ]
    }
   ],
   "source": [
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72ff1db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n"
     ]
    }
   ],
   "source": [
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "\n",
    "print(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bec4c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10bd18",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "503e9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee56daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'artificial': 7, 'intelligence': 23, 'ai': 4, 'refers': 37, 'to': 46, 'the': 43, 'simulation': 38, 'of': 33, 'human': 19, 'in': 22, 'machines': 30, 'that': 42, 'are': 6, 'programmed': 34, 'think': 45, 'like': 28, 'humans': 20, 'and': 5, 'mimic': 31, 'their': 44, 'actions': 2, 'ideal': 21, 'characteristic': 12, 'is': 24, 'its': 25, 'ability': 0, 'rationalize': 36, 'take': 41, 'have': 18, 'best': 9, 'chance': 11, 'achieving': 1, 'specific': 39, 'goal': 17, 'subset': 40, 'machine': 29, 'learning': 27, 'which': 47, 'concept': 14, 'computer': 13, 'programs': 35, 'can': 10, 'automatically': 8, 'learn': 26, 'from': 16, 'adapt': 3, 'new': 32, 'data': 15}\n",
      "Encoded Document is:\n",
      "[[0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 2 0 0 0 0 1 0 1 1 0 1 1 0\n",
      "  0 1 1 0 0 0 1 1 1 1 2 0]\n",
      " [1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 2 0 0\n",
      "  1 0 0 1 0 1 1 2 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
      "  0 1 0 0 1 0 1 1 0 0 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "  \n",
    "document = [\"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\",\n",
    "            \"The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\",\n",
    "            \"A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data\"]\n",
    "  \n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "  \n",
    "vectorizer.fit(document)\n",
    "  \n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "  \n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "  \n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e1b2a",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52d01468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.46263733109032296\n",
      "  (0, 7)\t0.8865476297873808\n",
      "  (1, 12)\t0.6108781210948048\n",
      "  (1, 9)\t0.318781545479458\n",
      "  (1, 5)\t0.3899155916311765\n",
      "  (1, 3)\t0.6108781210948048\n",
      "  (2, 11)\t0.38691946801941357\n",
      "  (2, 9)\t0.20191062952175412\n",
      "  (2, 8)\t0.38691946801941357\n",
      "  (2, 6)\t0.38691946801941357\n",
      "  (2, 5)\t0.24696568444132605\n",
      "  (2, 4)\t0.38691946801941357\n",
      "  (2, 1)\t0.38691946801941357\n",
      "  (2, 0)\t0.38691946801941357\n",
      "  (3, 15)\t0.4196006932295896\n",
      "  (3, 14)\t0.4196006932295896\n",
      "  (3, 13)\t0.4196006932295896\n",
      "  (3, 10)\t0.4196006932295896\n",
      "  (3, 9)\t0.2189650485963657\n",
      "  (3, 5)\t0.26782568715384725\n",
      "  (3, 2)\t0.4196006932295896\n",
      "{'about': 1.916290731874155, 'all': 1.916290731874155, 'baisc': 1.916290731874155, 'cool': 1.916290731874155, 'helping': 1.916290731874155, 'is': 1.2231435513142097, 'language': 1.916290731874155, 'love': 1.916290731874155, 'machines': 1.916290731874155, 'nlp': 1.0, 'on': 1.916290731874155, 'process': 1.916290731874155, 'so': 1.916290731874155, 'technique': 1.916290731874155, 'this': 1.916290731874155, 'tutorial': 1.916290731874155}\n"
     ]
    }
   ],
   "source": [
    "# Import TF-IDF Vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text1 = ['i love nlp', 'nlp is so cool', \n",
    "'nlp is all about helping machines process language', \n",
    "'this tutorial is on baisc nlp technique']\n",
    "\n",
    "# Initialize vectorizer\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# Fit the data into the vectorizer\n",
    "txt_fitted = tf.fit(text1)\n",
    "\n",
    "# Vectorized the data\n",
    "txt_transformed = txt_fitted.transform(text1)\n",
    "\n",
    "print(txt_transformed)\n",
    "\n",
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Introduction to Text Representations for Language Processing](https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-1-dc6e8068b8a4)\n",
    "2. [An Overview for Text Representations in NLP](https://towardsdatascience.com/an-overview-for-text-representations-in-nlp-311253730af1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
