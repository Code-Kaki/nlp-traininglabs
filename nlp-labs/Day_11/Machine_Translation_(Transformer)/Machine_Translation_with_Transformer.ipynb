{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b60bd6",
   "metadata": {},
   "source": [
    "> **Copyright (c) 2020 Skymind Holdings Berhad**<br><br>\n",
    "> **Copyright (c) 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    "<br>\n",
    "Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "<br>you may not use this file except in compliance with the License.\n",
    "<br>You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0/\n",
    "<br>\n",
    "<br>Unless required by applicable law or agreed to in writing, software\n",
    "<br>distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n",
    "<br>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "<br>See the License for the specific language governing permissions and\n",
    "<br>limitations under the License.\n",
    "<br>\n",
    "<br>\n",
    "**SPDX-License-Identifier: Apache-2.0**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66687604",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we are going to build neural machine translation (NMT) using Transformer with pytorch. This NMT could translate English to French. \n",
    "\n",
    "Let's get started.\n",
    "\n",
    "![Language Modelling](../../../images/NMT.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cceb6",
   "metadata": {},
   "source": [
    "# What will we accomplish?\n",
    "\n",
    "Steps to implement neural machine translation using Transformer with Pytorch:\n",
    "\n",
    "> Step 1: Load and preprocess dataset\n",
    "\n",
    "> Step 2: Building transformer architecture\n",
    "\n",
    "> Step 3: Train the transformer model\n",
    "\n",
    "> Step 4: Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d5b4f",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [Load Dataset](#Load-Dataset)\n",
    "\n",
    "\n",
    "* [Tokenization](#Tokenization)\n",
    "\n",
    "\n",
    "* [Preprocessing](#Preprocessing)\n",
    "\n",
    "    * [Train-Test Split](#Train-Test-Split)\n",
    "    * [TabularDataset](#TabularDataset)\n",
    "    * [BucketIterator](#BucketIterator)\n",
    "    * [Custom Iterator](#Custom-Iterator)\n",
    "\n",
    "\n",
    "* [Dive Deep into Transformer](#Dive-Deep-into-Transformer)\n",
    "\n",
    "    * [Embedding](#Embedding)\n",
    "    * [Positional Encoding](#Positional-Encoding)\n",
    "    * [Masking](#Masking)\n",
    "        * [Input Masks](#Input-Masks)\n",
    "        * [Target Sequence Masks](#Target-Sequence-Masks)\n",
    "    * [Multi-Headed Attention](#Multi-Headed-Attention)\n",
    "    * [Attention](#Attention)\n",
    "    * [Feed-Forward Network](#Feed-Forward-Network)\n",
    "    * [Normalisation](#Normalisation)\n",
    "    \n",
    "    \n",
    "* [Building Transformer](#Building-Transformer)\n",
    "    \n",
    "    * [EncoderLayer](#EncoderLayer)\n",
    "    * [DecoderLayer](#DecoderLayer)\n",
    "    * [Encoder](#Encoder)\n",
    "    * [Decoder](#Decoder)\n",
    "    * [Transformer](#Transformer)\n",
    "    \n",
    "    \n",
    "* [Training the Model](#Training-the-Model)\n",
    "\n",
    "\n",
    "* [Testing the Model](#Testing-the-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d25c41",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852087b8",
   "metadata": {},
   "source": [
    "The dataset we used is [parallel corpus French-English](http://www.statmt.org/europarl/v7/fr-en.tgz) dataset from [European Parliament Proceedings Parallel Corpus (1996–2011)](http://www.statmt.org/europarl/). This dataset contains 15 years of write-ups from E.U. proceedings, weighing in at 2,007,724 sentences, and 50,265,039 words. You should found the dataset in the `datasets` folder, else you may download it [here](http://www.statmt.org/europarl/v7/fr-en.tgz). You will have the following files after unzipping the downloaded file:\n",
    "\n",
    "1. europarl-v7.fr-en.en\n",
    "2. europarl-v7.fr-en.fr\n",
    "\n",
    "![](../../../images/fr-en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7396b8d",
   "metadata": {},
   "source": [
    "Now we are going to load the dataset for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafe26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en = open('../../../resources/day_11/fr-en/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open('../../../resources/day_11/fr-en/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628a64f",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960895bb",
   "metadata": {},
   "source": [
    "The first job we need done is to **create a tokenizer for each language**. This is a function that will split the text into separate words and assign them unique numbers (indexes). This number will come into play later when we discuss embeddings.\n",
    "\n",
    "![Tokenization](../../../images/tokenize.png)\n",
    "\n",
    "He we will tokenize the text using **Torchtext** and **Spacy** together. Spacy is a library that has been specifically built to take sentences in various languages and split them into different tokens (see [here](https://spacy.io/) for more information). Without Spacy, Torchtext defaults to a simple .split(' ') method for tokenization. This is much less nuanced than Spacy’s approach, which will also split words like “don’t” into “do” and “n’t”, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8757823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cf908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee49ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_lg\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b3211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_lg')\n",
    "fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "def tokenize_fr(sentence):\n",
    "    return [tok.text for tok in fr.tokenizer(sentence)]\n",
    "\n",
    "EN_TEXT = Field(tokenize=tokenize_en)\n",
    "FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7885f",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The best way to work with Torchtext is to turn your data into **spreadsheet format**, no matter the original format of your data file. This is due to the incredible versatility of the **Torchtext TabularDataset** function, which creates datasets from spreadsheet formats. So first to turn our data into an appropriate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458ec02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very long sentences and sentences where translations are not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7e60b",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Now we are going to split the data into train set and test set. Fortunately Sklearn and Torchtext together make this process incredibly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2f2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train and validation set \n",
    "train, test = train_test_split(df, test_size=0.1)\n",
    "train.to_csv(\"../../../resources/day_11/train.csv\", index=False)\n",
    "test.to_csv(\"../../../resources/day_11/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e6fc1",
   "metadata": {},
   "source": [
    "This creates a train and test csv each with two columns (English, French), where each row contains an English sentence in the 'English' column, and its French translation in the 'French' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d279c8",
   "metadata": {},
   "source": [
    "## TabularDataset\n",
    "\n",
    "Calling the magic `TabularDataset.splits` then returns a train and test dataset with the respective data loaded into them, processed(/tokenized) according to the fields we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f794bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n",
    "\n",
    "data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]\n",
    "\n",
    "train, test = TabularDataset.splits(path='../../../resources/day_11', train='train.csv', validation='test.csv', \n",
    "                                    format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907e535",
   "metadata": {},
   "source": [
    "Processing a few million words can take a while so grab a cup of tea here…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cc61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_TEXT.build_vocab(train, test)\n",
    "EN_TEXT.build_vocab(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718779",
   "metadata": {},
   "source": [
    "To see what numbers the tokens have been assigned and vice versa in each field, we can use `self.vocab.stoi` and `self.vocab.itos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b18579bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(EN_TEXT.vocab.stoi['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09301046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(EN_TEXT.vocab.itos[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d7eea",
   "metadata": {},
   "source": [
    "## BucketIterator\n",
    "\n",
    "**BucketIterator** Defines an iterator that batches examples of similar lengths together.\n",
    "\n",
    "It minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. See pool for the bucketing procedure used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def235b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=20, sort_key=lambda x: len(x.French), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bb660",
   "metadata": {},
   "source": [
    "The `sort_key` dictates how to form each batch. The lambda function tells the iterator to try and find sentences of the **same length** (meaning more of the matrix is filled with useful data and less with padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae8071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 315, 1296,  315,  ..., 3004,   68,   68],\n",
      "        [  13,    3,  579,  ...,   84, 1225,  474],\n",
      "        [ 152,   15, 1515,  ...,  813, 1024, 1407],\n",
      "        ...,\n",
      "        [   1,   85,    1,  ...,    2,    1,    1],\n",
      "        [   1,    4,    1,  ...,  146,    1,    1],\n",
      "        [   1,    1,    1,  ...,    4,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "print(batch.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757a9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns:\", len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba2bce",
   "metadata": {},
   "source": [
    "In each batch, sentences have been transposed so they are descending vertically (important: we will need to transpose these again to work with transformer). **Each index represents a token (word)**, and **each column represents a sentence**. We have 20 columns, as 20 was the batch_size we specified.\n",
    "\n",
    "You might notice all the ‘1’s and think which incredibly common word is this the index for? Well the ‘1’ is not of course a word, but purely **padding**. While Torchtext is brilliant, it’s `sort_key` based batching leaves a little to be desired. Often the sentences aren’t of the same length at all, and you end up feeding a lot of padding into your network (as you can see with all the 1s in the last figure). We will solve this by implementing our own iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e8389",
   "metadata": {},
   "source": [
    "## Custom Iterator\n",
    "\n",
    "The custom iterator is built in reference to the code from http://nlp.seas.harvard.edu/2018/04/03/attention.html. Feel free to explore yourself to have more understanding about `MyIterator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8c03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.English))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.French) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    \n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)     \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074e0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = MyIterator(train, batch_size= 64, device=device, repeat=False, \n",
    "                        sort_key= lambda x: (len(x.English), len(x.French)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98879c8",
   "metadata": {},
   "source": [
    "# Dive Deep into Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb516a5b",
   "metadata": {},
   "source": [
    "![Transformer](../../../images/transformer.png)\n",
    "\n",
    "The diagram above shows the overview of the Transformer model. The inputs to the encoder will be the **English** sentence, and the 'Outputs' from the decoder will be the **French** sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42492084",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "Embedding words has become standard practice in NMT, feeding the network with far more information about words than a one hot encoding would.\n",
    "\n",
    "![Embedding Layer](../../../images/embeddings.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7199d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d440f",
   "metadata": {},
   "source": [
    "When each word is fed into the network, this code will perform a look-up and retrieve its embedding vector. These vectors will then be learnt as a parameters by the model, adjusted with each iteration of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e9c80",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In order for the model to make sense of a sentence, it needs to know two things about each word: what does the **word mean**? And what is its **position** in the sentence?\n",
    "\n",
    "The embedding vector for each word will **learn the meaning**, so now we need to input something that tells the network about the word’s position.\n",
    "\n",
    "*Vasmari et al* answered this problem by using these functions to create a constant of position-specific values:\n",
    "\n",
    "![Position Encoding](../../../images/pos_encoding_1.png)\n",
    "\n",
    "![Position Encoding](../../../images/pos_encoding_2.png)\n",
    "\n",
    "This constant is a 2D matrix. Pos refers to the order in the sentence, and $i$ refers to the position along the embedding vector dimension. Each value in the pos/i matrix is then worked out using the equations above.\n",
    "\n",
    "![Position Encoding](../../../images/pos_encoding_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17ac4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create constant 'pe' matrix with values dependant on pos and i\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # Add constant to embedding\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        \n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37f413",
   "metadata": {},
   "source": [
    "`PositionalEncoder` lets us add the **positional encoding to the embedding vector**, providing information about structure to the model.\n",
    "\n",
    "The reason we increase the embedding values before addition is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector won’t be lost when we add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04333ed7",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Masking plays an important role in the transformer. It serves two purposes:\n",
    "\n",
    "* In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.\n",
    "\n",
    "\n",
    "* In the decoder: To prevent the decoder ‘peaking’ ahead at the rest of the translated sentence when predicting the next word.\n",
    "\n",
    "![Masking](../../../images/masking.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb434475",
   "metadata": {},
   "source": [
    "### Input Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ae739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "input_seq = batch.English.transpose(0,1)\n",
    "input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "\n",
    "# creates mask with 0s wherever there is padding in the input\n",
    "input_msk = (input_seq != input_pad).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5f614",
   "metadata": {},
   "source": [
    "### Target Sequence Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcfcc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "target_seq = batch.French.transpose(0,1)\n",
    "target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
    "target_msk = (target_seq != target_pad).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4a209",
   "metadata": {},
   "source": [
    "The initial input into the decoder will be the **target sequence** (the French translation). The way the decoder predicts each output word is by making use of all the encoder outputs and the French sentence only up until the point of each word its predicting.\n",
    "\n",
    "Therefore we need to prevent the first output predictions from being able to see later into the sentence. For this we use the `nopeak_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43dd9bd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ...,  True, False, False],\n",
      "         [ True,  True,  True,  ...,  True,  True, False],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Get seq_len for matrix\n",
    "size = target_seq.size(1) \n",
    "\n",
    "nopeak_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0).cuda()\n",
    "\n",
    "print(nopeak_mask)\n",
    "\n",
    "target_msk = target_msk & nopeak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c90b3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, trg):\n",
    "    src_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "    trg_pad = FR_TEXT.vocab.stoi['<pad>']\n",
    "    \n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        \n",
    "        # Get seq_len for matrix\n",
    "        size = trg.size(1) \n",
    "        np_mask = nopeak_mask(size)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "751006c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size):\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    \n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596adfda",
   "metadata": {},
   "source": [
    "If we later apply this mask to the attention scores, the values wherever the input is ahead will not be able to contribute when calculating the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22198690",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention\n",
    "\n",
    "Once we have our embedded values (with positional encodings) and our masks, we can start building the layers of our model.\n",
    "Here is an overview of the multi-headed attention layer:\n",
    "\n",
    "![Multi-Headed Attention](../../../images/multi-head-attention.png)\n",
    "\n",
    "In multi-headed attention layer, each **input is split into multiple heads** which allows the network to simultaneously attend to different subsections of each embedding.\n",
    "\n",
    "$V$, $K$ and $Q$ stand for ***key***, ***value*** and ***query***. These are terms used in attention functions. In the case of the Encoder, $V$, $K$ and $Q$ will simply be identical copies of the embedding vector (plus positional encoding). They will have the dimensions `Batch_size` * `seq_len` * `embedding_dimension`.\n",
    "\n",
    "In multi-head attention we split the embedding vector into $N$ heads, so they will then have the dimensions `batch_size` * `N` * `seq_len` * (`embedding_dimension` / `N`).\n",
    "\n",
    "This final dimension (`embedding_dimension` / `N`) we will refer to as $d_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "660b2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # Perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # Transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # Calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4888d5",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The equation below is the attention formula with retrieved from [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper and it does a good job at explaining each step.\n",
    "\n",
    "![Attention Equation](../../../images/attention.png)\n",
    "\n",
    "![Attention Equation](../../../images/attention-img.png)\n",
    "\n",
    "Each arrow in the diagram reflects a part of the equation.\n",
    "\n",
    "Initially we must **multiply** $Q$ by the transpose of $K$. This is then scaled by **dividing the output by the square root** of $d_k$.\n",
    "\n",
    "A step that’s not shown in the equation is the masking operation. Before we perform **Softmax**, we apply our mask and hence reduce values where the input is padding (or in the decoder, also where the input is ahead of the current word). Another step not shown is **dropout**, which we will apply after Softmax.\n",
    "\n",
    "Finally, the last step is doing a **dot product** between the result so far and $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "196140ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1afb2d",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "The feed-forward layer just consists of two linear operations, with a **relu** and **dropout** operation in between them. It simply deepens our network, employing linear layers to **analyse patterns** in the attention layers output.\n",
    "\n",
    "![FeedForward Neural Network](../../../images/feed-forward-nn.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b169e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b3ce7",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "Normalisation is highly important in deep neural networks. It prevents the range of values in the layers changing too much, meaning the model **trains faster** and has **better ability to generalise**.\n",
    "\n",
    "![Normalization](../../../images/norm.png)\n",
    "\n",
    "We will be normalising our results between each layer in the encoder/decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6b89b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c374b",
   "metadata": {},
   "source": [
    "# Building Transformer\n",
    "\n",
    "Let’s have another look at the over-all architecture and start building:\n",
    "\n",
    "![Transformer](../../../images/transformer.png)\n",
    "\n",
    "**One last Variable**: If you look at the diagram closely you can see a $N_x$ next to the encoder and decoder architectures. In reality, the encoder and decoder in the diagram above represent one layer of an encoder and one of the decoder. $N$ is the variable for the **number of layers** there will be. Eg. if `N=6`, the data goes through six encoder layers (with the architecture seen above), then these outputs are passed to the decoder which also consists of six repeating decoder layers.\n",
    "\n",
    "We will now build `EncoderLayer` and `DecoderLayer` modules with the architecture shown in the model above. Then when we build the encoder and decoder we can define how many of these layers to have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76cf6d",
   "metadata": {},
   "source": [
    "## EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79f801a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an encoder layer with one multi-head attention layer and one feed-forward layer\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b70573",
   "metadata": {},
   "source": [
    "## DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "823ebf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and one feed-forward layer\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda()\n",
    "        \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "            x2 = self.norm_1(x)\n",
    "            x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "\n",
    "            x2 = self.norm_2(x)\n",
    "            x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "\n",
    "            x2 = self.norm_3(x)\n",
    "            x = x + self.dropout_3(self.ff(x2))\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec72b85",
   "metadata": {},
   "source": [
    "We can then build a convenient cloning function that can generate multiple layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a07d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b248bb",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9808aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292ed7a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33842982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d844ecf",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10921ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b5c87",
   "metadata": {},
   "source": [
    "**Note**: We don't perform softmax on the output as this will be handled automatically by our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1564c95",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "With the transformer built, all that remains is to train on the dataset. The coding part is done, but be prepared to wait for about 2 days for this model to start converging! However, in this session, we only perform minimal epoch to train the model and you may try to use more epoch during your self-study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f70e7",
   "metadata": {},
   "source": [
    "Let’s define some parameters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cfc373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 512\n",
    "heads = 4\n",
    "N = 6\n",
    "\n",
    "src_vocab = len(EN_TEXT.vocab)\n",
    "trg_vocab = len(FR_TEXT.vocab)\n",
    "\n",
    "model = Transformer(src_vocab, trg_vocab, embedding_dimension, N, heads)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model.cuda()\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "        \n",
    "# This code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf06ab",
   "metadata": {},
   "source": [
    "Now we’re good to train the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d479917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def train_model(epochs, print_every=100, timelimit=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    \n",
    "    total_loss = 0\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "       \n",
    "        for i, batch in enumerate(train_iter):\n",
    "            src = batch.English.transpose(0,1)\n",
    "            trg = batch.French.transpose(0,1)\n",
    "            # the French sentence we input has all words except\n",
    "            # the last, as it is using each word to predict the next\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            \n",
    "            # the words we are trying to predict\n",
    "            \n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create function to make masks using mask code above\n",
    "            \n",
    "            src_mask, trg_mask = create_masks(src, trg_input)\n",
    "            \n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=target_pad)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optim.step()\n",
    "                        \n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                \n",
    "                duration = (time.time() - start) // 60\n",
    "                \n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % \n",
    "                      (duration, epoch + 1, i + 1, loss_avg, time.time() - temp, print_every))\n",
    "                \n",
    "                if loss_avg < min_loss:\n",
    "                    min_loss = loss_avg\n",
    "                    torch.save(model, \"model/training.model\")\n",
    "                    print(\"Current best model saved\", \"loss =\", loss_avg)\n",
    "                    \n",
    "                if (timelimit and duration >= timelimit):\n",
    "                    break\n",
    "                \n",
    "                total_loss = 0\n",
    "                temp = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad0308ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedder(\n",
       "      (embed): Embedding(109944, 512)\n",
       "    )\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedder(\n",
       "      (embed): Embedding(143313, 512)\n",
       "    )\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=143313, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_model(1, timelimit=300)\n",
    "\n",
    "torch.load(\"model/pretrained.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0067b34",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "\n",
    "We can use the below function to translate sentences. We can feed it sentences directly from our batches, or input custom strings.\n",
    "\n",
    "The translator works by running a loop. We start off by encoding the English sentence. We then feed the decoder the `<sos>` token index and the encoder outputs. The decoder makes a prediction for the first word, and we add this to our decoder input with the sos token. We rerun the loop, getting the next prediction and adding this to the decoder input, until we reach the `<eos>` token letting us know it has finished translating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f24ed59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src, max_len = 80, custom_string=False):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if custom_string == True:\n",
    "        src = tokenize_en(src)\n",
    "        src = Variable(torch.LongTensor([[EN_TEXT.vocab.stoi[tok] for tok in src]])).cuda()\n",
    "            \n",
    "    src_mask = (src != input_pad).unsqueeze(-2)\n",
    "    e_outputs = model.encoder(src, src_mask)\n",
    "\n",
    "    outputs = torch.zeros(max_len).type_as(src.data)\n",
    "    outputs[0] = torch.LongTensor([FR_TEXT.vocab.stoi['<sos>']])\n",
    "    \n",
    "    for i in range(1, max_len):\n",
    "        trg_mask = np.triu(np.ones((1, i, i)), k=1).astype('uint8')\n",
    "\n",
    "        trg_mask = Variable(torch.from_numpy(trg_mask) == 0).cuda()\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:i].unsqueeze(0), e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        val, ix = out[:, -1].data.topk(1)\n",
    "\n",
    "        outputs[i] = ix[0][0]\n",
    "\n",
    "        if ix[0][0] == FR_TEXT.vocab.stoi['<eos>']:\n",
    "            break\n",
    "                           \n",
    "    return ' '.join([FR_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a10adcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> Essential Essential Essential strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises pro-indonésiennes strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises Interrogé Interrogé Interrogé strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises strasbourgeoises'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, \"How're you my friend?\", custom_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15800fff",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d908e",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [How to Code The Transformer in Pytorch](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#b0ed)\n",
    "\n",
    "2. [How to Use TorchText for Neural Machine Translation](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
