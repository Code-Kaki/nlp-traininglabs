{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c730b369",
   "metadata": {},
   "source": [
    "![license_header_logo](../../../images/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    "<br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407be03",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will introduce you the basic of spaCy and some simple implementations of spaCy. SpaCy is widely used in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7286a59",
   "metadata": {},
   "source": [
    "# Notebook Content\n",
    "\n",
    "* [What’s SpaCy?](#What’s-spaCy?)\n",
    "\n",
    "\n",
    "* [Features](#Features)\n",
    "\n",
    "\n",
    "* [Installation](#Installation)\n",
    "\n",
    "\n",
    "* [Linguistic Annotations](#Linguistic-Annotations)\n",
    "\n",
    "\n",
    "* [Tokenization](#Tokenization)\n",
    "\n",
    "\n",
    "* [Part-of-speech Tags and Dependencies](#Part-of-speech-Tags-and-Dependencies)\n",
    "    \n",
    "\n",
    "* [Named Entities](#Named-Entities)\n",
    "\n",
    "\n",
    "* [Word Vectors and Similarity](#Word-Vectors-and-Similarity)\n",
    "\n",
    "\n",
    "* [Vocab, Hashes and Lexemes](#Vocab,-Hashes-and-Lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae899a",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"300\" height=\"300\" src=\"../../../images/spacy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c153159",
   "metadata": {},
   "source": [
    "# What’s spaCy?\n",
    "    \n",
    "spaCy is a free, **open-source library** for **advanced Natural Language Processing (NLP)** in Python.\n",
    "\n",
    "If you’re working with a lot of text, you’ll eventually want to know more about it. For example, what’s it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "\n",
    "spaCy is designed specifically for **production use** and helps you build applications that process and “understand” large volumes of text. It can be used to build **information extraction** or **natural language understanding systems**, or to **pre-process text for deep learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae32b7b",
   "metadata": {},
   "source": [
    "# Features\n",
    "    \n",
    "In this notebook, you’ll come across mentions of spaCy’s features and capabilities. Some of them refer to linguistic concepts, while others are related to more general machine learning functionality.\n",
    "\n",
    "![todo](../../../images/todo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95d175",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "spaCy’s trained pipelines can be installed as Python packages. This means that they’re a component of your application, just like any other module. They’re versioned and can be defined as a dependency in your requirements.txt. Trained pipelines can be installed from a download URL or a local directory, manually or via pip. Their data can be located anywhere on your file system.\n",
    "\n",
    "Installation link: https://spacy.io/usage/models\n",
    "\n",
    "![installation](../../../images/installation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e5692",
   "metadata": {},
   "source": [
    "# Linguistic Annotations\n",
    "\n",
    "spaCy provides a variety of linguistic annotations to give you **insights into a text’s grammatical structure**. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cef87b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68baa68e",
   "metadata": {},
   "source": [
    "Even though a `Doc` is processed – e.g. split into individual words and annotated – it still holds **all information of the original text**, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you’ll never lose any information when processing text with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be7c3c",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "During processing, spaCy first **tokenizes** the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each `Doc` consists of individual tokens, and we can iterate over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d0ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c303fc",
   "metadata": {},
   "source": [
    "![tokenization](../../../images/tokenization.png)\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "    1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "    \n",
    "    2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "    \n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split **complex, nested tokens** like combinations of abbreviations and multiple punctuation marks.\n",
    "\n",
    "![tokenization](../../../images/tokenization_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597c75b",
   "metadata": {},
   "source": [
    "# Part-of-speech Tags and Dependencies\n",
    "\n",
    "After tokenization, spaCy can **parse** and **tag** a given `Doc`. This is where the trained pipeline and its statistical models come in, which enable spaCy to **make predictions** of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "Linguistic annotations are available as Token attributes. Like many NLP libraries, spaCy **encodes all strings to hash values** to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore `_` to its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a4b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617fb40",
   "metadata": {},
   "source": [
    "![POS](../../../images/pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88744b1",
   "metadata": {},
   "source": [
    "# Named Entities\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can **recognize various types of named entities in a document, by asking the model for a prediction**. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work *perfectly* and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the `ents` property of a `Doc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2bce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d11ae",
   "metadata": {},
   "source": [
    "![Named Entity](../../../images/named_entity.png)\n",
    "\n",
    "Here’s what our example sentence and its named entities look like:\n",
    "\n",
    "![Named Entity](../../../images/named_entity_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83229e",
   "metadata": {},
   "source": [
    "# Word Vectors and Similarity\n",
    "\n",
    "Similarity is determined by comparing **word vectors** or **“word embeddings”**, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like **word2vec** and usually look like this:\n",
    "\n",
    "array([\n",
    "<p>2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "<br> 3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "<br> -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "<br> 5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "<br> -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "<br> 1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "<br> 5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "<br> 2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "<br> 1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "<br> # ... and so on ...\n",
    "<br> 3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "<br> -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "<br> -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01,</p><br>] dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22badf4",
   "metadata": {},
   "source": [
    "> To make them compact and fast, spaCy’s small **pipeline packages** (all packages that end in `sm`) **don’t ship with word vectors**, and only include context-sensitive **tensors**. This means you can still use the `similarity()` methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> Small pipeline: python -m spacy download en_core_web_sm\n",
    "\n",
    "> Large pipeline: python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73122fed",
   "metadata": {},
   "source": [
    "Pipeline packages that come with built-in word vectors make them available as the `Token.vector attribute`. `Doc.vector` and `Span.vector` will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the **L2 norm**, which can be used to normalize vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5836c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "# To install en_core_web_md use > python -m spacy download en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f596fd1",
   "metadata": {},
   "source": [
    "The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of `0`, which means it’s practically nonexistent. If your application will benefit from a **large vocabulary** with more vectors, you should consider using one of the larger pipeline packages or loading in a full vector package, for example, `en_core_web_lg`, which includes 685k unique vectors.\n",
    "\n",
    "spaCy is able to compare two objects, and make a prediction of **how similar they are**. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one.\n",
    "\n",
    "Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether two words, spans or documents are similar really depends on how you’re looking at it. spaCy’s similarity implementation usually assumes a pretty general-purpose definition of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ecd7c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.7687607012190486\n",
      "salty fries <-> hamburgers 0.6949788\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger package!\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723eb546",
   "metadata": {},
   "source": [
    "Computing similarity scores can be helpful in many situations, but it’s also important to maintain **realistic expectations** about what information it can provide. Words can be related to each other in many ways, so a single “similarity” score will always be a **mix of different signals**, and vectors trained on different data can produce very different results that may not be useful for your purpose. Here are some important considerations to keep in mind:\n",
    "\n",
    "> + There’s no objective definition of similarity. Whether “I like burgers” and “I like pasta” is similar **depends on your application**. Both talk about food preferences, which makes them very similar – but if you’re analyzing mentions of food, those sentences are pretty dissimilar, because they talk about very different foods.\n",
    "> + The similarity of `Doc` and `Span` objects defaults to the **average** of the token vectors. This means that the vector for “fast food” is the average of the vectors for “fast” and “food”, which isn’t necessarily representative of the phrase “fast food”.\n",
    "> + Vector averaging means that the vector of multiple tokens is **insensitive to the order of the words**. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a8af9",
   "metadata": {},
   "source": [
    "# Vocab, Hashes and Lexemes\n",
    "\n",
    "Whenever possible, spaCy tries to store data in a vocabulary, the `Vocab`, that will be **shared by multiple documents**. To save memory, spaCy also encodes all strings to **hash values** – in this case for example, “coffee” has the hash `3197928453018144401`. Entity labels like “ORG” and part-of-speech tags like “VERB” are also encoded. Internally, spaCy only “speaks” in hash values.\n",
    "\n",
    "![Vocab Hashing](../../../images/hashing.png)\n",
    "\n",
    "If you process lots of documents containing the word “coffee” in all kinds of different contexts, storing the exact string “coffee” every time would take up way too much space. So instead, spaCy hashes the string and stores it in the `StringStore`. You can think of the `StringStore` as a **lookup table that works in both directions** – you can look up a string to get its hash, or a hash to get its string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0269c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab6823",
   "metadata": {},
   "source": [
    "Now that all strings are encoded, the entries in the vocabulary **don’t need to include the word text** themselves. Instead, they can look it up in the StringStore via its hash value. Each entry in the vocabulary, also called `Lexeme`, contains the **context-independent** information about a word. For example, no matter if “love” is used as a verb or a noun in some context, its spelling and whether it consists of alphabetic characters won’t ever change. Its hash value will also always be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0433953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213989b",
   "metadata": {},
   "source": [
    "The mapping of words to hashes doesn’t depend on any state. To make sure each value is unique, spaCy uses a hash function to calculate the hash **based on the word string**. This also means that the hash for “coffee” will always be the same, no matter which pipeline you’re using or how you’ve configured spaCy.\n",
    "\n",
    "However, hashes **cannot be reversed** and there’s no way to resolve `3197928453018144401` back to “coffee”. All spaCy can do is look it up in the vocabulary. That’s why you always need to make sure all objects you create have access to the same vocabulary. If they don’t, spaCy might not be able to find the strings it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acc6d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n",
      "coffee\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")  # Original Doc\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n",
    "\n",
    "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
    "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
    "\n",
    "empty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\n",
    "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n",
    "\n",
    "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
    "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a42da",
   "metadata": {},
   "source": [
    "If the vocabulary doesn’t contain a string for `3197928453018144401`, spaCy will raise an error. You can re-add “coffee” manually, but this only works if you actually know that the document contains that word. To prevent this problem, spaCy will also export the `Vocab` when you save a `Doc` or `nlp` object. This will give you the object and its encoded annotations, plus the “key” to decode it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7019e",
   "metadata": {},
   "source": [
    "# Contributors\n",
    "\n",
    "**Author**\n",
    "<br>Chee Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c6b9b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [SpaCy API Documentation](https://spacy.io/api)\n",
    "2. [SpaCy 101: Everything You Need to Know](https://spacy.io/usage/spacy-101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
